{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046fe913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:30:41.982026Z",
     "iopub.status.busy": "2024-12-11T18:30:41.981437Z",
     "iopub.status.idle": "2024-12-11T18:32:06.576007Z",
     "shell.execute_reply": "2024-12-11T18:32:06.574651Z"
    },
    "papermill": {
     "duration": 84.610008,
     "end_time": "2024-12-11T18:32:06.578048",
     "exception": false,
     "start_time": "2024-12-11T18:30:41.968040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/setup-eedi-environment/uv-0.4.30-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: uv\r\n",
      "Successfully installed uv-0.4.30\r\n",
      "\u001b[2mUsing Python 3.10.14 environment at /opt/conda\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m25 packages\u001b[0m \u001b[2min 126ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m17 packages\u001b[0m \u001b[2min 19.82s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m6 packages\u001b[0m \u001b[2min 1.28s\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m17 packages\u001b[0m \u001b[2min 333ms\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.6.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.9.0 (from file:///home/conda/feedstock_root/build_artifacts/fsspec_1725543257300/work)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.10.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.1.3.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.0.2.54\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.2.106\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.4.5.107\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.1.0.106\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.20.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.3.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.4.0 (from file:///home/conda/feedstock_root/build_artifacts/pillow_1726075077786/work)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.0.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.0 (from file:///tmp/torch/torch-2.4.0-cp310-cp310-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.0 (from file:///tmp/torch/torchvision-0.19.0-cp310-cp310-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.0.0\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.10.14 environment at /opt/conda\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m113 packages\u001b[0m \u001b[2min 159ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m36 packages\u001b[0m \u001b[2min 14.34s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m25 packages\u001b[0m \u001b[2min 1.80s\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m36 packages\u001b[0m \u001b[2min 371ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.6.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.0.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1meinops\u001b[0m\u001b[2m==0.8.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.111.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.115.4\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.10.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.9.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.10.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.7.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\r\n",
      " \u001b[33m~\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.6\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.4.4\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mmsgpack\u001b[0m\u001b[2m==1.0.8\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mmsgpack\u001b[0m\u001b[2m==1.1.0 (from file:///home/conda/feedstock_root/build_artifacts/msgpack-python_1725974993022/work)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgpack\u001b[0m\u001b[2m==1.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.18.6\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.54.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.0.46\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==21.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==24.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.2 (from file:///home/conda/feedstock_root/build_artifacts/pandas_1715897614105/work)\u001b[0m\r\n",
      " \u001b[33m~\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post4\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.0.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.4.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.0.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==3.20.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==4.25.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.28.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==5.9.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==5.9.8 (from file:///home/conda/feedstock_root/build_artifacts/psutil_1705722392846/work)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==6.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyairports\u001b[0m\u001b[2m==2.1.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2024.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2024.2 (from file:///home/conda/feedstock_root/build_artifacts/pytz_1726055524169/work)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2024.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.37.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.41.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.7.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.45.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.46.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2024.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2024.2 (from file:///home/conda/feedstock_root/build_artifacts/python-tzdata_1727140567071/work)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2024.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==1.26.18\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.2.1 (from file:///home/conda/feedstock_root/build_artifacts/urllib3_1708239446578/work)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.2.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.6.3.post1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.27.post2\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.10.14 environment at /opt/conda\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==3.2.1 (from file:///kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.2.1-py3-none-any.whl)\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.10.14 environment at /opt/conda\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m39 packages\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.13.2\u001b[0m\r\n",
      "CPU times: user 1.05 s, sys: 276 ms, total: 1.32 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip install /kaggle/input/setup-eedi-environment/uv-0.4.30-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!uv pip install --system --no-index --find-links=/kaggle/input/vllm-0-6-3-post1 torchvision==0.19.1\n",
    "!uv pip install --system --no-index --find-links=/kaggle/input/vllm-0-6-3-post1 vllm\n",
    "!uv pip install --system --no-deps /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.2.1-py3-none-any.whl\n",
    "!uv pip install --system --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e512864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:06.620134Z",
     "iopub.status.busy": "2024-12-11T18:32:06.619820Z",
     "iopub.status.idle": "2024-12-11T18:32:16.343356Z",
     "shell.execute_reply": "2024-12-11T18:32:16.342205Z"
    },
    "papermill": {
     "duration": 9.74674,
     "end_time": "2024-12-11T18:32:16.345805",
     "exception": false,
     "start_time": "2024-12-11T18:32:06.599065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q --no-index --find-links=/kaggle/input/eedi-faiss-dependencies/faiss-dependencies faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57cae0",
   "metadata": {
    "papermill": {
     "duration": 0.018982,
     "end_time": "2024-12-11T18:32:16.384111",
     "exception": false,
     "start_time": "2024-12-11T18:32:16.365129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# rihanpiggy part(retrieve SubjectName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d2a069",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:16.423418Z",
     "iopub.status.busy": "2024-12-11T18:32:16.423069Z",
     "iopub.status.idle": "2024-12-11T18:32:17.521817Z",
     "shell.execute_reply": "2024-12-11T18:32:17.521182Z"
    },
    "papermill": {
     "duration": 1.1207,
     "end_time": "2024-12-11T18:32:17.523682",
     "exception": false,
     "start_time": "2024-12-11T18:32:16.402982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append('/kaggle/input/eedi-source-comp/eedi-kaggle')\n",
    "os.environ[\"PYTHONPATH\"] = \"/kaggle/input/eedi-source-comp/eedi-kaggle\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"]=\"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"]=\"1\"\n",
    "# os.environ[\"VLLM_ATTENTION_BACKEND\"]=\"FLASHINFER\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79995915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:17.567396Z",
     "iopub.status.busy": "2024-12-11T18:32:17.566988Z",
     "iopub.status.idle": "2024-12-11T18:32:17.581902Z",
     "shell.execute_reply": "2024-12-11T18:32:17.581299Z"
    },
    "papermill": {
     "duration": 0.039757,
     "end_time": "2024-12-11T18:32:17.583646",
     "exception": false,
     "start_time": "2024-12-11T18:32:17.543889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\n",
    "is_true_test = len(df_test)>3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a7191c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:17.624540Z",
     "iopub.status.busy": "2024-12-11T18:32:17.623911Z",
     "iopub.status.idle": "2024-12-11T18:32:17.627405Z",
     "shell.execute_reply": "2024-12-11T18:32:17.626644Z"
    },
    "papermill": {
     "duration": 0.024828,
     "end_time": "2024-12-11T18:32:17.629067",
     "exception": false,
     "start_time": "2024-12-11T18:32:17.604239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = '/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583ea6ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:17.677562Z",
     "iopub.status.busy": "2024-12-11T18:32:17.677322Z",
     "iopub.status.idle": "2024-12-11T18:32:46.052896Z",
     "shell.execute_reply": "2024-12-11T18:32:46.052054Z"
    },
    "papermill": {
     "duration": 28.40739,
     "end_time": "2024-12-11T18:32:46.054975",
     "exception": false,
     "start_time": "2024-12-11T18:32:17.647585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\r\n",
      "Namespace(filepath='/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv', index_db_path='/kaggle/input/eedi-source-comp/eedi-kaggle/eedi/operation/1.create-vector-db/faiss_subject.index', subject_master_path='/kaggle/input/eedi-source-comp/eedi-kaggle/master/subject_metadata.csv', model_path='/kaggle/input/eedi-pretrained-models/transformers/default/3/gte-base-en-v1.5', output_dir='/tmp')\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "saving file to /tmp/test.csv\r\n"
     ]
    }
   ],
   "source": [
    "# SubjectNameをretrieve\n",
    "test_filepath = '/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv'\n",
    "index_db_path = '/kaggle/input/eedi-source-comp/eedi-kaggle/eedi/operation/1.create-vector-db/faiss_subject.index'\n",
    "subject_master_path = '/kaggle/input/eedi-source-comp/eedi-kaggle/master/subject_metadata.csv'\n",
    "model_path = '/kaggle/input/eedi-pretrained-models/transformers/default/3/gte-base-en-v1.5'\n",
    "!echo  $test_filepath\n",
    "!cd /kaggle/input/eedi-source-comp/eedi-kaggle/eedi/training && python3 run_subject_search.py --filepath $test_filepath \\\n",
    "                               --index-db-path $index_db_path \\\n",
    "                               --subject-master-path $subject_master_path \\\n",
    "                               --model-path $model_path \\\n",
    "                               --output-dir $output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c9f582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:46.094971Z",
     "iopub.status.busy": "2024-12-11T18:32:46.094389Z",
     "iopub.status.idle": "2024-12-11T18:32:46.120187Z",
     "shell.execute_reply": "2024-12-11T18:32:46.119420Z"
    },
    "papermill": {
     "duration": 0.047241,
     "end_time": "2024-12-11T18:32:46.121956",
     "exception": false,
     "start_time": "2024-12-11T18:32:46.074715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectId</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>AnswerAText</th>\n",
       "      <th>AnswerBText</th>\n",
       "      <th>AnswerCText</th>\n",
       "      <th>AnswerDText</th>\n",
       "      <th>FirstSubjectId</th>\n",
       "      <th>FirstSubjectName</th>\n",
       "      <th>SecondSubjectId</th>\n",
       "      <th>SecondSubjectName</th>\n",
       "      <th>ThirdSubjectId</th>\n",
       "      <th>ThirdSubjectName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>32</td>\n",
       "      <td>Number</td>\n",
       "      <td>144</td>\n",
       "      <td>Basic Arithmetic</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>49</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>255</td>\n",
       "      <td>Algebraic Fractions</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>B</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>Neither is correct</td>\n",
       "      <td>101</td>\n",
       "      <td>Data and Statistics</td>\n",
       "      <td>338</td>\n",
       "      <td>Data Processing</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId  ConstructId                                      ConstructName  \\\n",
       "0        1869          856  Use the order of operations to carry out calcu...   \n",
       "1        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "2        1871         2774            Calculate the range from a list of data   \n",
       "\n",
       "   SubjectId                                        SubjectName CorrectAnswer  \\\n",
       "0         33                                             BIDMAS             A   \n",
       "1       1077                    Simplifying Algebraic Fractions             D   \n",
       "2        339  Range and Interquartile Range from a List of Data             B   \n",
       "\n",
       "                                        QuestionText            AnswerAText  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "2  Tom and Katie are discussing the \\( 5 \\) plant...              Only\\nTom   \n",
       "\n",
       "              AnswerBText            AnswerCText             AnswerDText  \\\n",
       "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets   \n",
       "1               \\( m+2 \\)              \\( m-1 \\)       Does not simplify   \n",
       "2             Only\\nKatie     Both Tom and Katie      Neither is correct   \n",
       "\n",
       "   FirstSubjectId     FirstSubjectName  SecondSubjectId    SecondSubjectName  \\\n",
       "0              32               Number              144     Basic Arithmetic   \n",
       "1              49              Algebra              255  Algebraic Fractions   \n",
       "2             101  Data and Statistics              338      Data Processing   \n",
       "\n",
       "   ThirdSubjectId                                   ThirdSubjectName  \n",
       "0              33                                             BIDMAS  \n",
       "1            1077                    Simplifying Algebraic Fractions  \n",
       "2             339  Range and Interquartile Range from a List of Data  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/tmp/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49695cdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:46.160851Z",
     "iopub.status.busy": "2024-12-11T18:32:46.160599Z",
     "iopub.status.idle": "2024-12-11T18:32:50.142868Z",
     "shell.execute_reply": "2024-12-11T18:32:50.142001Z"
    },
    "papermill": {
     "duration": 4.004135,
     "end_time": "2024-12-11T18:32:50.145095",
     "exception": false,
     "start_time": "2024-12-11T18:32:46.140960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(filepath='/tmp/test.csv', output_dir='/tmp')\r\n",
      "saving file to /tmp/test.csv\r\n"
     ]
    }
   ],
   "source": [
    "# test.csvを横持ちに変換\n",
    "test_filepath = '/tmp/test.csv'\n",
    "!cd /kaggle/input/eedi-source-comp/eedi-kaggle/eedi/training && python3 run_prepare_df.py --filepath $test_filepath --output-dir $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "445ff1f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:50.185959Z",
     "iopub.status.busy": "2024-12-11T18:32:50.185357Z",
     "iopub.status.idle": "2024-12-11T18:32:50.205491Z",
     "shell.execute_reply": "2024-12-11T18:32:50.204711Z"
    },
    "papermill": {
     "duration": 0.042219,
     "end_time": "2024-12-11T18:32:50.206945",
     "exception": false,
     "start_time": "2024-12-11T18:32:50.164726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectId</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>Answer</th>\n",
       "      <th>AnswerText</th>\n",
       "      <th>Correct</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>CorrectAnswerText</th>\n",
       "      <th>FirstSubjectId</th>\n",
       "      <th>FirstSubjectName</th>\n",
       "      <th>SecondSubjectId</th>\n",
       "      <th>SecondSubjectName</th>\n",
       "      <th>ThirdSubjectId</th>\n",
       "      <th>ThirdSubjectName</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>B</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>32</td>\n",
       "      <td>Number</td>\n",
       "      <td>144</td>\n",
       "      <td>Basic Arithmetic</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>C</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>32</td>\n",
       "      <td>Number</td>\n",
       "      <td>144</td>\n",
       "      <td>Basic Arithmetic</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>D</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>32</td>\n",
       "      <td>Number</td>\n",
       "      <td>144</td>\n",
       "      <td>Basic Arithmetic</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>A</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>49</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>255</td>\n",
       "      <td>Algebraic Fractions</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>B</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>49</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>255</td>\n",
       "      <td>Algebraic Fractions</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>C</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>49</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>255</td>\n",
       "      <td>Algebraic Fractions</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>A</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>101</td>\n",
       "      <td>Data and Statistics</td>\n",
       "      <td>338</td>\n",
       "      <td>Data Processing</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>C</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>101</td>\n",
       "      <td>Data and Statistics</td>\n",
       "      <td>338</td>\n",
       "      <td>Data Processing</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>D</td>\n",
       "      <td>Neither is correct</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>101</td>\n",
       "      <td>Data and Statistics</td>\n",
       "      <td>338</td>\n",
       "      <td>Data Processing</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer  QuestionId  ConstructId  \\\n",
       "0            1869_B        1869          856   \n",
       "1            1869_C        1869          856   \n",
       "2            1869_D        1869          856   \n",
       "3            1870_A        1870         1612   \n",
       "4            1870_B        1870         1612   \n",
       "5            1870_C        1870         1612   \n",
       "6            1871_A        1871         2774   \n",
       "7            1871_C        1871         2774   \n",
       "8            1871_D        1871         2774   \n",
       "\n",
       "                                       ConstructName  SubjectId  \\\n",
       "0  Use the order of operations to carry out calcu...         33   \n",
       "1  Use the order of operations to carry out calcu...         33   \n",
       "2  Use the order of operations to carry out calcu...         33   \n",
       "3  Simplify an algebraic fraction by factorising ...       1077   \n",
       "4  Simplify an algebraic fraction by factorising ...       1077   \n",
       "5  Simplify an algebraic fraction by factorising ...       1077   \n",
       "6            Calculate the range from a list of data        339   \n",
       "7            Calculate the range from a list of data        339   \n",
       "8            Calculate the range from a list of data        339   \n",
       "\n",
       "                                         SubjectName  \\\n",
       "0                                             BIDMAS   \n",
       "1                                             BIDMAS   \n",
       "2                                             BIDMAS   \n",
       "3                    Simplifying Algebraic Fractions   \n",
       "4                    Simplifying Algebraic Fractions   \n",
       "5                    Simplifying Algebraic Fractions   \n",
       "6  Range and Interquartile Range from a List of Data   \n",
       "7  Range and Interquartile Range from a List of Data   \n",
       "8  Range and Interquartile Range from a List of Data   \n",
       "\n",
       "                                        QuestionText Answer  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...      B   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...      C   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...      D   \n",
       "3  Simplify the following, if possible: \\( \\frac{...      A   \n",
       "4  Simplify the following, if possible: \\( \\frac{...      B   \n",
       "5  Simplify the following, if possible: \\( \\frac{...      C   \n",
       "6  Tom and Katie are discussing the \\( 5 \\) plant...      A   \n",
       "7  Tom and Katie are discussing the \\( 5 \\) plant...      C   \n",
       "8  Tom and Katie are discussing the \\( 5 \\) plant...      D   \n",
       "\n",
       "               AnswerText  Correct CorrectAnswer      CorrectAnswerText  \\\n",
       "0  \\( 3 \\times 2+(4-5) \\)        0             A  \\( 3 \\times(2+4)-5 \\)   \n",
       "1   \\( 3 \\times(2+4-5) \\)        0             A  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  Does not need brackets        0             A  \\( 3 \\times(2+4)-5 \\)   \n",
       "3               \\( m+1 \\)        0             D      Does not simplify   \n",
       "4               \\( m+2 \\)        0             D      Does not simplify   \n",
       "5               \\( m-1 \\)        0             D      Does not simplify   \n",
       "6               Only\\nTom        0             B            Only\\nKatie   \n",
       "7      Both Tom and Katie        0             B            Only\\nKatie   \n",
       "8      Neither is correct        0             B            Only\\nKatie   \n",
       "\n",
       "   FirstSubjectId     FirstSubjectName  SecondSubjectId    SecondSubjectName  \\\n",
       "0              32               Number              144     Basic Arithmetic   \n",
       "1              32               Number              144     Basic Arithmetic   \n",
       "2              32               Number              144     Basic Arithmetic   \n",
       "3              49              Algebra              255  Algebraic Fractions   \n",
       "4              49              Algebra              255  Algebraic Fractions   \n",
       "5              49              Algebra              255  Algebraic Fractions   \n",
       "6             101  Data and Statistics              338      Data Processing   \n",
       "7             101  Data and Statistics              338      Data Processing   \n",
       "8             101  Data and Statistics              338      Data Processing   \n",
       "\n",
       "   ThirdSubjectId                                   ThirdSubjectName  \\\n",
       "0              33                                             BIDMAS   \n",
       "1              33                                             BIDMAS   \n",
       "2              33                                             BIDMAS   \n",
       "3            1077                    Simplifying Algebraic Fractions   \n",
       "4            1077                    Simplifying Algebraic Fractions   \n",
       "5            1077                    Simplifying Algebraic Fractions   \n",
       "6             339  Range and Interquartile Range from a List of Data   \n",
       "7             339  Range and Interquartile Range from a List of Data   \n",
       "8             339  Range and Interquartile Range from a List of Data   \n",
       "\n",
       "   MisconceptionId  \n",
       "0               -1  \n",
       "1               -1  \n",
       "2               -1  \n",
       "3               -1  \n",
       "4               -1  \n",
       "5               -1  \n",
       "6               -1  \n",
       "7               -1  \n",
       "8               -1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/tmp/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba3852",
   "metadata": {
    "papermill": {
     "duration": 0.019699,
     "end_time": "2024-12-11T18:32:50.246027",
     "exception": false,
     "start_time": "2024-12-11T18:32:50.226328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# rihanpiggy part(misunderstanding CoT infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66609a02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:50.285635Z",
     "iopub.status.busy": "2024-12-11T18:32:50.285379Z",
     "iopub.status.idle": "2024-12-11T18:32:50.295251Z",
     "shell.execute_reply": "2024-12-11T18:32:50.294685Z"
    },
    "papermill": {
     "duration": 0.031466,
     "end_time": "2024-12-11T18:32:50.296855",
     "exception": false,
     "start_time": "2024-12-11T18:32:50.265389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_filepath = '/tmp/test.csv'\n",
    "exp_name = 'p000-qwen25-32b-instruct-cot'\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "if is_true_test:\n",
    "    !cd /kaggle/input/eedi-source-comp/eedi-kaggle/eedi/training && python3 run_misunderstanding_cot_infer.py --filepath $test_filepath \\\n",
    "                                                                    --exp-name $exp_name --model-path $model_path --output-dir $output_dir\n",
    "else:\n",
    "    df_tmp = pd.read_csv(test_filepath)\n",
    "    df_tmp[f'{exp_name}_misunderstanding'] = 'dummy'\n",
    "    df_tmp.to_csv(test_filepath,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb54ba53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:50.338318Z",
     "iopub.status.busy": "2024-12-11T18:32:50.337933Z",
     "iopub.status.idle": "2024-12-11T18:32:50.359253Z",
     "shell.execute_reply": "2024-12-11T18:32:50.358311Z"
    },
    "papermill": {
     "duration": 0.043787,
     "end_time": "2024-12-11T18:32:50.361035",
     "exception": false,
     "start_time": "2024-12-11T18:32:50.317248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectId</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>Answer</th>\n",
       "      <th>AnswerText</th>\n",
       "      <th>Correct</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>CorrectAnswerText</th>\n",
       "      <th>FirstSubjectId</th>\n",
       "      <th>FirstSubjectName</th>\n",
       "      <th>SecondSubjectId</th>\n",
       "      <th>SecondSubjectName</th>\n",
       "      <th>ThirdSubjectId</th>\n",
       "      <th>ThirdSubjectName</th>\n",
       "      <th>MisconceptionId</th>\n",
       "      <th>p000-qwen25-32b-instruct-cot_misunderstanding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>B</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>32</td>\n",
       "      <td>Number</td>\n",
       "      <td>144</td>\n",
       "      <td>Basic Arithmetic</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>-1</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>C</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>32</td>\n",
       "      <td>Number</td>\n",
       "      <td>144</td>\n",
       "      <td>Basic Arithmetic</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>-1</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>D</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>32</td>\n",
       "      <td>Number</td>\n",
       "      <td>144</td>\n",
       "      <td>Basic Arithmetic</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>-1</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>A</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>49</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>255</td>\n",
       "      <td>Algebraic Fractions</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>-1</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>B</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>49</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>255</td>\n",
       "      <td>Algebraic Fractions</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>-1</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>C</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>49</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>255</td>\n",
       "      <td>Algebraic Fractions</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>-1</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>A</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>101</td>\n",
       "      <td>Data and Statistics</td>\n",
       "      <td>338</td>\n",
       "      <td>Data Processing</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>-1</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>C</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>101</td>\n",
       "      <td>Data and Statistics</td>\n",
       "      <td>338</td>\n",
       "      <td>Data Processing</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>-1</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>D</td>\n",
       "      <td>Neither is correct</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>101</td>\n",
       "      <td>Data and Statistics</td>\n",
       "      <td>338</td>\n",
       "      <td>Data Processing</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>-1</td>\n",
       "      <td>dummy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer  QuestionId  ConstructId  \\\n",
       "0            1869_B        1869          856   \n",
       "1            1869_C        1869          856   \n",
       "2            1869_D        1869          856   \n",
       "3            1870_A        1870         1612   \n",
       "4            1870_B        1870         1612   \n",
       "5            1870_C        1870         1612   \n",
       "6            1871_A        1871         2774   \n",
       "7            1871_C        1871         2774   \n",
       "8            1871_D        1871         2774   \n",
       "\n",
       "                                       ConstructName  SubjectId  \\\n",
       "0  Use the order of operations to carry out calcu...         33   \n",
       "1  Use the order of operations to carry out calcu...         33   \n",
       "2  Use the order of operations to carry out calcu...         33   \n",
       "3  Simplify an algebraic fraction by factorising ...       1077   \n",
       "4  Simplify an algebraic fraction by factorising ...       1077   \n",
       "5  Simplify an algebraic fraction by factorising ...       1077   \n",
       "6            Calculate the range from a list of data        339   \n",
       "7            Calculate the range from a list of data        339   \n",
       "8            Calculate the range from a list of data        339   \n",
       "\n",
       "                                         SubjectName  \\\n",
       "0                                             BIDMAS   \n",
       "1                                             BIDMAS   \n",
       "2                                             BIDMAS   \n",
       "3                    Simplifying Algebraic Fractions   \n",
       "4                    Simplifying Algebraic Fractions   \n",
       "5                    Simplifying Algebraic Fractions   \n",
       "6  Range and Interquartile Range from a List of Data   \n",
       "7  Range and Interquartile Range from a List of Data   \n",
       "8  Range and Interquartile Range from a List of Data   \n",
       "\n",
       "                                        QuestionText Answer  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...      B   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...      C   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...      D   \n",
       "3  Simplify the following, if possible: \\( \\frac{...      A   \n",
       "4  Simplify the following, if possible: \\( \\frac{...      B   \n",
       "5  Simplify the following, if possible: \\( \\frac{...      C   \n",
       "6  Tom and Katie are discussing the \\( 5 \\) plant...      A   \n",
       "7  Tom and Katie are discussing the \\( 5 \\) plant...      C   \n",
       "8  Tom and Katie are discussing the \\( 5 \\) plant...      D   \n",
       "\n",
       "               AnswerText  Correct CorrectAnswer      CorrectAnswerText  \\\n",
       "0  \\( 3 \\times 2+(4-5) \\)        0             A  \\( 3 \\times(2+4)-5 \\)   \n",
       "1   \\( 3 \\times(2+4-5) \\)        0             A  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  Does not need brackets        0             A  \\( 3 \\times(2+4)-5 \\)   \n",
       "3               \\( m+1 \\)        0             D      Does not simplify   \n",
       "4               \\( m+2 \\)        0             D      Does not simplify   \n",
       "5               \\( m-1 \\)        0             D      Does not simplify   \n",
       "6               Only\\nTom        0             B            Only\\nKatie   \n",
       "7      Both Tom and Katie        0             B            Only\\nKatie   \n",
       "8      Neither is correct        0             B            Only\\nKatie   \n",
       "\n",
       "   FirstSubjectId     FirstSubjectName  SecondSubjectId    SecondSubjectName  \\\n",
       "0              32               Number              144     Basic Arithmetic   \n",
       "1              32               Number              144     Basic Arithmetic   \n",
       "2              32               Number              144     Basic Arithmetic   \n",
       "3              49              Algebra              255  Algebraic Fractions   \n",
       "4              49              Algebra              255  Algebraic Fractions   \n",
       "5              49              Algebra              255  Algebraic Fractions   \n",
       "6             101  Data and Statistics              338      Data Processing   \n",
       "7             101  Data and Statistics              338      Data Processing   \n",
       "8             101  Data and Statistics              338      Data Processing   \n",
       "\n",
       "   ThirdSubjectId                                   ThirdSubjectName  \\\n",
       "0              33                                             BIDMAS   \n",
       "1              33                                             BIDMAS   \n",
       "2              33                                             BIDMAS   \n",
       "3            1077                    Simplifying Algebraic Fractions   \n",
       "4            1077                    Simplifying Algebraic Fractions   \n",
       "5            1077                    Simplifying Algebraic Fractions   \n",
       "6             339  Range and Interquartile Range from a List of Data   \n",
       "7             339  Range and Interquartile Range from a List of Data   \n",
       "8             339  Range and Interquartile Range from a List of Data   \n",
       "\n",
       "   MisconceptionId p000-qwen25-32b-instruct-cot_misunderstanding  \n",
       "0               -1                                         dummy  \n",
       "1               -1                                         dummy  \n",
       "2               -1                                         dummy  \n",
       "3               -1                                         dummy  \n",
       "4               -1                                         dummy  \n",
       "5               -1                                         dummy  \n",
       "6               -1                                         dummy  \n",
       "7               -1                                         dummy  \n",
       "8               -1                                         dummy  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/tmp/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5eaba1",
   "metadata": {
    "papermill": {
     "duration": 0.020396,
     "end_time": "2024-12-11T18:32:50.407976",
     "exception": false,
     "start_time": "2024-12-11T18:32:50.387580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd67b996",
   "metadata": {
    "papermill": {
     "duration": 0.019977,
     "end_time": "2024-12-11T18:32:50.447811",
     "exception": false,
     "start_time": "2024-12-11T18:32:50.427834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# rihanpiggy part(Retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2fa04ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:50.488576Z",
     "iopub.status.busy": "2024-12-11T18:32:50.487994Z",
     "iopub.status.idle": "2024-12-11T18:32:50.493905Z",
     "shell.execute_reply": "2024-12-11T18:32:50.493248Z"
    },
    "papermill": {
     "duration": 0.027821,
     "end_time": "2024-12-11T18:32:50.495412",
     "exception": false,
     "start_time": "2024-12-11T18:32:50.467591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def gather_result(output_dir):\n",
    "    fs = sorted(glob.glob(os.path.join(output_dir,'*.pkl')))\n",
    "    results = {\n",
    "        'question_id_answers': [],\n",
    "        'ids': [],\n",
    "        'embeddings': [],\n",
    "        'embeddings_mapping': [],\n",
    "        'cosine_similarity': [],\n",
    "    }\n",
    "    for i,path in enumerate(fs):\n",
    "        res_d = pd.read_pickle(path)\n",
    "        results['question_id_answers'].append(res_d['question_id_answers'])\n",
    "        if i==0:\n",
    "            results['ids'].append(res_d['ids'])\n",
    "            results['embeddings_mapping'].append(res_d['embeddings_mapping'])\n",
    "        results['embeddings'].append(res_d['embeddings'])\n",
    "        results['cosine_similarity'].append(res_d['cosine_similarity'])\n",
    "    for k,v in results.items():\n",
    "        results[k] = np.concatenate(v)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27aae208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:32:50.536534Z",
     "iopub.status.busy": "2024-12-11T18:32:50.536261Z",
     "iopub.status.idle": "2024-12-11T18:35:06.087647Z",
     "shell.execute_reply": "2024-12-11T18:35:06.086813Z"
    },
    "papermill": {
     "duration": 135.574489,
     "end_time": "2024-12-11T18:35:06.089657",
     "exception": false,
     "start_time": "2024-12-11T18:32:50.515168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_type='misconception-retrieve' exp_name='e032-mistral-synthetic-gen1-gen2-gen3' seed=42 n_folds=5 num_classes=2587 max_epochs=60 check_val_every_n_epoch=1 sync_batchnorm=True use_lora=True precision='16-mixed' num_workers=16 train_batch_size=1 accumulate_grad_batchs=6 valid_batch_size=12 test_batch_size=8 dataset_config=DatasetConfig(name='EediClsDatasetV2', prompt_name='mistral', num_classes=2587, params={'use_negative_samples': False, 'synthetic_negative': False, 'max_tokens': 768, 'p_synthetic_exist': 0.43, 'p_other_exist': 0.3, 'p_other_generate': 0.3}) llm_config=LLMConfig(name='EediClsV2', backbone='Linq-AI-Research/Linq-Embed-Mistral', num_classes=2587, use_metrics_learning=True, metrics_learning_module=MetricsLearningModule(module_name='ArcFace', params={'params1': {'ratio': 1.0, 's': 20.0, 'm': 0.3}}), cls_head=True, seq_head=True, params={}) loss_config=LossConfig(name='Seq2SeqCls2wayLoss', params={'seq_weight': 0.5, 'cls_weight': 0.5}) lr_scheduler_config=LRSchedulerConfig(name='cosine_schedule_with_warmup', interval='step', params={'num_cycles': 0.5, 'max_epochs': 60.0, 'warmup_steps_ratio': 0.1}) optimizer_config=OptimizerConfig(name='AdamW', params={'lr': 4e-05, 'weight_decay': 0.001}) logger='wandb'\r\n",
      "> SEEDING DONE\r\n",
      "overwriting llm config... backbone: Linq-AI-Research/Linq-Embed-Mistral -> /kaggle/input/linq-embed-mistral/transformers/default/1\r\n",
      "overwriting params... test_batch_size: 8 -> 8\r\n",
      "overwriting params... num_workers: 16 -> 4\r\n",
      "Test size: 9\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [01:40<00:00, 33.55s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:06<00:00,  2.18s/it]\r\n",
      "loaded model from /kaggle/input/e032-mistral-synthetic-gen123-full-syn03-ep60/e032-mistral-synthetic-gen1-gen2-gen3_best_fold100\r\n",
      "PromptLoader: mistral\r\n",
      "PromptLoader: mistral\r\n",
      "question_aug_prompts: []\r\n",
      "question_aug_prompts: []\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:03<00:00,  3.24s/it]\r\n",
      "\r\n",
      " 50%|██████████████████████▌                      | 1/2 [00:03<00:03,  3.21s/it]\u001b[Asaving results to: /tmp/e032-mistral-synthetic-gen1-gen2-gen3/exist/fold100/results_e032-mistral-synthetic-gen1-gen2-gen3_rank0.pkl\r\n",
      "\r\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:04<00:00,  2.06s/it]\r\n",
      "saving results to: /tmp/e032-mistral-synthetic-gen1-gen2-gen3/exist/fold100/results_e032-mistral-synthetic-gen1-gen2-gen3_rank1.pkl\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_filepath = '/tmp/test.csv'\n",
    "model_dir = '/kaggle/input'\n",
    "# !cp '/kaggle/input/mis-data/misconception_mapping_with_paragraph_v3 (2).csv' '/tmp/misconception_mapping_with_paragraph_v3.csv'\n",
    "misconception_filepath = '/kaggle/input/eedi-misconception-mapping/misconception_mapping_with_paragraph_v3.csv'\n",
    "pretrain_model_path_gte = '/kaggle/input/eedi-pretrained-models/transformers/default/3/gte-large-en-v1.5'\n",
    "pretrain_model_path_minicpm = '/kaggle/input/minicpm-embedding/transformers/default/1'\n",
    "pretrain_model_path_mistral = '/kaggle/input/linq-embed-mistral/transformers/default/1'\n",
    "other_results_path = '/tmp/other_results.pkl'\n",
    "\n",
    "if is_true_test:\n",
    "    embedding_exp_names = {\n",
    "        'e032-mistral-synthetic-gen1-gen2-gen3': [\"e032-mistral-synthetic-gen123-full-syn03-ep60\", pretrain_model_path_mistral, [100], False, False, True], # folds\n",
    "\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    embedding_exp_names = {\n",
    "        'e032-mistral-synthetic-gen1-gen2-gen3': [\"e032-mistral-synthetic-gen123-full-syn03-ep60\", pretrain_model_path_mistral, [100], False, False, True], # folds\n",
    "    }\n",
    "\n",
    "all_results_exist = {}\n",
    "all_results_not_exist = {}\n",
    "for key, (model_path_name, pretrain_model_path, folds, use_not_exist, use_ddp, is_lora) in embedding_exp_names.items():\n",
    "    all_results_exist[key] = {}\n",
    "    all_results_not_exist[key] = {}\n",
    "\n",
    "    exp_name = key.replace('_rev','')\n",
    "    mapping_embeddings = np.load(os.path.join(model_dir,model_path_name,f'{exp_name}_mapping_embedding.npz'))\n",
    "    ids = mapping_embeddings['ids']\n",
    "    for fold in folds:\n",
    "        model_exp_dir = os.path.join(model_dir,model_path_name)\n",
    "        config_path = os.path.join('/kaggle/input/eedi-source-comp/eedi-kaggle/eedi/training/config',exp_name+'.yaml')\n",
    "        if not is_lora:\n",
    "            model_path_exist = os.path.join(model_exp_dir,f'{exp_name}_best_fold{fold}.pth')\n",
    "            model_path_not_exist = os.path.join(model_exp_dir,f'{exp_name}_best_not_exist_fold{fold}.pth')\n",
    "        else:\n",
    "            model_path_exist = os.path.join(model_exp_dir,f'{exp_name}_best_fold{fold}')\n",
    "            model_path_not_exist = os.path.join(model_exp_dir,f'{exp_name}_best_not_exist_fold{fold}')\n",
    "            \n",
    "        model_exist_output_dir = os.path.join(output_dir,key,'exist', f'fold{fold}')\n",
    "        model_not_exist_output_dir = os.path.join(output_dir,key,'not_exist', f'fold{fold}')\n",
    "        \n",
    "        embeddings_mapping = mapping_embeddings[f'embedding_mapping_exist_fold{fold}']\n",
    "        np.savez_compressed('/tmp/artifact',ids=ids, embeddings_mapping=embeddings_mapping)\n",
    "        mapping_embeddings_fold_path = '/tmp/artifact.npz'\n",
    "        if use_ddp:\n",
    "            !cd /kaggle/input/eedi-source-comp/eedi-kaggle/eedi/training && torchrun \\\n",
    "                --nproc_per_node=2 run_inference.py \\\n",
    "                --config $config_path \\\n",
    "                --filepath $test_filepath \\\n",
    "                --misconception-filepath $misconception_filepath \\\n",
    "                --mapping-embedding-path $mapping_embeddings_fold_path \\\n",
    "                --pretrain-model-path $pretrain_model_path \\\n",
    "                --model-path $model_path_exist \\\n",
    "                --output-dir $model_exist_output_dir \\\n",
    "                --ddp\n",
    "        else:\n",
    "            !cd /kaggle/input/eedi-source-comp/eedi-kaggle/eedi/training && python3 run_inference.py \\\n",
    "                --config $config_path \\\n",
    "                --filepath $test_filepath \\\n",
    "                --misconception-filepath $misconception_filepath \\\n",
    "                --mapping-embedding-path $mapping_embeddings_fold_path \\\n",
    "                --pretrain-model-path $pretrain_model_path \\\n",
    "                --model-path $model_path_exist \\\n",
    "                --output-dir $model_exist_output_dir\n",
    "\n",
    "        result_gathered = gather_result(model_exist_output_dir)\n",
    "        all_results_exist[key][fold] = result_gathered\n",
    "            \n",
    "        if use_not_exist:\n",
    "            embeddings_mapping = mapping_embeddings[f'embedding_mapping_not_exist_fold{fold}']\n",
    "            np.savez_compressed('/tmp/artifact',ids=ids, embeddings_mapping=embeddings_mapping)\n",
    "            mapping_embeddings_fold_path = '/tmp/artifact.npz'\n",
    "            \n",
    "            if use_ddp:\n",
    "                !cd /kaggle/input/eedi-source-comp/eedi-kaggle/eedi/training && torchrun \\\n",
    "                    --nproc_per_node=2 run_inference.py \\\n",
    "                    --config $config_path \\\n",
    "                    --filepath $test_filepath \\\n",
    "                    --misconception-filepath $misconception_filepath \\\n",
    "                    --mapping-embedding-path $mapping_embeddings_fold_path \\\n",
    "                    --pretrain-model-path $pretrain_model_path \\\n",
    "                    --model-path $model_path_not_exist \\\n",
    "                    --output-dir $model_not_exist_output_dir \\\n",
    "                    --ddp\n",
    "            else:\n",
    "                !cd /kaggle/input/eedi-source-comp/eedi-kaggle/eedi/training && python3 run_inference.py \\\n",
    "                    --config $config_path \\\n",
    "                    --filepath $test_filepath \\\n",
    "                    --misconception-filepath $misconception_filepath \\\n",
    "                    --mapping-embedding-path $mapping_embeddings_fold_path \\\n",
    "                    --pretrain-model-path $pretrain_model_path \\\n",
    "                    --model-path $model_path_not_exist \\\n",
    "                    --output-dir $model_not_exist_output_dir\n",
    "            result_gathered = gather_result(model_not_exist_output_dir)\n",
    "            all_results_not_exist[key][fold] = result_gathered\n",
    "\n",
    "del result_gathered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1089c342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:35:06.133337Z",
     "iopub.status.busy": "2024-12-11T18:35:06.132746Z",
     "iopub.status.idle": "2024-12-11T18:35:06.146677Z",
     "shell.execute_reply": "2024-12-11T18:35:06.146104Z"
    },
    "papermill": {
     "duration": 0.037234,
     "end_time": "2024-12-11T18:35:06.148181",
     "exception": false,
     "start_time": "2024-12-11T18:35:06.110947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from eedi.training.semantic_search import SemanticSearcher\n",
    "\n",
    "\n",
    "def gather_all_results():\n",
    "\n",
    "    question_id_answers = None\n",
    "    embeddings_exist = {}\n",
    "    embeddings_mapping_exist = {}\n",
    "    \n",
    "    ids = None\n",
    "    total_len = len(pd.read_csv('/tmp/test.csv'))\n",
    "    for key, exp_results in all_results_exist.items():\n",
    "        embedding = None\n",
    "        embedding_mapping = None\n",
    "        num_fold = len(exp_results)\n",
    "        embeddings_exist_key = []\n",
    "        embeddings_mapping_exist_key = []\n",
    "        for fold, exp_result_fold in exp_results.items():\n",
    "            question_id_answers_fold_exp = exp_result_fold['question_id_answers'][:total_len]\n",
    "            question_id_answers_idx_sort = np.argsort(question_id_answers_fold_exp)\n",
    "            question_id_answers_fold_exp = question_id_answers_fold_exp[question_id_answers_idx_sort]\n",
    "            \n",
    "            if question_id_answers is None:\n",
    "                question_id_answers = question_id_answers_fold_exp\n",
    "            if ids is None:\n",
    "                ids = exp_result_fold['ids']\n",
    "    \n",
    "            embedding = exp_result_fold['embeddings'][:total_len][question_id_answers_idx_sort]\n",
    "    \n",
    "            embedding_mapping = exp_result_fold['embeddings_mapping']\n",
    "    \n",
    "            embeddings_exist_key.append(embedding)\n",
    "            embeddings_mapping_exist_key.append(embedding_mapping)\n",
    "        embeddings_exist_key = np.concatenate(embeddings_exist_key,axis=1)\n",
    "        embeddings_mapping_exist_key = np.concatenate(embeddings_mapping_exist_key,axis=1)\n",
    "        embeddings_exist[key] = embeddings_exist_key\n",
    "        embeddings_mapping_exist[key] = embeddings_mapping_exist_key\n",
    "    \n",
    "        \n",
    "#    for key, exp_results in all_results_not_exist.items():\n",
    "#        embedding = None\n",
    "#        embedding_mapping = None\n",
    "#        num_fold = len(exp_results)\n",
    "#        for fold, exp_result_fold in exp_results.items():\n",
    "#            question_id_answers_fold_exp = exp_result_fold['question_id_answers'][:total_len]\n",
    "#            question_id_answers_idx_sort = np.argsort(question_id_answers_fold_exp)\n",
    "#            question_id_answers_fold_exp = question_id_answers_fold_exp[question_id_answers_idx_sort]\n",
    "            \n",
    "#            if question_id_answers is None:\n",
    "#                question_id_answers = question_id_answers_fold_exp\n",
    "#            if ids is None:\n",
    "#                ids = exp_result_fold['ids']\n",
    "    \n",
    "#            embedding = exp_result_fold['embeddings'][:total_len][question_id_answers_idx_sort]\n",
    "    \n",
    "#            embedding_mapping = exp_result_fold['embeddings_mapping']\n",
    "    \n",
    "#            embeddings_not_exist.append(embedding)\n",
    "#            embeddings_mapping_not_exist.append(embedding_mapping)\n",
    "    \n",
    "#    has_not_exist = False\n",
    "#    if len(embeddings_mapping_not_exist) > 0:\n",
    "#        has_not_exist = True\n",
    "#        embeddings_not_exist = np.concatenate(embeddings_not_exist,axis=1)\n",
    "#        embeddings_mapping_not_exist = np.concatenate(embeddings_mapping_not_exist,axis=1)\n",
    "    \n",
    "    #print(embeddings_exist.shape, embeddings_not_exist.shape)\n",
    "\n",
    "#    if has_not_exist:\n",
    "#        embeddings = np.concatenate([embeddings_exist, embeddings_not_exist],axis=1)\n",
    "#        embeddings_mapping = np.concatenate([embeddings_mapping_exist, embeddings_mapping_not_exist],axis=1)\n",
    "#    else:\n",
    "#        embeddings = embeddings_exist\n",
    "#        embeddings_mapping = embeddings_mapping_exist\n",
    "    for key in embeddings_exist.keys():\n",
    "        embeddings = embeddings_exist[key]\n",
    "        embeddings_mapping = embeddings_mapping_exist[key]\n",
    "        np.savez_compressed(f'/tmp/rihanpiggy_embedding_{key}',question_id_answers=question_id_answers, embeddings=embeddings, embeddings_mapping=embeddings_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c663838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:35:06.190718Z",
     "iopub.status.busy": "2024-12-11T18:35:06.190040Z",
     "iopub.status.idle": "2024-12-11T18:35:08.385719Z",
     "shell.execute_reply": "2024-12-11T18:35:08.384752Z"
    },
    "papermill": {
     "duration": 2.219108,
     "end_time": "2024-12-11T18:35:08.387798",
     "exception": false,
     "start_time": "2024-12-11T18:35:06.168690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gather_all_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2979241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:35:08.431000Z",
     "iopub.status.busy": "2024-12-11T18:35:08.430682Z",
     "iopub.status.idle": "2024-12-11T18:35:08.497225Z",
     "shell.execute_reply": "2024-12-11T18:35:08.496481Z"
    },
    "papermill": {
     "duration": 0.08982,
     "end_time": "2024-12-11T18:35:08.498844",
     "exception": false,
     "start_time": "2024-12-11T18:35:08.409024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import gc\n",
    "del all_results_exist\n",
    "del all_results_not_exist\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01dff2",
   "metadata": {
    "papermill": {
     "duration": 0.020729,
     "end_time": "2024-12-11T18:35:08.540997",
     "exception": false,
     "start_time": "2024-12-11T18:35:08.520268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Copasta Part(Retrieve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a6d06",
   "metadata": {
    "papermill": {
     "duration": 0.034497,
     "end_time": "2024-12-11T18:35:08.596366",
     "exception": false,
     "start_time": "2024-12-11T18:35:08.561869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37b752fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:35:08.663168Z",
     "iopub.status.busy": "2024-12-11T18:35:08.662839Z",
     "iopub.status.idle": "2024-12-11T18:35:09.348539Z",
     "shell.execute_reply": "2024-12-11T18:35:09.347814Z"
    },
    "papermill": {
     "duration": 0.715945,
     "end_time": "2024-12-11T18:35:09.350487",
     "exception": false,
     "start_time": "2024-12-11T18:35:08.634542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "\n",
    "df_tmp = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "misconception_mapping = pd.read_csv(f\"{DATA_PATH}/misconception_mapping.csv\")\n",
    "\n",
    "df_answer = df_tmp[\n",
    "    [\"QuestionId\"] +\n",
    "    [f\"Answer{alpha}Text\" for alpha in [\"A\", \"B\", \"C\", \"D\"]]\n",
    "    # [f\"Misconception{alpha}Id\" for alpha in [\"A\", \"B\", \"C\", \"D\"]]\n",
    "].set_index(\"QuestionId\").stack().reset_index()\n",
    "df_answer.columns = [\"QuestionId\", \"AnswerType\", \"AnswerText\"]\n",
    "df_answer[\"AnswerAlphabet\"] = df_answer[\"AnswerType\"].map(lambda x:x.replace(\"Answer\", \"\").replace(\"Text\", \"\"))\n",
    "df_answer[\"QuestionId_Answer\"] = df_answer[\"QuestionId\"].map(str) + \"_\" + df_answer[\"AnswerAlphabet\"]\n",
    "\n",
    "common_col = [\n",
    "    \"QuestionId\",\n",
    "    \"ConstructId\",\n",
    "    \"ConstructName\",\n",
    "    \"SubjectId\",\n",
    "    \"SubjectName\",\n",
    "    \"QuestionText\",\n",
    "    \"CorrectAnswer\",\n",
    "]\n",
    "df = df_answer.merge(\n",
    "    df_tmp[common_col],\n",
    "    on=\"QuestionId\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df = df.merge(\n",
    "    df_answer[[\"QuestionId\", \"AnswerAlphabet\", \"AnswerText\"]],\n",
    "    left_on=[\"QuestionId\", \"CorrectAnswer\"],\n",
    "    right_on=[\"QuestionId\", \"AnswerAlphabet\"],\n",
    "    suffixes=(\"\",\"Correct\"),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df = df[df[\"AnswerAlphabet\"] != df[\"CorrectAnswer\"]].reset_index(drop=True)\n",
    "\n",
    "df.to_csv(\"/tmp/preprocess.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780cb7a",
   "metadata": {
    "papermill": {
     "duration": 0.021122,
     "end_time": "2024-12-11T18:35:09.393652",
     "exception": false,
     "start_time": "2024-12-11T18:35:09.372530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3a2b88c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:35:09.437600Z",
     "iopub.status.busy": "2024-12-11T18:35:09.437255Z",
     "iopub.status.idle": "2024-12-11T18:36:58.173502Z",
     "shell.execute_reply": "2024-12-11T18:36:58.172773Z"
    },
    "papermill": {
     "duration": 108.760861,
     "end_time": "2024-12-11T18:36:58.175444",
     "exception": false,
     "start_time": "2024-12-11T18:35:09.414583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.6.3.post1 requires transformers>=4.45.2, but you have transformers 4.45.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.37s/it]\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:08<00:00,  8.60s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "\n",
    "import os\n",
    "os.system('python -m pip install -qq --no-index --find-links=/kaggle/input/eedi-library-qwen45 transformers==4.45.0 sentence-transformers')\n",
    "os.system('python -m pip install -qq --no-index --find-links=/kaggle/input/eedi-library bitsandbytes')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "from tqdm.autonotebook import trange\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "\n",
    "from transformers import Qwen2Model, Qwen2ForCausalLM, Qwen2PreTrainedModel, Qwen2Config\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.models.qwen2.modeling_qwen2 import (\n",
    "    Qwen2DecoderLayer,\n",
    "    Qwen2RMSNorm,\n",
    "    Qwen2Attention,\n",
    "    Qwen2FlashAttention2,\n",
    "    Qwen2SdpaAttention,\n",
    "    Qwen2MLP,\n",
    "    Qwen2RotaryEmbedding\n",
    ")\n",
    "from torch import nn\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "class ModifiedQwen2Attention(Qwen2Attention):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_causal = False\n",
    "\n",
    "\n",
    "class ModifiedQwen2FlashAttention2(Qwen2FlashAttention2):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_causal = False\n",
    "\n",
    "\n",
    "class ModifiedQwen2SdpaAttention(Qwen2SdpaAttention):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_causal = False\n",
    "\n",
    "\n",
    "QWEN2_ATTENTION_CLASSES = {\n",
    "    \"eager\": ModifiedQwen2Attention,\n",
    "    \"flash_attention_2\": ModifiedQwen2FlashAttention2,\n",
    "    \"sdpa\": ModifiedQwen2SdpaAttention,\n",
    "}\n",
    "\n",
    "\n",
    "class ModifiedQwen2DecoderLayer(Qwen2DecoderLayer):\n",
    "    def __init__(self, config: Qwen2Config, layer_idx: int):\n",
    "        nn.Module.__init__(self)\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = QWEN2_ATTENTION_CLASSES[config._attn_implementation](\n",
    "            config=config, layer_idx=layer_idx\n",
    "        )\n",
    "\n",
    "        self.mlp = Qwen2MLP(config)\n",
    "        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Qwen2RMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps\n",
    "        )\n",
    "\n",
    "\n",
    "class Qwen2BiModel(Qwen2Model):\n",
    "    _no_split_modules = [\"ModifiedQwen2DecoderLayer\"]\n",
    "\n",
    "    def __init__(self, config: Qwen2Config):\n",
    "        Qwen2PreTrainedModel.__init__(self, config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, self.padding_idx\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                ModifiedQwen2DecoderLayer(config, layer_idx)\n",
    "                for layer_idx in range(config.num_hidden_layers)\n",
    "            ]\n",
    "        )\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = Qwen2RotaryEmbedding(config=config)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "class Qwen2BiForMNTP(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        Qwen2PreTrainedModel.__init__(self, config)\n",
    "        self.model = Qwen2BiModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    # getter for PEFT model\n",
    "    def get_model_for_peft(self):\n",
    "        return self.model\n",
    "\n",
    "    # setter for PEFT model\n",
    "    def set_model_for_peft(self, model: PeftModel):\n",
    "        self.model = model\n",
    "\n",
    "    # save the PEFT model\n",
    "    def save_peft_model(self, path):\n",
    "        self.model.save_pretrained(path)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/tmp/preprocess.csv\")\n",
    "misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "task_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception.'\n",
    "\n",
    "def get_query_text(row):\n",
    "    query_text = f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{row['AnswerTextCorrect']}\\n###Misconcepte Incorrect answer###:{row['AnswerText']}\"\n",
    "    return query_text\n",
    "\n",
    "df[\"InputQuery\"] = df.apply(get_query_text, axis=1)\n",
    "df['query_text'] = df[\"InputQuery\"].map(lambda x: get_detailed_instruct(task_description, x))\n",
    "\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 16\n",
    "    max_length = 2048\n",
    "    sentences = list(df['query_text'].values)\n",
    "\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    return df, np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "\n",
    "device_0 = torch.device('cuda:0')\n",
    "device_1 = torch.device('cuda:1')\n",
    "print(device_0, device_1)\n",
    "\n",
    "MODEL_PATH = \"/kaggle/input/eedi-colab-exp017-full\"\n",
    "tokenizer_0 = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model_0 = Qwen2BiModel.from_pretrained(MODEL_PATH, use_cache=False, device_map=device_0)\n",
    "model_0 = model_0.eval()\n",
    "\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model_1 = Qwen2BiModel.from_pretrained(MODEL_PATH, use_cache=False, device_map=device_1)\n",
    "model_1 = model_1.eval()\n",
    "\n",
    "df[\"length\"] = df[\"query_text\"].apply(len)\n",
    "df = df.sort_values(\"length\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "sub_1 = df.iloc[0::2].copy()\n",
    "sub_2 = df.iloc[1::2].copy()\n",
    "\n",
    "# V_answer = inference(df, model, tokenizer, device)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (tokenizer_0, tokenizer_1), (device_0, device_1))\n",
    "\n",
    "results_list = list(results)\n",
    "\n",
    "df = pd.concat([r[0] for r in results_list])\n",
    "V_answer = np.concatenate([r[1] for r in results_list], axis=0)\n",
    "\n",
    "# misconception_mapping[\"query_text\"] = misconception_mapping[\"MisconceptionName\"]\n",
    "# V_misconception = inference(misconception_mapping, model, tokenizer, device)\n",
    "V_misconception = np.load(f\"{MODEL_PATH}/misconception_mapping_vec.npy\")\n",
    "\n",
    "question_id_answers = df.apply(lambda x:'{}_{}'.format(x['QuestionId'], x['AnswerAlphabet']), axis=1).values\n",
    "question_id_answers_idx_sort = np.argsort(question_id_answers)\n",
    "question_id_answers = question_id_answers[question_id_answers_idx_sort]\n",
    "V_answer = V_answer[question_id_answers_idx_sort,:]\n",
    "\n",
    "np.savez_compressed('/tmp/copasta_embedding',question_id_answers=question_id_answers, embeddings=V_answer, embeddings_mapping=V_misconception)\n",
    "#print(V_answer.shape, V_misconception.shape)\n",
    "\n",
    "#test_cos_sim_arr = cosine_similarity(V_answer, V_misconception)\n",
    "#test_sorted_indices = np.argsort(-test_cos_sim_arr, axis=1)\n",
    "\n",
    "#df[\"MisconceptionId\"] = test_sorted_indices[:, :25].tolist()\n",
    "#df.to_csv(\"st_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2064326",
   "metadata": {
    "papermill": {
     "duration": 0.021458,
     "end_time": "2024-12-11T18:36:58.219400",
     "exception": false,
     "start_time": "2024-12-11T18:36:58.197942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "182b8feb",
   "metadata": {
    "papermill": {
     "duration": 0.021337,
     "end_time": "2024-12-11T18:36:58.261798",
     "exception": false,
     "start_time": "2024-12-11T18:36:58.240461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## rihan 14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3110b880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:36:58.305511Z",
     "iopub.status.busy": "2024-12-11T18:36:58.305227Z",
     "iopub.status.idle": "2024-12-11T18:40:58.263400Z",
     "shell.execute_reply": "2024-12-11T18:40:58.262684Z"
    },
    "papermill": {
     "duration": 239.982583,
     "end_time": "2024-12-11T18:40:58.265436",
     "exception": false,
     "start_time": "2024-12-11T18:36:58.282853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.6.3.post1 requires transformers>=4.45.2, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:24<00:00, 42.19s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.42s/it]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batches: 100%|██████████| 1/1 [00:08<00:00,  8.58s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:11<00:00, 11.81s/it]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "\n",
    "import os\n",
    "os.system('python -m pip install -qq --no-index --find-links=/kaggle/input/eedi-library-llm2vec transformers==4.40.2 sentence-transformers llm2vec')\n",
    "#os.system('python -m pip install -qq --no-index --find-links=/kaggle/input/eedi-library bitsandbytes')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "from tqdm.autonotebook import trange\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from llm2vec.models import Qwen2BiModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from peft import PeftModelForFeatureExtraction\n",
    "\n",
    "df = pd.read_csv('/tmp/test.csv')\n",
    "\n",
    "def get_query_text(row):\n",
    "    task_description = f'Given a math question and an incorrect answer, please retrieve the most relevant misconception causing the incorrect answer.'\n",
    "    query_text = f\"###Question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{row['CorrectAnswerText']}\\n###Incorrect answer###:{row['AnswerText']}\"\n",
    "    return f'Instruct: {task_description}\\nQuery: {query_text}'\n",
    "\n",
    "# %%\n",
    "df['query_text'] = df.apply(lambda x: get_query_text(x), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 16\n",
    "    max_length = 2048\n",
    "    sentences = list(df['query_text'].values)\n",
    "\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    return df, np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "device_0 = torch.device('cuda:0')\n",
    "device_1 = torch.device('cuda:1')\n",
    "print(device_0, device_1)\n",
    "\n",
    "MODEL_PATH = \"/kaggle/input/qwen-14b-retriever/transformers/1/3\"\n",
    "embedding_path = '/kaggle/input/qwen-14b-retriever/transformers/1/3/qwen2.5-14b-misconception-ckpt17420.npy'\n",
    "\n",
    "tokenizer_0 = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model_0 = Qwen2BiModel.from_pretrained(MODEL_PATH, use_cache=False, device_map=device_0)\n",
    "model_0 = model_0.eval()\n",
    "\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model_1 = Qwen2BiModel.from_pretrained(MODEL_PATH, use_cache=False, device_map=device_1)\n",
    "model_1 = model_1.eval()\n",
    "\n",
    "df[\"length\"] = df[\"query_text\"].apply(len)\n",
    "df = df.sort_values(\"length\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "sub_1 = df.iloc[0::2].copy()\n",
    "sub_2 = df.iloc[1::2].copy()\n",
    "\n",
    "# V_answer = inference(df, model, tokenizer, device)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (tokenizer_0, tokenizer_1), (device_0, device_1))\n",
    "\n",
    "results_list = list(results)\n",
    "\n",
    "df = pd.concat([r[0] for r in results_list])\n",
    "V_answer = np.concatenate([r[1] for r in results_list], axis=0)\n",
    "\n",
    "V_misconception = np.load(embedding_path)\n",
    "\n",
    "question_id_answers = df['QuestionId_Answer'].values\n",
    "question_id_answers_idx_sort = np.argsort(question_id_answers)\n",
    "question_id_answers = question_id_answers[question_id_answers_idx_sort]\n",
    "V_answer = V_answer[question_id_answers_idx_sort,:]\n",
    "\n",
    "np.savez_compressed('/tmp/rihan_embedding_qwen_14b',question_id_answers=question_id_answers, embeddings=V_answer, embeddings_mapping=V_misconception)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f18f56",
   "metadata": {
    "papermill": {
     "duration": 0.021481,
     "end_time": "2024-12-11T18:40:58.310045",
     "exception": false,
     "start_time": "2024-12-11T18:40:58.288564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Retrieve Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3abb479b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:40:58.354247Z",
     "iopub.status.busy": "2024-12-11T18:40:58.353968Z",
     "iopub.status.idle": "2024-12-11T18:40:59.404535Z",
     "shell.execute_reply": "2024-12-11T18:40:59.403703Z"
    },
    "papermill": {
     "duration": 1.074873,
     "end_time": "2024-12-11T18:40:59.406455",
     "exception": false,
     "start_time": "2024-12-11T18:40:58.331582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copasta_embedding.npz\r\n",
      "rihan_embedding_qwen_14b.npz\r\n",
      "rihanpiggy_embedding_e032-mistral-synthetic-gen1-gen2-gen3.npz\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp | grep embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e15783b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:40:59.453383Z",
     "iopub.status.busy": "2024-12-11T18:40:59.452504Z",
     "iopub.status.idle": "2024-12-11T18:40:59.463688Z",
     "shell.execute_reply": "2024-12-11T18:40:59.463102Z"
    },
    "papermill": {
     "duration": 0.035685,
     "end_time": "2024-12-11T18:40:59.465276",
     "exception": false,
     "start_time": "2024-12-11T18:40:59.429591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def extract_retrieve_result(result):\n",
    "    question_id_answers = result['question_id_answers']\n",
    "    embeddings = result['embeddings']\n",
    "    embeddings_mapping = result['embeddings_mapping']\n",
    "    return question_id_answers, embeddings, embeddings_mapping\n",
    "\n",
    "def resort_retrieve_result(question_id_answers, embeddings):\n",
    "    question_id_answers_idx_sort = np.argsort(question_id_answers)\n",
    "    question_id_answers = question_id_answers[question_id_answers_idx_sort]\n",
    "    embeddings = embeddings[question_id_answers_idx_sort,:]\n",
    "    return question_id_answers, embeddings\n",
    "\n",
    "result_rihanpiggy_mistral = np.load('/tmp/rihanpiggy_embedding_e032-mistral-synthetic-gen1-gen2-gen3.npz', allow_pickle=True)\n",
    "result_copasta = np.load('/tmp/copasta_embedding.npz', allow_pickle=True)\n",
    "\n",
    "result_rihanpiggy_qwen = np.load('/tmp/rihan_embedding_qwen_14b.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cbb6e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:40:59.509909Z",
     "iopub.status.busy": "2024-12-11T18:40:59.509511Z",
     "iopub.status.idle": "2024-12-11T18:41:00.508277Z",
     "shell.execute_reply": "2024-12-11T18:41:00.507335Z"
    },
    "papermill": {
     "duration": 1.023375,
     "end_time": "2024-12-11T18:41:00.510425",
     "exception": false,
     "start_time": "2024-12-11T18:40:59.487050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_id_answers_rihan_mistral, embeddings_rihan_mistral, embeddings_mapping_rihan_mistral = extract_retrieve_result(result_rihanpiggy_mistral)\n",
    "question_id_answers_copasta, embeddings_copasta, embeddings_mapping_copasta = extract_retrieve_result(result_copasta)\n",
    "question_id_answers_rihan_qwen, embeddings_rihan_qwen, embeddings_mapping_rihan_qwen = extract_retrieve_result(result_rihanpiggy_qwen)\n",
    "\n",
    "# retrieve段階でQuestionId_Answersでsort済みだが、バグ防止と疎結合にさせるため、ここでもう一回ソートする\n",
    "question_id_answers_rihan_mistral, embeddings_rihan_mistral = resort_retrieve_result(question_id_answers_rihan_mistral, embeddings_rihan_mistral)\n",
    "question_id_answers_copasta, embeddings_copasta = resort_retrieve_result(question_id_answers_copasta, embeddings_copasta)\n",
    "question_id_answers_rihan_qwen, embeddings_rihan_qwen = resort_retrieve_result(question_id_answers_rihan_qwen, embeddings_rihan_qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec12804a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:00.555611Z",
     "iopub.status.busy": "2024-12-11T18:41:00.555314Z",
     "iopub.status.idle": "2024-12-11T18:41:00.560761Z",
     "shell.execute_reply": "2024-12-11T18:41:00.560111Z"
    },
    "papermill": {
     "duration": 0.029343,
     "end_time": "2024-12-11T18:41:00.562200",
     "exception": false,
     "start_time": "2024-12-11T18:41:00.532857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_ensemble(embeddings_set_list, weights, ids):\n",
    "    assert len(embeddings_set_list) == len(weights)\n",
    "    batch_size = 64\n",
    "    cosines = []\n",
    "    for (embeddings, embeddings_mapping) in  embeddings_set_list:\n",
    "        cos = SemanticSearcher._cosine_similarity(embeddings, embeddings_mapping, batch_size)\n",
    "        cosines.append(cos)\n",
    "    cosine = np.zeros(shape=cosines[0].shape)\n",
    "    for cos, weight in zip(cosines, weights):\n",
    "        cosine += cos * weight\n",
    "        \n",
    "    predict_ids = []\n",
    "    for i in range(len(cosine)):\n",
    "        descending_idx = np.argsort(cosine[i])[::-1]\n",
    "        predict_ids.append(ids[descending_idx].tolist())\n",
    "    return predict_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d413833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:00.607035Z",
     "iopub.status.busy": "2024-12-11T18:41:00.606675Z",
     "iopub.status.idle": "2024-12-11T18:41:00.612625Z",
     "shell.execute_reply": "2024-12-11T18:41:00.611977Z"
    },
    "papermill": {
     "duration": 0.029553,
     "end_time": "2024-12-11T18:41:00.614052",
     "exception": false,
     "start_time": "2024-12-11T18:41:00.584499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concat_ensemble(embeddings_set_list, weights, ids):\n",
    "    assert len(embeddings_set_list) == len(weights)\n",
    "    batch_size = 64\n",
    "    embeddings_list = []\n",
    "    embeddings_mapping_list = []\n",
    "    for ((embeddings, embeddings_mapping), weight) in zip(embeddings_set_list, weights):\n",
    "        embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings_mapping_norm = embeddings_mapping / np.linalg.norm(embeddings_mapping, axis=1, keepdims=True)\n",
    "        \n",
    "        embeddings_norm = embeddings_norm * weight\n",
    "        embeddings_mapping_norm = embeddings_mapping_norm * weight\n",
    "\n",
    "        embeddings_list.append(embeddings_norm)\n",
    "        embeddings_mapping_list.append(embeddings_mapping_norm)\n",
    "    embeddings = np.concatenate(embeddings_list, axis=1)\n",
    "    embeddings_mapping = np.concatenate(embeddings_mapping_list, axis=1)\n",
    "    print(embeddings.shape, embeddings_mapping.shape)\n",
    "    predict_ids = SemanticSearcher.cosine_similarity_search(embeddings, embeddings_mapping, ids, batch_size=batch_size)\n",
    "    return predict_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5bbe646",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:00.694298Z",
     "iopub.status.busy": "2024-12-11T18:41:00.693986Z",
     "iopub.status.idle": "2024-12-11T18:41:00.706587Z",
     "shell.execute_reply": "2024-12-11T18:41:00.705630Z"
    },
    "papermill": {
     "duration": 0.037037,
     "end_time": "2024-12-11T18:41:00.708384",
     "exception": false,
     "start_time": "2024-12-11T18:41:00.671347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nembeddings_set_list = [\\n    (embeddings_rihan, embeddings_mapping_rihan),\\n    (embeddings_copasta, embeddings_mapping_copasta),\\n]\\n\\nweights = [0.4, 0.6]\\nmisconception_ids = mis_maps['MisconceptionId'].values\\n\\npredict_ids = cosine_ensemble(embeddings_set_list, weights, ids)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_maps=pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "'''\n",
    "embeddings_set_list = [\n",
    "    (embeddings_rihan, embeddings_mapping_rihan),\n",
    "    (embeddings_copasta, embeddings_mapping_copasta),\n",
    "]\n",
    "\n",
    "weights = [0.4, 0.6]\n",
    "misconception_ids = mis_maps['MisconceptionId'].values\n",
    "\n",
    "predict_ids = cosine_ensemble(embeddings_set_list, weights, ids)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e206a6ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:00.754582Z",
     "iopub.status.busy": "2024-12-11T18:41:00.754321Z",
     "iopub.status.idle": "2024-12-11T18:41:02.442564Z",
     "shell.execute_reply": "2024-12-11T18:41:02.441572Z"
    },
    "papermill": {
     "duration": 1.715192,
     "end_time": "2024-12-11T18:41:02.445744",
     "exception": false,
     "start_time": "2024-12-11T18:41:00.730552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 14336) (2587, 14336)\n"
     ]
    }
   ],
   "source": [
    "ids = np.array(list(range(2587)))\n",
    "embeddings_set_list = [\n",
    "    (embeddings_rihan_mistral, embeddings_mapping_rihan_mistral),\n",
    "    (embeddings_copasta, embeddings_mapping_copasta),\n",
    "    (embeddings_rihan_qwen, embeddings_mapping_rihan_qwen),\n",
    "]\n",
    "\n",
    "weights = [0.36, 0.32, 0.32]\n",
    "misconception_ids = mis_maps['MisconceptionId'].values\n",
    "\n",
    "predict_ids = concat_ensemble(embeddings_set_list, weights, ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fae6af3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:02.552401Z",
     "iopub.status.busy": "2024-12-11T18:41:02.551651Z",
     "iopub.status.idle": "2024-12-11T18:41:02.589137Z",
     "shell.execute_reply": "2024-12-11T18:41:02.588159Z"
    },
    "papermill": {
     "duration": 0.073779,
     "end_time": "2024-12-11T18:41:02.590812",
     "exception": false,
     "start_time": "2024-12-11T18:41:02.517033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>misconception_id_retrieve</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>[706, 1345, 1507, 1672, 2306, 1516, 1005, 1963...</td>\n",
       "      <td>706 1345 1507 1672 2306 1516 1005 1963 2181 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>[1345, 706, 2306, 1507, 2532, 1672, 1005, 1392...</td>\n",
       "      <td>1345 706 2306 1507 2532 1672 1005 1392 1516 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>[706, 1672, 1345, 1005, 1507, 2532, 2306, 1516...</td>\n",
       "      <td>706 1672 1345 1005 1507 2532 2306 1516 1392 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>[891, 2142, 1755, 143, 2068, 167, 1535, 2078, ...</td>\n",
       "      <td>891 2142 1755 143 2068 167 1535 2078 418 1904 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>[891, 143, 1755, 2142, 2398, 167, 2078, 2068, ...</td>\n",
       "      <td>891 143 1755 2142 2398 167 2078 2068 1535 885 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>[2142, 1755, 891, 2068, 167, 143, 1535, 1904, ...</td>\n",
       "      <td>2142 1755 891 2068 167 143 1535 1904 418 2398 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>[1287, 1073, 2439, 1094, 1665, 1797, 1521, 827...</td>\n",
       "      <td>1287 1073 2439 1094 1665 1797 1521 827 2551 36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>[1287, 1073, 2439, 1094, 1665, 1521, 1797, 827...</td>\n",
       "      <td>1287 1073 2439 1094 1665 1521 1797 827 365 203...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>[1287, 1073, 2439, 1797, 2151, 365, 2551, 1059...</td>\n",
       "      <td>1287 1073 2439 1797 2151 365 2551 1059 1094 16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                          misconception_id_retrieve  \\\n",
       "0            1869_B  [706, 1345, 1507, 1672, 2306, 1516, 1005, 1963...   \n",
       "1            1869_C  [1345, 706, 2306, 1507, 2532, 1672, 1005, 1392...   \n",
       "2            1869_D  [706, 1672, 1345, 1005, 1507, 2532, 2306, 1516...   \n",
       "3            1870_A  [891, 2142, 1755, 143, 2068, 167, 1535, 2078, ...   \n",
       "4            1870_B  [891, 143, 1755, 2142, 2398, 167, 2078, 2068, ...   \n",
       "5            1870_C  [2142, 1755, 891, 2068, 167, 143, 1535, 1904, ...   \n",
       "6            1871_A  [1287, 1073, 2439, 1094, 1665, 1797, 1521, 827...   \n",
       "7            1871_C  [1287, 1073, 2439, 1094, 1665, 1521, 1797, 827...   \n",
       "8            1871_D  [1287, 1073, 2439, 1797, 2151, 365, 2551, 1059...   \n",
       "\n",
       "                                     MisconceptionId  \n",
       "0  706 1345 1507 1672 2306 1516 1005 1963 2181 25...  \n",
       "1  1345 706 2306 1507 2532 1672 1005 1392 1516 19...  \n",
       "2  706 1672 1345 1005 1507 2532 2306 1516 1392 32...  \n",
       "3  891 2142 1755 143 2068 167 1535 2078 418 1904 ...  \n",
       "4  891 143 1755 2142 2398 167 2078 2068 1535 885 ...  \n",
       "5  2142 1755 891 2068 167 143 1535 1904 418 2398 ...  \n",
       "6  1287 1073 2439 1094 1665 1797 1521 827 2551 36...  \n",
       "7  1287 1073 2439 1094 1665 1521 1797 827 365 203...  \n",
       "8  1287 1073 2439 1797 2151 365 2551 1059 1094 16...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(question_id_answers_copasta,columns=['QuestionId_Answer'])\n",
    "predict_ids_str = []\n",
    "for p in predict_ids:\n",
    "    p = [str(pp) for pp in p]\n",
    "    predict_ids_str.append(' '.join(p))\n",
    "\n",
    "submission['misconception_id_retrieve'] = predict_ids\n",
    "submission['MisconceptionId'] = predict_ids_str\n",
    "\n",
    "submission.to_pickle(\"/tmp/submission.pkl\")\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04a28deb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:02.637774Z",
     "iopub.status.busy": "2024-12-11T18:41:02.637505Z",
     "iopub.status.idle": "2024-12-11T18:41:02.648942Z",
     "shell.execute_reply": "2024-12-11T18:41:02.648048Z"
    },
    "papermill": {
     "duration": 0.037091,
     "end_time": "2024-12-11T18:41:02.650991",
     "exception": false,
     "start_time": "2024-12-11T18:41:02.613900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8th ~ 50th\n",
    "submission_7to50 = pd.read_pickle(\"/tmp/submission.pkl\")\n",
    "submission_7to50 = submission_7to50.drop('misconception_id_retrieve',axis=1)\n",
    "submission_7to50['MisconceptionId'] = submission_7to50['MisconceptionId'].apply(lambda x: ' '.join(x.split(' ')[7:51]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c631441f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:02.733084Z",
     "iopub.status.busy": "2024-12-11T18:41:02.732806Z",
     "iopub.status.idle": "2024-12-11T18:41:02.737252Z",
     "shell.execute_reply": "2024-12-11T18:41:02.736496Z"
    },
    "papermill": {
     "duration": 0.047501,
     "end_time": "2024-12-11T18:41:02.739206",
     "exception": false,
     "start_time": "2024-12-11T18:41:02.691705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_7to50.to_pickle(\"/tmp/submission_7to50.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da31750",
   "metadata": {
    "papermill": {
     "duration": 0.022772,
     "end_time": "2024-12-11T18:41:02.796984",
     "exception": false,
     "start_time": "2024-12-11T18:41:02.774212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Yoku、Qihang Part(Rerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "727d6d6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:02.852419Z",
     "iopub.status.busy": "2024-12-11T18:41:02.852098Z",
     "iopub.status.idle": "2024-12-11T18:41:07.405496Z",
     "shell.execute_reply": "2024-12-11T18:41:07.404583Z"
    },
    "papermill": {
     "duration": 4.578935,
     "end_time": "2024-12-11T18:41:07.407384",
     "exception": false,
     "start_time": "2024-12-11T18:41:02.828449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.14 environment at /opt/conda\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m25 packages\u001b[0m \u001b[2min 65ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 546ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 469ms\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.1\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.10.14 environment at /opt/conda\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m113 packages\u001b[0m \u001b[2min 54ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 53ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 343ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 427ms\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.19.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.20.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.40.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.46.2\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# copasta part installed transformers==4.40.0 due to dependencies of llm2vec\n",
    "# which is incompatible with vllm, so reinstall here\n",
    "!uv pip install --system --no-index --find-links=/kaggle/input/vllm-0-6-3-post1 torchvision==0.19.1\n",
    "!uv pip install --system --no-index --find-links=/kaggle/input/vllm-0-6-3-post1 vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74afd5be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:07.456232Z",
     "iopub.status.busy": "2024-12-11T18:41:07.455935Z",
     "iopub.status.idle": "2024-12-11T18:41:07.464940Z",
     "shell.execute_reply": "2024-12-11T18:41:07.464186Z"
    },
    "papermill": {
     "duration": 0.035345,
     "end_time": "2024-12-11T18:41:07.466618",
     "exception": false,
     "start_time": "2024-12-11T18:41:07.431273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rerank.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rerank.py\n",
    "\n",
    "import argparse \n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('model_name')\n",
    "parser.add_argument('--sub_pkl_name', type=str)\n",
    "parser.add_argument('--use_construct', action='store_true')\n",
    "parser.add_argument('--out_name', type=str, default=\"prompts_df_sorted.pkl\") \n",
    "parser.add_argument('--quantization', type=str, default=\"awq\") \n",
    "parser.add_argument('--top_n', type=int, default=10) \n",
    "parser.add_argument('--reverse', action='store_true')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"{args=}\")\n",
    "\n",
    "# load files\n",
    "submission = pd.read_pickle(args.sub_pkl_name)\n",
    "mis_maps=pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "misconception_dict_id_to_name = dict(zip(mis_maps['MisconceptionId'], mis_maps['MisconceptionName']))\n",
    "\n",
    "df_tmp = pd.read_csv('/tmp/test.csv')\n",
    "df_tmp = df_tmp.drop('MisconceptionId',axis=1)\n",
    "df_tmp[\"knowledge\"]=df_tmp[[\"FirstSubjectName\", \"SecondSubjectName\", \"ThirdSubjectName\", \"ConstructName\"]].agg(\",\".join, axis=1)\n",
    "\n",
    "infer_target = submission.merge(df_tmp, on=\"QuestionId_Answer\", how=\"left\")\n",
    "\n",
    "\n",
    "# =========== model ===========\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "import vllm\n",
    "import string\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    args.model_name,\n",
    "    quantization=args.quantization,\n",
    "    tensor_parallel_size=2, \n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=2048,\n",
    "    enable_prefix_caching=True,\n",
    "    #distributed_executor_backend=\"ray\",\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "\n",
    "# =========== prompt ===========\n",
    "RANK_TOP_N=args.top_n\n",
    "choices = [choice for i, choice in enumerate(string.ascii_uppercase) if i<RANK_TOP_N]\n",
    "\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x,add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "\n",
    "prompts = []\n",
    "for _, row in infer_target.iterrows():\n",
    "    # Extract row data\n",
    "    QuestionId_Answer = row[\"QuestionId_Answer\"]\n",
    "    knowledge = row[\"knowledge\"]\n",
    "    construct = row[\"ConstructName\"]\n",
    "    question_text = row[\"QuestionText\"]\n",
    "    correct_answer_text = row[\"CorrectAnswerText\"]\n",
    "    answer_text = row[\"AnswerText\"]\n",
    "    cot_misunderstanding = row[\"p000-qwen25-32b-instruct-cot_misunderstanding\"]\n",
    "    misconceptions = row[\"MisconceptionId\"]\n",
    "\n",
    "    # Generate list of misconceptions with letters (A, B, C, D...)\n",
    "    misconceptions_list = misconceptions[:RANK_TOP_N] if isinstance(misconceptions, list) else misconceptions.split(\" \")[:RANK_TOP_N]\n",
    "    misconceptions_list = [misconception_dict_id_to_name[int(mis_id)] for mis_id in misconceptions_list]\n",
    "    if args.reverse:\n",
    "        misconceptions_list = misconceptions_list[::-1]\n",
    "    misconceptions_text = \"\\n\".join([f\"{letter}. {mis}\" for letter, mis in zip(string.ascii_uppercase, misconceptions_list)])\n",
    "\n",
    "    # Create prompt for selection of misconceptions\n",
    "    if args.use_construct:\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "You are a mathematics teacher. Your job is to infer and identify the misconceptions behind the incorrect answers to the questions.<|im_end|>\n",
    "<|im_start|>user\n",
    "# OBJECTIVE #\n",
    "Based on the problem, please select which of the following misconceptions might have led to the student's incorrect answer.\n",
    "\n",
    "Categories: {knowledge}\n",
    "Question: {question_text}\n",
    "Construct Name: {construct}\n",
    "Correct Answer: {correct_answer_text}\n",
    "Student's Answer: {answer_text}\n",
    "Step-by-Step Solution: {cot_misunderstanding}\n",
    "\n",
    "Possible Misconceptions:\\n{misconceptions_text}\n",
    "\n",
    "Please respond only with the letter(s) of the misconception(s) you think are most likely to be the cause.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "You are a mathematics teacher. Your job is to infer and identify the misconceptions behind the incorrect answers to the questions.<|im_end|>\n",
    "<|im_start|>user\n",
    "# OBJECTIVE #\n",
    "Based on the problem, please select which of the following misconceptions might have led to the student's incorrect answer.\n",
    "\n",
    "Categories: {knowledge}\n",
    "Question: {question_text}\n",
    "Correct Answer: {correct_answer_text}\n",
    "Student's Answer: {answer_text}\n",
    "Step-by-Step Solution: {cot_misunderstanding}\n",
    "\n",
    "Possible Misconceptions:\\n{misconceptions_text}\n",
    "\n",
    "Please respond only with the letter(s) of the misconception(s) you think are most likely to be the cause.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "    prompts.append({\n",
    "        \"QuestionId_Answer\": QuestionId_Answer,\n",
    "        \"Prompt\": prompt,\n",
    "    })\n",
    "\n",
    "# Convert the list of prompts to a DataFrame\n",
    "prompts_df = pd.DataFrame(prompts)\n",
    "all_prompts = prompts_df.Prompt.values\n",
    "VALIDATE = len(all_prompts)\n",
    "\n",
    "\n",
    "# =========== infrence ===========\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores\n",
    "\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "responses = llm.generate(\n",
    "    all_prompts,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs = RANK_TOP_N\n",
    "    ),\n",
    "    use_tqdm = True\n",
    ")\n",
    "\n",
    "end = time()\n",
    "elapsed = (end-start)/60. #minutes\n",
    "\n",
    "print(f\"Inference of {VALIDATE} samples took {elapsed} minutes!\")\n",
    "\n",
    "\n",
    "# =========== postprocess ===========\n",
    "import os, math, numpy as np\n",
    "\n",
    "results = []\n",
    "for i, response in enumerate(responses):\n",
    "    x = response.outputs[0].logprobs[0]\n",
    "    logprobs = []\n",
    "    for k in KEEP:  # Assuming KEEP now contains tokens for \"A\", \"B\", \"C\", \"D\", and \"E\"\n",
    "        if k in x:\n",
    "            logprobs.append(math.exp(x[k].logprob))\n",
    "        else:\n",
    "            logprobs.append(0)\n",
    "            print(f\"bad logits {i}\")\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    logprobs = np.array(logprobs)\n",
    "    logprobs /= logprobs.sum()  # Normalize to get probabilities\n",
    "\n",
    "    if args.reverse:\n",
    "        logprobs = logprobs[::-1]\n",
    "    results.append(logprobs)\n",
    "\n",
    "results = np.vstack(results)  # Stack results vertically to form a matrix\n",
    "\n",
    "\n",
    "# Assuming each column in `results` corresponds to A, B, C, D, and E in that order\n",
    "# Convert the probability matrix into individual scores for each option\n",
    "for idx, choice in enumerate(choices):\n",
    "    prompts_df[f\"score_{choice}\"] = results[:, idx]\n",
    "\n",
    "\n",
    "prompts_df.to_pickle(args.out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fd97bc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:07.514517Z",
     "iopub.status.busy": "2024-12-11T18:41:07.514210Z",
     "iopub.status.idle": "2024-12-11T18:41:07.522178Z",
     "shell.execute_reply": "2024-12-11T18:41:07.521334Z"
    },
    "papermill": {
     "duration": 0.033769,
     "end_time": "2024-12-11T18:41:07.523815",
     "exception": false,
     "start_time": "2024-12-11T18:41:07.490046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rerank_72b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rerank_72b.py\n",
    "\n",
    "import argparse \n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('model_name')\n",
    "parser.add_argument('--sub_pkl_name', type=str)\n",
    "parser.add_argument('--use_construct', action='store_true')\n",
    "parser.add_argument('--out_name', type=str, default=\"prompts_df_sorted.pkl\") \n",
    "parser.add_argument('--quantization', type=str, default=\"awq\") \n",
    "parser.add_argument('--top_n', type=int, default=10) \n",
    "parser.add_argument('--reverse', action='store_true')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"{args=}\")\n",
    "\n",
    "import vllm\n",
    "llm = vllm.LLM(\n",
    "    args.model_name,\n",
    "    quantization=args.quantization,\n",
    "    tensor_parallel_size=2, \n",
    "    gpu_memory_utilization=0.98, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=2000,\n",
    "    enable_prefix_caching=True,\n",
    "    disable_log_stats=True,\n",
    "    cpu_offload_gb=8,\n",
    "    swap_space=1,\n",
    "    device='cuda',\n",
    "    max_num_seqs=20,\n",
    "    #distributed_executor_backend=\"ray\",\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "# load files\n",
    "submission = pd.read_pickle(args.sub_pkl_name)\n",
    "mis_maps=pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "misconception_dict_id_to_name = dict(zip(mis_maps['MisconceptionId'], mis_maps['MisconceptionName']))\n",
    "\n",
    "df_tmp = pd.read_csv('/tmp/test.csv')\n",
    "df_tmp = df_tmp.drop('MisconceptionId',axis=1)\n",
    "df_tmp[\"knowledge\"]=df_tmp[[\"FirstSubjectName\", \"SecondSubjectName\", \"ThirdSubjectName\", \"ConstructName\"]].agg(\",\".join, axis=1)\n",
    "\n",
    "infer_target = submission.merge(df_tmp, on=\"QuestionId_Answer\", how=\"left\")\n",
    "\n",
    "\n",
    "# =========== model ===========\n",
    "import string\n",
    "\n",
    "# =========== prompt ===========\n",
    "RANK_TOP_N=args.top_n\n",
    "choices = [choice for i, choice in enumerate(string.ascii_uppercase) if i<RANK_TOP_N]\n",
    "\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x,add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "\n",
    "prompts = []\n",
    "for _, row in infer_target.iterrows():\n",
    "    # Extract row data\n",
    "    QuestionId_Answer = row[\"QuestionId_Answer\"]\n",
    "    knowledge = row[\"knowledge\"]\n",
    "    construct = row[\"ConstructName\"]\n",
    "    question_text = row[\"QuestionText\"]\n",
    "    correct_answer_text = row[\"CorrectAnswerText\"]\n",
    "    answer_text = row[\"AnswerText\"]\n",
    "    cot_misunderstanding = row[\"p000-qwen25-32b-instruct-cot_misunderstanding\"]\n",
    "    misconceptions = row[\"MisconceptionId\"]\n",
    "\n",
    "    # Generate list of misconceptions with letters (A, B, C, D...)\n",
    "    misconceptions_list = misconceptions[:RANK_TOP_N] if isinstance(misconceptions, list) else misconceptions.split(\" \")[:RANK_TOP_N]\n",
    "    misconceptions_list = [misconception_dict_id_to_name[int(mis_id)] for mis_id in misconceptions_list]\n",
    "    if args.reverse:\n",
    "        misconceptions_list = misconceptions_list[::-1]\n",
    "    misconceptions_text = \"\\n\".join([f\"{letter}. {mis}\" for letter, mis in zip(string.ascii_uppercase, misconceptions_list)])\n",
    "\n",
    "    # Create prompt for selection of misconceptions\n",
    "    if args.use_construct:\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "You are a mathematics teacher. Your job is to infer and identify the misconceptions behind the incorrect answers to the questions.<|im_end|>\n",
    "<|im_start|>user\n",
    "# OBJECTIVE #\n",
    "Based on the problem, please select which of the following misconceptions might have led to the student's incorrect answer.\n",
    "\n",
    "Categories: {knowledge}\n",
    "Question: {question_text}\n",
    "Construct Name: {construct}\n",
    "Correct Answer: {correct_answer_text}\n",
    "Student's Answer: {answer_text}\n",
    "Step-by-Step Solution: {cot_misunderstanding}\n",
    "\n",
    "Possible Misconceptions:\\n{misconceptions_text}\n",
    "\n",
    "Please respond only with the letter(s) of the misconception(s) you think are most likely to be the cause.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "You are a mathematics teacher. Your job is to infer and identify the misconceptions behind the incorrect answers to the questions.<|im_end|>\n",
    "<|im_start|>user\n",
    "# OBJECTIVE #\n",
    "Based on the problem, please select which of the following misconceptions might have led to the student's incorrect answer.\n",
    "\n",
    "Categories: {knowledge}\n",
    "Question: {question_text}\n",
    "Correct Answer: {correct_answer_text}\n",
    "Student's Answer: {answer_text}\n",
    "Step-by-Step Solution: {cot_misunderstanding}\n",
    "\n",
    "Possible Misconceptions:\\n{misconceptions_text}\n",
    "\n",
    "Please respond only with the letter(s) of the misconception(s) you think are most likely to be the cause.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "    prompts.append({\n",
    "        \"QuestionId_Answer\": QuestionId_Answer,\n",
    "        \"Prompt\": prompt,\n",
    "    })\n",
    "\n",
    "# Convert the list of prompts to a DataFrame\n",
    "prompts_df = pd.DataFrame(prompts)\n",
    "all_prompts = prompts_df.Prompt.values\n",
    "VALIDATE = len(all_prompts)\n",
    "\n",
    "\n",
    "# =========== infrence ===========\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores\n",
    "\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "responses = llm.generate(\n",
    "    all_prompts,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs = RANK_TOP_N\n",
    "    ),\n",
    "    use_tqdm = True\n",
    ")\n",
    "\n",
    "end = time()\n",
    "elapsed = (end-start)/60. #minutes\n",
    "\n",
    "print(f\"Inference of {VALIDATE} samples took {elapsed} minutes!\")\n",
    "\n",
    "\n",
    "# =========== postprocess ===========\n",
    "import os, math, numpy as np\n",
    "\n",
    "results = []\n",
    "for i, response in enumerate(responses):\n",
    "    x = response.outputs[0].logprobs[0]\n",
    "    logprobs = []\n",
    "    for k in KEEP:  # Assuming KEEP now contains tokens for \"A\", \"B\", \"C\", \"D\", and \"E\"\n",
    "        if k in x:\n",
    "            logprobs.append(math.exp(x[k].logprob))\n",
    "        else:\n",
    "            logprobs.append(0)\n",
    "            print(f\"bad logits {i}\")\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    logprobs = np.array(logprobs)\n",
    "    logprobs /= logprobs.sum()  # Normalize to get probabilities\n",
    "\n",
    "    if args.reverse:\n",
    "        logprobs = logprobs[::-1]\n",
    "    results.append(logprobs)\n",
    "\n",
    "results = np.vstack(results)  # Stack results vertically to form a matrix\n",
    "\n",
    "\n",
    "# Assuming each column in `results` corresponds to A, B, C, D, and E in that order\n",
    "# Convert the probability matrix into individual scores for each option\n",
    "for idx, choice in enumerate(choices):\n",
    "    prompts_df[f\"score_{choice}\"] = results[:, idx]\n",
    "\n",
    "\n",
    "prompts_df.to_pickle(args.out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b57bbc19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:07.576304Z",
     "iopub.status.busy": "2024-12-11T18:41:07.575618Z",
     "iopub.status.idle": "2024-12-11T18:41:07.582151Z",
     "shell.execute_reply": "2024-12-11T18:41:07.581391Z"
    },
    "papermill": {
     "duration": 0.034774,
     "end_time": "2024-12-11T18:41:07.583688",
     "exception": false,
     "start_time": "2024-12-11T18:41:07.548914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def rerank(df, submission, out_path, RANK_TOP_N=10):\n",
    "    choices = [choice for i, choice in enumerate(string.ascii_uppercase) if i<RANK_TOP_N]\n",
    "    submission = submission.merge(df,how='left',on='QuestionId_Answer')\n",
    "    \n",
    "    misconception_ids_reranks = []\n",
    "    for i in range(len(submission)):\n",
    "        row = submission.iloc[i]\n",
    "        scores = np.array([row[f'score_{choice}'] for choice in choices])\n",
    "        misconception_ids = row['MisconceptionId'].split(' ')\n",
    "        idx_sort = np.argsort(scores)[::-1]\n",
    "        misconception_ids = np.array(misconception_ids)[:RANK_TOP_N][idx_sort].tolist() + misconception_ids[RANK_TOP_N:]\n",
    "        misconception_ids_reranks.append(' '.join(misconception_ids))\n",
    "        \n",
    "    submission['MisconceptionId'] = misconception_ids_reranks\n",
    "    submission = submission[['QuestionId_Answer', 'MisconceptionId']]\n",
    "    submission.to_csv(out_path, index=False)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241bad3",
   "metadata": {
    "papermill": {
     "duration": 0.022754,
     "end_time": "2024-12-11T18:41:07.629320",
     "exception": false,
     "start_time": "2024-12-11T18:41:07.606566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Rerank 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "011ab753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:41:07.684241Z",
     "iopub.status.busy": "2024-12-11T18:41:07.683850Z",
     "iopub.status.idle": "2024-12-11T18:43:14.637433Z",
     "shell.execute_reply": "2024-12-11T18:43:14.636353Z"
    },
    "papermill": {
     "duration": 126.987264,
     "end_time": "2024-12-11T18:43:14.639504",
     "exception": false,
     "start_time": "2024-12-11T18:41:07.652240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args=Namespace(model_name='/kaggle/input/eedi-top25-qwen14b/transformers/merged/2', sub_pkl_name='/tmp/submission_7to50.pkl', use_construct=False, out_name='/tmp/prompts_df_sorted_14B_7to50.pkl', quantization='gptq', top_n=10, reverse=False)\r\n",
      "WARNING 12-11 18:41:12 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\n",
      "WARNING 12-11 18:41:29 config.py:321] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-11 18:41:29 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-11 18:41:29 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-11 18:41:29 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/eedi-top25-qwen14b/transformers/merged/2', speculative_config=None, tokenizer='/kaggle/input/eedi-top25-qwen14b/transformers/merged/2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/eedi-top25-qwen14b/transformers/merged/2, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "INFO 12-11 18:41:29 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 12-11 18:41:30 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:41:30 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:41:30 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-11 18:41:30 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:41:30 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:41:31 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-11 18:41:31 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:41:31 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 18:41:31 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 18:41:32 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-11 18:41:49 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:41:49 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 12-11 18:41:49 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m WARNING 12-11 18:41:49 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-11 18:41:49 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fc82d3d9e10>, local_subscribe_port=48653, remote_subscribe_port=None)\r\n",
      "INFO 12-11 18:41:49 model_runner.py:1056] Starting to load model /kaggle/input/eedi-top25-qwen14b/transformers/merged/2...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:41:49 model_runner.py:1056] Starting to load model /kaggle/input/eedi-top25-qwen14b/transformers/merged/2...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:41:49 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:41:49 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-11 18:41:49 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 18:41:49 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:59<00:00, 59.08s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:59<00:00, 59.08s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 18:42:49 model_runner.py:1067] Loading model weights took 4.6681 GB\r\n",
      "INFO 12-11 18:42:49 model_runner.py:1067] Loading model weights took 4.6681 GB\r\n",
      "INFO 12-11 18:42:52 distributed_gpu_executor.py:57] # GPU blocks: 4556, # CPU blocks: 2730\r\n",
      "INFO 12-11 18:42:52 distributed_gpu_executor.py:61] Maximum concurrency for 2048 tokens per request: 35.59x\r\n",
      "Force predictions to be tokens [32, 33, 34, 35, 36, 37, 38, 39, 40, 41] which are ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'].\r\n",
      "Processed prompts: 100%|█| 9/9 [00:09<00:00,  1.01s/it, est. speed input: 374.23\r\n",
      "Inference of 9 samples took 0.1517252524693807 minutes!\r\n"
     ]
    }
   ],
   "source": [
    "rerank_top_n_1st = 10\n",
    "target_file_1st = '/tmp/submission_7to50.pkl'\n",
    "out_path_1st_14b = \"/tmp/prompts_df_sorted_14B_7to50.pkl\"\n",
    "out_path_1st_32b = \"/tmp/prompts_df_sorted_32B_7to50.pkl\"\n",
    "submission_target_1st = pd.read_pickle(target_file_1st)\n",
    "\n",
    "# sliding window推論\n",
    "# 7位~18位をrerank\n",
    "!python rerank.py \"/kaggle/input/eedi-top25-qwen14b/transformers/merged/2\" --sub_pkl_name $target_file_1st --out_name $out_path_1st_14b --quantization \"gptq\" --top_n $rerank_top_n_1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de24ed34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:14.690771Z",
     "iopub.status.busy": "2024-12-11T18:43:14.690465Z",
     "iopub.status.idle": "2024-12-11T18:43:14.694583Z",
     "shell.execute_reply": "2024-12-11T18:43:14.693929Z"
    },
    "papermill": {
     "duration": 0.030998,
     "end_time": "2024-12-11T18:43:14.696083",
     "exception": false,
     "start_time": "2024-12-11T18:43:14.665085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python rerank.py \"/kaggle/input/eedi-top25-qwen32b/transformers/merged-gen2.5/1\" --sub_pkl_name $target_file_1st --use_construct --out_name $out_path_1st_32b --top_n $rerank_top_n_1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f3c17be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:14.745400Z",
     "iopub.status.busy": "2024-12-11T18:43:14.745117Z",
     "iopub.status.idle": "2024-12-11T18:43:14.761923Z",
     "shell.execute_reply": "2024-12-11T18:43:14.761315Z"
    },
    "papermill": {
     "duration": 0.043344,
     "end_time": "2024-12-11T18:43:14.763590",
     "exception": false,
     "start_time": "2024-12-11T18:43:14.720246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8位~17位をreranking\n",
    "#prompts_df_32B_sorted7to50 = pd.read_pickle(out_path_1st_32b)\n",
    "prompts_df_14B_sorted7to50 = pd.read_pickle(out_path_1st_14b)\n",
    "\n",
    "# 同じ順に並び替え\n",
    "#prompts_df_14B_sorted7to50 = prompts_df_32B_sorted7to50[[\"QuestionId_Answer\"]].merge(prompts_df_14B_sorted7to50, on=\"QuestionId_Answer\", how=\"left\")\n",
    "\n",
    "# アンサンブル\n",
    "#score_cols = prompts_df_32B_sorted7to50.columns[prompts_df_32B_sorted7to50.columns.str.startswith(\"score_\")]\n",
    "\n",
    "#prompts_df_32B_sorted7to50.loc[:, score_cols] = \\\n",
    "#0.6 * prompts_df_32B_sorted7to50[score_cols] + 0.4 * prompts_df_14B_sorted7to50[score_cols]\n",
    "\n",
    "prompts_df_sorted7to50 = prompts_df_14B_sorted7to50\n",
    "df_rerank = rerank(prompts_df_sorted7to50, submission_target_1st, out_path=\"/tmp/submission_7to50.csv\", RANK_TOP_N=rerank_top_n_1st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca79347f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:14.813131Z",
     "iopub.status.busy": "2024-12-11T18:43:14.812887Z",
     "iopub.status.idle": "2024-12-11T18:43:14.826062Z",
     "shell.execute_reply": "2024-12-11T18:43:14.825515Z"
    },
    "papermill": {
     "duration": 0.039768,
     "end_time": "2024-12-11T18:43:14.827620",
     "exception": false,
     "start_time": "2024-12-11T18:43:14.787852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8位~17位を置換\n",
    "submission_retrieve = pd.read_pickle(\"/tmp/submission.pkl\")\n",
    "rows_rerank = []\n",
    "for row_base, row_rerank in zip(submission_retrieve[\"MisconceptionId\"].values, df_rerank[\"MisconceptionId\"].values):\n",
    "    row_base = row_base.split(' ')\n",
    "    row_rerank = row_rerank.split(' ')\n",
    "    \n",
    "    row_base[7:7+rerank_top_n_1st] = row_rerank[:rerank_top_n_1st]\n",
    "    rows_rerank.append(' '.join(row_base))\n",
    "\n",
    "submission_retrieve[\"MisconceptionId\"] = rows_rerank\n",
    "submission_retrieve.to_pickle(\"/tmp/submission_7to50_reranked.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b69aa834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:14.876745Z",
     "iopub.status.busy": "2024-12-11T18:43:14.876506Z",
     "iopub.status.idle": "2024-12-11T18:43:14.885249Z",
     "shell.execute_reply": "2024-12-11T18:43:14.884553Z"
    },
    "papermill": {
     "duration": 0.034924,
     "end_time": "2024-12-11T18:43:14.886729",
     "exception": false,
     "start_time": "2024-12-11T18:43:14.851805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1963 2181 2532 2586 2518 328 1054 1392 1338 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1392 1516 1963 2181 2518 2586 1941 328 1338 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>1516 1392 328 2518 2586 1941 987 1963 2181 315...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>2078 418 1904 2398 1256 885 979 59 320 1540 80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>2068 1535 885 979 1256 1904 418 80 59 1991 154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>1904 418 2398 2078 1256 885 59 979 320 1593 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>827 2551 365 2151 203 2064 632 1059 771 1853 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>827 365 203 2551 2151 771 632 2064 1059 1853 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1059 1094 1665 2165 1521 827 2064 449 1923 115...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  1963 2181 2532 2586 2518 328 1054 1392 1338 19...\n",
       "1            1869_C  1392 1516 1963 2181 2518 2586 1941 328 1338 65...\n",
       "2            1869_D  1516 1392 328 2518 2586 1941 987 1963 2181 315...\n",
       "3            1870_A  2078 418 1904 2398 1256 885 979 59 320 1540 80...\n",
       "4            1870_B  2068 1535 885 979 1256 1904 418 80 59 1991 154...\n",
       "5            1870_C  1904 418 2398 2078 1256 885 59 979 320 1593 15...\n",
       "6            1871_A  827 2551 365 2151 203 2064 632 1059 771 1853 1...\n",
       "7            1871_C  827 365 203 2551 2151 771 632 2064 1059 1853 1...\n",
       "8            1871_D  1059 1094 1665 2165 1521 827 2064 449 1923 115..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('/tmp/submission_7to50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f0c1324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:14.936271Z",
     "iopub.status.busy": "2024-12-11T18:43:14.936020Z",
     "iopub.status.idle": "2024-12-11T18:43:14.950858Z",
     "shell.execute_reply": "2024-12-11T18:43:14.950061Z"
    },
    "papermill": {
     "duration": 0.041468,
     "end_time": "2024-12-11T18:43:14.952388",
     "exception": false,
     "start_time": "2024-12-11T18:43:14.910920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>score_A</th>\n",
       "      <th>score_B</th>\n",
       "      <th>score_C</th>\n",
       "      <th>score_D</th>\n",
       "      <th>score_E</th>\n",
       "      <th>score_F</th>\n",
       "      <th>score_G</th>\n",
       "      <th>score_H</th>\n",
       "      <th>score_I</th>\n",
       "      <th>score_J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.089646</td>\n",
       "      <td>0.061613</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>0.167481</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.189781</td>\n",
       "      <td>0.089646</td>\n",
       "      <td>0.024128</td>\n",
       "      <td>0.115108</td>\n",
       "      <td>0.101582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.021410</td>\n",
       "      <td>0.065947</td>\n",
       "      <td>0.115741</td>\n",
       "      <td>0.084678</td>\n",
       "      <td>0.095952</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>0.095952</td>\n",
       "      <td>0.245023</td>\n",
       "      <td>0.090139</td>\n",
       "      <td>0.061952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.078644</td>\n",
       "      <td>0.089115</td>\n",
       "      <td>0.166489</td>\n",
       "      <td>0.078644</td>\n",
       "      <td>0.177226</td>\n",
       "      <td>0.083716</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>0.078644</td>\n",
       "      <td>0.021167</td>\n",
       "      <td>0.200824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.023033</td>\n",
       "      <td>0.170190</td>\n",
       "      <td>0.055253</td>\n",
       "      <td>0.317957</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.192851</td>\n",
       "      <td>0.043031</td>\n",
       "      <td>0.033512</td>\n",
       "      <td>0.070946</td>\n",
       "      <td>0.055253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.363411</td>\n",
       "      <td>0.071560</td>\n",
       "      <td>0.133692</td>\n",
       "      <td>0.038303</td>\n",
       "      <td>0.040774</td>\n",
       "      <td>0.055731</td>\n",
       "      <td>0.171663</td>\n",
       "      <td>0.104119</td>\n",
       "      <td>0.006656</td>\n",
       "      <td>0.014091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.082061</td>\n",
       "      <td>0.119398</td>\n",
       "      <td>0.416740</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>0.034208</td>\n",
       "      <td>0.196854</td>\n",
       "      <td>0.038763</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.018310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.013415</td>\n",
       "      <td>0.305332</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>0.013415</td>\n",
       "      <td>0.026680</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.570436</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>0.034257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.009409</td>\n",
       "      <td>0.353078</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.006075</td>\n",
       "      <td>0.050866</td>\n",
       "      <td>0.012861</td>\n",
       "      <td>0.546858</td>\n",
       "      <td>0.010662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.857607</td>\n",
       "      <td>0.009527</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>0.029346</td>\n",
       "      <td>0.007420</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>0.003093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                             Prompt  \\\n",
       "0            1869_B  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "1            1869_C  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "2            1869_D  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "3            1870_A  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "4            1870_B  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "5            1870_C  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "6            1871_A  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "7            1871_C  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "8            1871_D  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "\n",
       "    score_A   score_B   score_C   score_D   score_E   score_F   score_G  \\\n",
       "0  0.089646  0.061613  0.065587  0.167481  0.095428  0.189781  0.089646   \n",
       "1  0.021410  0.065947  0.115741  0.084678  0.095952  0.123205  0.095952   \n",
       "2  0.078644  0.089115  0.166489  0.078644  0.177226  0.083716  0.025532   \n",
       "3  0.023033  0.170190  0.055253  0.317957  0.037975  0.192851  0.043031   \n",
       "4  0.363411  0.071560  0.133692  0.038303  0.040774  0.055731  0.171663   \n",
       "5  0.082061  0.119398  0.416740  0.015180  0.034208  0.196854  0.038763   \n",
       "6  0.013415  0.305332  0.002190  0.002482  0.013415  0.026680  0.028400   \n",
       "7  0.005036  0.002099  0.009409  0.353078  0.003055  0.006075  0.050866   \n",
       "8  0.857607  0.009527  0.002409  0.029346  0.007420  0.000474  0.084915   \n",
       "\n",
       "    score_H   score_I   score_J  \n",
       "0  0.024128  0.115108  0.101582  \n",
       "1  0.245023  0.090139  0.061952  \n",
       "2  0.078644  0.021167  0.200824  \n",
       "3  0.033512  0.070946  0.055253  \n",
       "4  0.104119  0.006656  0.014091  \n",
       "5  0.022086  0.056400  0.018310  \n",
       "6  0.570436  0.003392  0.034257  \n",
       "7  0.012861  0.546858  0.010662  \n",
       "8  0.000419  0.004791  0.003093  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_df_sorted7to50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9eb08ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:15.003132Z",
     "iopub.status.busy": "2024-12-11T18:43:15.002590Z",
     "iopub.status.idle": "2024-12-11T18:43:15.009528Z",
     "shell.execute_reply": "2024-12-11T18:43:15.008737Z"
    },
    "papermill": {
     "duration": 0.033985,
     "end_time": "2024-12-11T18:43:15.011052",
     "exception": false,
     "start_time": "2024-12-11T18:43:14.977067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>328 2586 1338 1941 2518 1054 1963 2532 2181 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>328 2586 1963 1941 2518 1338 2181 1516 657 139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>315 2586 328 1392 1941 2518 1516 1963 987 2181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>2398 885 418 320 1540 1904 979 1256 59 2078 80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>2068 418 885 80 1535 1904 1256 979 1991 59 154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>2398 885 418 1904 320 59 1256 979 1593 2078 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1059 2551 1853 632 2064 203 827 771 2151 365 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1059 2551 632 2064 1853 203 771 827 2151 365 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1059 2064 2165 1094 1521 1923 1150 1665 827 44...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  328 2586 1338 1941 2518 1054 1963 2532 2181 13...\n",
       "1            1869_C  328 2586 1963 1941 2518 1338 2181 1516 657 139...\n",
       "2            1869_D  315 2586 328 1392 1941 2518 1516 1963 987 2181...\n",
       "3            1870_A  2398 885 418 320 1540 1904 979 1256 59 2078 80...\n",
       "4            1870_B  2068 418 885 80 1535 1904 1256 979 1991 59 154...\n",
       "5            1870_C  2398 885 418 1904 320 59 1256 979 1593 2078 15...\n",
       "6            1871_A  1059 2551 1853 632 2064 203 827 771 2151 365 1...\n",
       "7            1871_C  1059 2551 632 2064 1853 203 771 827 2151 365 1...\n",
       "8            1871_D  1059 2064 2165 1094 1521 1923 1150 1665 827 44..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c3f8557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:15.062274Z",
     "iopub.status.busy": "2024-12-11T18:43:15.062021Z",
     "iopub.status.idle": "2024-12-11T18:43:15.077536Z",
     "shell.execute_reply": "2024-12-11T18:43:15.076726Z"
    },
    "papermill": {
     "duration": 0.042582,
     "end_time": "2024-12-11T18:43:15.079126",
     "exception": false,
     "start_time": "2024-12-11T18:43:15.036544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>misconception_id_retrieve</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>[706, 1345, 1507, 1672, 2306, 1516, 1005, 1963...</td>\n",
       "      <td>706 1345 1507 1672 2306 1516 1005 1963 2181 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>[1345, 706, 2306, 1507, 2532, 1672, 1005, 1392...</td>\n",
       "      <td>1345 706 2306 1507 2532 1672 1005 1392 1516 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>[706, 1672, 1345, 1005, 1507, 2532, 2306, 1516...</td>\n",
       "      <td>706 1672 1345 1005 1507 2532 2306 1516 1392 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>[891, 2142, 1755, 143, 2068, 167, 1535, 2078, ...</td>\n",
       "      <td>891 2142 1755 143 2068 167 1535 2078 418 1904 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>[891, 143, 1755, 2142, 2398, 167, 2078, 2068, ...</td>\n",
       "      <td>891 143 1755 2142 2398 167 2078 2068 1535 885 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>[2142, 1755, 891, 2068, 167, 143, 1535, 1904, ...</td>\n",
       "      <td>2142 1755 891 2068 167 143 1535 1904 418 2398 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>[1287, 1073, 2439, 1094, 1665, 1797, 1521, 827...</td>\n",
       "      <td>1287 1073 2439 1094 1665 1797 1521 827 2551 36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>[1287, 1073, 2439, 1094, 1665, 1521, 1797, 827...</td>\n",
       "      <td>1287 1073 2439 1094 1665 1521 1797 827 365 203...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>[1287, 1073, 2439, 1797, 2151, 365, 2551, 1059...</td>\n",
       "      <td>1287 1073 2439 1797 2151 365 2551 1059 1094 16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                          misconception_id_retrieve  \\\n",
       "0            1869_B  [706, 1345, 1507, 1672, 2306, 1516, 1005, 1963...   \n",
       "1            1869_C  [1345, 706, 2306, 1507, 2532, 1672, 1005, 1392...   \n",
       "2            1869_D  [706, 1672, 1345, 1005, 1507, 2532, 2306, 1516...   \n",
       "3            1870_A  [891, 2142, 1755, 143, 2068, 167, 1535, 2078, ...   \n",
       "4            1870_B  [891, 143, 1755, 2142, 2398, 167, 2078, 2068, ...   \n",
       "5            1870_C  [2142, 1755, 891, 2068, 167, 143, 1535, 1904, ...   \n",
       "6            1871_A  [1287, 1073, 2439, 1094, 1665, 1797, 1521, 827...   \n",
       "7            1871_C  [1287, 1073, 2439, 1094, 1665, 1521, 1797, 827...   \n",
       "8            1871_D  [1287, 1073, 2439, 1797, 2151, 365, 2551, 1059...   \n",
       "\n",
       "                                     MisconceptionId  \n",
       "0  706 1345 1507 1672 2306 1516 1005 1963 2181 25...  \n",
       "1  1345 706 2306 1507 2532 1672 1005 1392 1516 19...  \n",
       "2  706 1672 1345 1005 1507 2532 2306 1516 1392 32...  \n",
       "3  891 2142 1755 143 2068 167 1535 2078 418 1904 ...  \n",
       "4  891 143 1755 2142 2398 167 2078 2068 1535 885 ...  \n",
       "5  2142 1755 891 2068 167 143 1535 1904 418 2398 ...  \n",
       "6  1287 1073 2439 1094 1665 1797 1521 827 2551 36...  \n",
       "7  1287 1073 2439 1094 1665 1521 1797 827 365 203...  \n",
       "8  1287 1073 2439 1797 2151 365 2551 1059 1094 16...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('/tmp/submission.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f389f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:15.130586Z",
     "iopub.status.busy": "2024-12-11T18:43:15.130333Z",
     "iopub.status.idle": "2024-12-11T18:43:15.143389Z",
     "shell.execute_reply": "2024-12-11T18:43:15.142656Z"
    },
    "papermill": {
     "duration": 0.040707,
     "end_time": "2024-12-11T18:43:15.144944",
     "exception": false,
     "start_time": "2024-12-11T18:43:15.104237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>misconception_id_retrieve</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>[706, 1345, 1507, 1672, 2306, 1516, 1005, 1963...</td>\n",
       "      <td>706 1345 1507 1672 2306 1516 1005 328 2586 133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>[1345, 706, 2306, 1507, 2532, 1672, 1005, 1392...</td>\n",
       "      <td>1345 706 2306 1507 2532 1672 1005 328 2586 196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>[706, 1672, 1345, 1005, 1507, 2532, 2306, 1516...</td>\n",
       "      <td>706 1672 1345 1005 1507 2532 2306 315 2586 328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>[891, 2142, 1755, 143, 2068, 167, 1535, 2078, ...</td>\n",
       "      <td>891 2142 1755 143 2068 167 1535 2398 885 418 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>[891, 143, 1755, 2142, 2398, 167, 2078, 2068, ...</td>\n",
       "      <td>891 143 1755 2142 2398 167 2078 2068 418 885 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>[2142, 1755, 891, 2068, 167, 143, 1535, 1904, ...</td>\n",
       "      <td>2142 1755 891 2068 167 143 1535 2398 885 418 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>[1287, 1073, 2439, 1094, 1665, 1797, 1521, 827...</td>\n",
       "      <td>1287 1073 2439 1094 1665 1797 1521 1059 2551 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>[1287, 1073, 2439, 1094, 1665, 1521, 1797, 827...</td>\n",
       "      <td>1287 1073 2439 1094 1665 1521 1797 1059 2551 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>[1287, 1073, 2439, 1797, 2151, 365, 2551, 1059...</td>\n",
       "      <td>1287 1073 2439 1797 2151 365 2551 1059 2064 21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                          misconception_id_retrieve  \\\n",
       "0            1869_B  [706, 1345, 1507, 1672, 2306, 1516, 1005, 1963...   \n",
       "1            1869_C  [1345, 706, 2306, 1507, 2532, 1672, 1005, 1392...   \n",
       "2            1869_D  [706, 1672, 1345, 1005, 1507, 2532, 2306, 1516...   \n",
       "3            1870_A  [891, 2142, 1755, 143, 2068, 167, 1535, 2078, ...   \n",
       "4            1870_B  [891, 143, 1755, 2142, 2398, 167, 2078, 2068, ...   \n",
       "5            1870_C  [2142, 1755, 891, 2068, 167, 143, 1535, 1904, ...   \n",
       "6            1871_A  [1287, 1073, 2439, 1094, 1665, 1797, 1521, 827...   \n",
       "7            1871_C  [1287, 1073, 2439, 1094, 1665, 1521, 1797, 827...   \n",
       "8            1871_D  [1287, 1073, 2439, 1797, 2151, 365, 2551, 1059...   \n",
       "\n",
       "                                     MisconceptionId  \n",
       "0  706 1345 1507 1672 2306 1516 1005 328 2586 133...  \n",
       "1  1345 706 2306 1507 2532 1672 1005 328 2586 196...  \n",
       "2  706 1672 1345 1005 1507 2532 2306 315 2586 328...  \n",
       "3  891 2142 1755 143 2068 167 1535 2398 885 418 3...  \n",
       "4  891 143 1755 2142 2398 167 2078 2068 418 885 8...  \n",
       "5  2142 1755 891 2068 167 143 1535 2398 885 418 1...  \n",
       "6  1287 1073 2439 1094 1665 1797 1521 1059 2551 1...  \n",
       "7  1287 1073 2439 1094 1665 1521 1797 1059 2551 6...  \n",
       "8  1287 1073 2439 1797 2151 365 2551 1059 2064 21...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678f89f",
   "metadata": {
    "papermill": {
     "duration": 0.02481,
     "end_time": "2024-12-11T18:43:15.194783",
     "exception": false,
     "start_time": "2024-12-11T18:43:15.169973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Rerank 2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d46a51a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:15.246131Z",
     "iopub.status.busy": "2024-12-11T18:43:15.245653Z",
     "iopub.status.idle": "2024-12-11T18:43:15.251276Z",
     "shell.execute_reply": "2024-12-11T18:43:15.250509Z"
    },
    "papermill": {
     "duration": 0.033079,
     "end_time": "2024-12-11T18:43:15.252930",
     "exception": false,
     "start_time": "2024-12-11T18:43:15.219851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rerank_top_n_2nd = 10\n",
    "target_file_2nd = '/tmp/submission_7to50_reranked.pkl'\n",
    "\n",
    "out_path_2nd_72b = \"/tmp/prompts_df_sorted.pkl\"\n",
    "out_path_2nd_14b = \"/tmp/prompts_df_sorted_14B.pkl\"\n",
    "out_path_2nd_72b_tta = \"/tmp/prompts_df_sorted_tta.pkl\"\n",
    "#out_path_2nd_32b = \"/tmp/prompts_df_sorted.pkl\"\n",
    "#out_path_2nd_32b_tta = \"/tmp/prompts_df_sorted_tta.pkl\"\n",
    "#out_path_2nd_14b_tta = \"/tmp/prompts_df_sorted_14B_tta.pkl\"\n",
    "submission_target_2nd = pd.read_pickle(target_file_2nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3057802a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:43:15.304759Z",
     "iopub.status.busy": "2024-12-11T18:43:15.304518Z",
     "iopub.status.idle": "2024-12-11T18:51:10.437828Z",
     "shell.execute_reply": "2024-12-11T18:51:10.436652Z"
    },
    "papermill": {
     "duration": 475.161407,
     "end_time": "2024-12-11T18:51:10.439988",
     "exception": false,
     "start_time": "2024-12-11T18:43:15.278581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args=Namespace(model_name='/kaggle/input/eedi-top25-qwen72b/transformers/qlora-gen2.5-merged/1', sub_pkl_name='/tmp/submission_7to50_reranked.pkl', use_construct=False, out_name='/tmp/prompts_df_sorted.pkl', quantization='gptq', top_n=10, reverse=False)\r\n",
      "WARNING 12-11 18:43:18 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\n",
      "WARNING 12-11 18:43:25 config.py:1668] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-11 18:43:34 config.py:321] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-11 18:43:34 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-11 18:43:34 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-11 18:43:34 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/eedi-top25-qwen72b/transformers/qlora-gen2.5-merged/1', speculative_config=None, tokenizer='/kaggle/input/eedi-top25-qwen72b/transformers/qlora-gen2.5-merged/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/eedi-top25-qwen72b/transformers/qlora-gen2.5-merged/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "INFO 12-11 18:43:34 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 12-11 18:43:34 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:43:34 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:43:34 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-11 18:43:34 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:43:35 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:43:36 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-11 18:43:36 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:43:36 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 18:43:36 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:43:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-11 18:43:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 12-11 18:43:36 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m WARNING 12-11 18:43:36 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-11 18:43:36 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x794fb9481540>, local_subscribe_port=35225, remote_subscribe_port=None)\r\n",
      "INFO 12-11 18:43:36 model_runner.py:1056] Starting to load model /kaggle/input/eedi-top25-qwen72b/transformers/qlora-gen2.5-merged/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:43:36 model_runner.py:1056] Starting to load model /kaggle/input/eedi-top25-qwen72b/transformers/qlora-gen2.5-merged/1...\r\n",
      "INFO 12-11 18:43:37 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 18:43:37 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:43:37 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:43:37 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [06:36<00:00, 396.12s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [06:36<00:00, 396.14s/it]\r\n",
      "\r\n",
      "INFO 12-11 18:50:24 model_runner.py:1067] Loading model weights took 11.3453 GB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=424)\u001b[0;0m INFO 12-11 18:50:24 model_runner.py:1067] Loading model weights took 11.3453 GB\r\n",
      "INFO 12-11 18:50:34 distributed_gpu_executor.py:57] # GPU blocks: 362, # CPU blocks: 409\r\n",
      "INFO 12-11 18:50:34 distributed_gpu_executor.py:61] Maximum concurrency for 2000 tokens per request: 2.90x\r\n",
      "Force predictions to be tokens [32, 33, 34, 35, 36, 37, 38, 39, 40, 41] which are ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'].\r\n",
      "Processed prompts: 100%|█| 9/9 [00:21<00:00,  2.42s/it, est. speed input: 159.73\r\n",
      "Inference of 9 samples took 0.3638289252916972 minutes!\r\n",
      "INFO 12-11 18:51:01 multiproc_worker_utils.py:120] Killing local vLLM worker processes\r\n"
     ]
    }
   ],
   "source": [
    "!python rerank_72b.py \"/kaggle/input/eedi-top25-qwen72b/transformers/qlora-gen2.5-merged/1\" --sub_pkl_name $target_file_2nd --out_name $out_path_2nd_72b --quantization \"gptq\" --top_n $rerank_top_n_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65c7aec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:51:10.496213Z",
     "iopub.status.busy": "2024-12-11T18:51:10.495907Z",
     "iopub.status.idle": "2024-12-11T18:59:11.397610Z",
     "shell.execute_reply": "2024-12-11T18:59:11.396598Z"
    },
    "papermill": {
     "duration": 480.931763,
     "end_time": "2024-12-11T18:59:11.399778",
     "exception": false,
     "start_time": "2024-12-11T18:51:10.468015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args=Namespace(model_name='/kaggle/input/llama-33-70b-reranker-awq/transformers/1/1', sub_pkl_name='/tmp/submission_7to50_reranked.pkl', use_construct=False, out_name='/tmp/prompts_df_sorted_tta.pkl', quantization='awq', top_n=10, reverse=False)\r\n",
      "WARNING 12-11 18:51:15 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\n",
      "WARNING 12-11 18:51:35 config.py:321] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-11 18:51:35 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-11 18:51:35 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-11 18:51:35 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/llama-33-70b-reranker-awq/transformers/1/1', speculative_config=None, tokenizer='/kaggle/input/llama-33-70b-reranker-awq/transformers/1/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/llama-33-70b-reranker-awq/transformers/1/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "INFO 12-11 18:51:35 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 12-11 18:51:36 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:51:36 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 18:51:36 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:51:36 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:51:36 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-11 18:51:38 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:51:38 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:51:38 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 18:51:38 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 18:51:38 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:51:38 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m WARNING 12-11 18:51:38 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "WARNING 12-11 18:51:38 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-11 18:51:38 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7d756c188700>, local_subscribe_port=42079, remote_subscribe_port=None)\r\n",
      "INFO 12-11 18:51:38 model_runner.py:1056] Starting to load model /kaggle/input/llama-33-70b-reranker-awq/transformers/1/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:51:38 model_runner.py:1056] Starting to load model /kaggle/input/llama-33-70b-reranker-awq/transformers/1/1...\r\n",
      "INFO 12-11 18:51:38 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:51:38 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 18:51:38 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:51:38 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:35<04:45, 35.73s/it]\r\n",
      "Loading safetensors checkpoint shards:  22% Completed | 2/9 [01:26<05:10, 44.37s/it]\r\n",
      "Loading safetensors checkpoint shards:  33% Completed | 3/9 [02:06<04:16, 42.70s/it]\r\n",
      "Loading safetensors checkpoint shards:  44% Completed | 4/9 [02:59<03:53, 46.68s/it]\r\n",
      "Loading safetensors checkpoint shards:  56% Completed | 5/9 [03:10<02:15, 33.87s/it]\r\n",
      "Loading safetensors checkpoint shards:  67% Completed | 6/9 [04:03<02:00, 40.30s/it]\r\n",
      "Loading safetensors checkpoint shards:  78% Completed | 7/9 [04:54<01:27, 43.70s/it]\r\n",
      "Loading safetensors checkpoint shards:  89% Completed | 8/9 [05:47<00:46, 46.63s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [06:36<00:00, 47.34s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [06:36<00:00, 44.01s/it]\r\n",
      "\r\n",
      "INFO 12-11 18:58:25 model_runner.py:1067] Loading model weights took 10.5511 GB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=555)\u001b[0;0m INFO 12-11 18:58:25 model_runner.py:1067] Loading model weights took 10.5511 GB\r\n",
      "INFO 12-11 18:58:36 distributed_gpu_executor.py:57] # GPU blocks: 850, # CPU blocks: 409\r\n",
      "INFO 12-11 18:58:36 distributed_gpu_executor.py:61] Maximum concurrency for 2000 tokens per request: 6.80x\r\n",
      "Force predictions to be tokens [32, 33, 34, 35, 36, 37, 38, 39, 40, 41] which are ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'].\r\n",
      "Processed prompts: 100%|█| 9/9 [00:23<00:00,  2.65s/it, est. speed input: 153.49\r\n",
      "Inference of 9 samples took 0.39801179567972816 minutes!\r\n",
      "ERROR 12-11 18:59:05 multiproc_worker_utils.py:116] Worker VllmWorkerProcess pid 555 died, exit code: -15\r\n",
      "Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stdout>'> at interpreter shutdown, possibly due to daemon threads\r\n",
      "Python runtime state: finalizing (tstate=0x0000570e87cd9d40)\r\n",
      "\r\n",
      "Current thread 0x00007d76d0497740 (most recent call first):\r\n",
      "  <no Python frame>\r\n",
      "\r\n",
      "Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, numexpr.interpreter, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cffi_backend, zstandard.backend_c, yaml._yaml, markupsafe._speedups, PIL._imaging, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, msgspec._core, sentencepiece._sentencepiece, PIL._imagingft, google._upb._message, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, cython.cimports.libc.math, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, jaxlib.cpu_feature_guard, regex._regex, msgpack._cmsgpack, setproctitle, uvloop.loop, ray._raylet, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, lz4._version, lz4.frame._frame, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.optimize._direct, zmq.backend.cython._zmq, cuda_utils, __triton_launcher (total: 174)\r\n"
     ]
    }
   ],
   "source": [
    "!python rerank_72b.py \"/kaggle/input/llama-33-70b-reranker-awq/transformers/1/1\" --sub_pkl_name $target_file_2nd --out_name $out_path_2nd_72b_tta --quantization \"awq\" --top_n $rerank_top_n_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42bb4b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:59:11.460644Z",
     "iopub.status.busy": "2024-12-11T18:59:11.460269Z",
     "iopub.status.idle": "2024-12-11T18:59:11.464746Z",
     "shell.execute_reply": "2024-12-11T18:59:11.464079Z"
    },
    "papermill": {
     "duration": 0.037603,
     "end_time": "2024-12-11T18:59:11.466306",
     "exception": false,
     "start_time": "2024-12-11T18:59:11.428703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python rerank.py \"/kaggle/input/eedi-top25-qwen32b/transformers/merged-gen2.5/1\" --sub_pkl_name $target_file_2nd --out_name $out_path_2nd_32b --use_construct --top_n $rerank_top_n_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7967530c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:59:11.523620Z",
     "iopub.status.busy": "2024-12-11T18:59:11.523361Z",
     "iopub.status.idle": "2024-12-11T18:59:11.527073Z",
     "shell.execute_reply": "2024-12-11T18:59:11.526268Z"
    },
    "papermill": {
     "duration": 0.034271,
     "end_time": "2024-12-11T18:59:11.528735",
     "exception": false,
     "start_time": "2024-12-11T18:59:11.494464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python rerank.py \"/kaggle/input/eedi-top25-qwen32b/transformers/merged-gen2.5/1\" --sub_pkl_name $target_file_2nd --out_name $out_path_2nd_32b_tta --use_construct --top_n $rerank_top_n_2nd --reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5e6ceae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T18:59:11.585741Z",
     "iopub.status.busy": "2024-12-11T18:59:11.585486Z",
     "iopub.status.idle": "2024-12-11T19:01:01.251725Z",
     "shell.execute_reply": "2024-12-11T19:01:01.250448Z"
    },
    "papermill": {
     "duration": 109.697327,
     "end_time": "2024-12-11T19:01:01.253953",
     "exception": false,
     "start_time": "2024-12-11T18:59:11.556626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args=Namespace(model_name='/kaggle/input/eedi-top25-qwen14b/transformers/merged/2', sub_pkl_name='/tmp/submission_7to50_reranked.pkl', use_construct=False, out_name='/tmp/prompts_df_sorted_14B.pkl', quantization='gptq', top_n=10, reverse=False)\r\n",
      "WARNING 12-11 18:59:17 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\n",
      "WARNING 12-11 18:59:38 config.py:321] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-11 18:59:38 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-11 18:59:38 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-11 18:59:38 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/eedi-top25-qwen14b/transformers/merged/2', speculative_config=None, tokenizer='/kaggle/input/eedi-top25-qwen14b/transformers/merged/2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/eedi-top25-qwen14b/transformers/merged/2, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "INFO 12-11 18:59:39 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 12-11 18:59:39 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 18:59:39 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 18:59:39 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 18:59:39 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 18:59:40 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-11 18:59:41 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 18:59:41 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 18:59:41 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 18:59:41 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 18:59:41 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-11 18:59:41 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m WARNING 12-11 18:59:41 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "WARNING 12-11 18:59:41 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-11 18:59:41 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7e7218752050>, local_subscribe_port=48749, remote_subscribe_port=None)\r\n",
      "INFO 12-11 18:59:41 model_runner.py:1056] Starting to load model /kaggle/input/eedi-top25-qwen14b/transformers/merged/2...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 18:59:41 model_runner.py:1056] Starting to load model /kaggle/input/eedi-top25-qwen14b/transformers/merged/2...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 18:59:42 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 18:59:42 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-11 18:59:42 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 18:59:42 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:57<00:00, 57.17s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:57<00:00, 57.17s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=685)\u001b[0;0m INFO 12-11 19:00:40 model_runner.py:1067] Loading model weights took 4.6681 GB\r\n",
      "INFO 12-11 19:00:40 model_runner.py:1067] Loading model weights took 4.6681 GB\r\n",
      "INFO 12-11 19:00:42 distributed_gpu_executor.py:57] # GPU blocks: 4556, # CPU blocks: 2730\r\n",
      "INFO 12-11 19:00:42 distributed_gpu_executor.py:61] Maximum concurrency for 2048 tokens per request: 35.59x\r\n",
      "Force predictions to be tokens [32, 33, 34, 35, 36, 37, 38, 39, 40, 41] which are ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'].\r\n",
      "Processed prompts: 100%|█| 9/9 [00:07<00:00,  1.27it/s, est. speed input: 491.47\r\n",
      "Inference of 9 samples took 0.11826343933741251 minutes!\r\n",
      "INFO 12-11 19:00:57 multiproc_worker_utils.py:120] Killing local vLLM worker processes\r\n",
      "Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stdout>'> at interpreter shutdown, possibly due to daemon threads\r\n",
      "Python runtime state: finalizing (tstate=0x00005b84757a4cc0)\r\n",
      "\r\n",
      "Current thread 0x00007e73838f2740 (most recent call first):\r\n",
      "  <no Python frame>\r\n",
      "\r\n",
      "Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, numexpr.interpreter, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cffi_backend, zstandard.backend_c, yaml._yaml, markupsafe._speedups, PIL._imaging, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, msgspec._core, sentencepiece._sentencepiece, PIL._imagingft, google._upb._message, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, cython.cimports.libc.math, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, jaxlib.cpu_feature_guard, regex._regex, msgpack._cmsgpack, setproctitle, uvloop.loop, ray._raylet, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, lz4._version, lz4.frame._frame, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.optimize._direct, zmq.backend.cython._zmq, cuda_utils, __triton_launcher (total: 174)\r\n"
     ]
    }
   ],
   "source": [
    "!python rerank.py \"/kaggle/input/eedi-top25-qwen14b/transformers/merged/2\" --sub_pkl_name $target_file_2nd --out_name $out_path_2nd_14b --quantization \"gptq\" --top_n $rerank_top_n_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2eb8f4ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.320485Z",
     "iopub.status.busy": "2024-12-11T19:01:01.319584Z",
     "iopub.status.idle": "2024-12-11T19:01:01.324320Z",
     "shell.execute_reply": "2024-12-11T19:01:01.323442Z"
    },
    "papermill": {
     "duration": 0.040291,
     "end_time": "2024-12-11T19:01:01.326096",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.285805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python rerank.py \"/kaggle/input/eedi-top25-qwen14b/transformers/merged/2\" --sub_pkl_name $target_file_2nd --out_name $out_path_2nd_14b_tta --quantization \"gptq\" --top_n $rerank_top_n_2nd --reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd3c1de5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.393184Z",
     "iopub.status.busy": "2024-12-11T19:01:01.392558Z",
     "iopub.status.idle": "2024-12-11T19:01:01.414911Z",
     "shell.execute_reply": "2024-12-11T19:01:01.414298Z"
    },
    "papermill": {
     "duration": 0.057548,
     "end_time": "2024-12-11T19:01:01.416712",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.359164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts_df_sorted1 = pd.read_pickle(out_path_2nd_72b)\n",
    "#prompts_df_sorted1 = pd.read_pickle(out_path_2nd_32b)\n",
    "prompts_df_sorted2 = pd.read_pickle(out_path_2nd_14b)\n",
    "prompts_df_sorted3 = pd.read_pickle(out_path_2nd_72b_tta)\n",
    "#prompts_df_sorted3 = pd.read_pickle(out_path_2nd_32b_tta)\n",
    "#prompts_df_sorted4 = pd.read_pickle(out_path_2nd_14b_tta)\n",
    "\n",
    "\n",
    "# 同じ順に並び替え\n",
    "prompts_df_sorted2 = prompts_df_sorted1[[\"QuestionId_Answer\"]].merge(prompts_df_sorted2, on=\"QuestionId_Answer\", how=\"left\")\n",
    "prompts_df_sorted3 = prompts_df_sorted1[[\"QuestionId_Answer\"]].merge(prompts_df_sorted3, on=\"QuestionId_Answer\", how=\"left\")\n",
    "#prompts_df_sorted4 = prompts_df_sorted1[[\"QuestionId_Answer\"]].merge(prompts_df_sorted4, on=\"QuestionId_Answer\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "707511bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.477083Z",
     "iopub.status.busy": "2024-12-11T19:01:01.476821Z",
     "iopub.status.idle": "2024-12-11T19:01:01.486643Z",
     "shell.execute_reply": "2024-12-11T19:01:01.485907Z"
    },
    "papermill": {
     "duration": 0.042309,
     "end_time": "2024-12-11T19:01:01.488247",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.445938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 重み付けアンサンブル\n",
    "\n",
    "score_cols = prompts_df_sorted1.columns[prompts_df_sorted1.columns.str.startswith(\"score_\")]\n",
    "\n",
    "prompts_df_sorted1.loc[:, score_cols] = \\\n",
    "0.4 * prompts_df_sorted1[score_cols] + 0.3 * prompts_df_sorted3[score_cols] + 0.3 * prompts_df_sorted2[score_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c3db12e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.547445Z",
     "iopub.status.busy": "2024-12-11T19:01:01.547160Z",
     "iopub.status.idle": "2024-12-11T19:01:01.550860Z",
     "shell.execute_reply": "2024-12-11T19:01:01.550051Z"
    },
    "papermill": {
     "duration": 0.035072,
     "end_time": "2024-12-11T19:01:01.552346",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.517274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prompts_df_sorted1[score_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1922beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.615752Z",
     "iopub.status.busy": "2024-12-11T19:01:01.615467Z",
     "iopub.status.idle": "2024-12-11T19:01:01.619056Z",
     "shell.execute_reply": "2024-12-11T19:01:01.618303Z"
    },
    "papermill": {
     "duration": 0.036205,
     "end_time": "2024-12-11T19:01:01.620830",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.584625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prompts_df_sorted3[score_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f136aba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.683834Z",
     "iopub.status.busy": "2024-12-11T19:01:01.683133Z",
     "iopub.status.idle": "2024-12-11T19:01:01.686934Z",
     "shell.execute_reply": "2024-12-11T19:01:01.686114Z"
    },
    "papermill": {
     "duration": 0.037378,
     "end_time": "2024-12-11T19:01:01.688776",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.651398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prompts_df_sorted2[score_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02239c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.752459Z",
     "iopub.status.busy": "2024-12-11T19:01:01.752107Z",
     "iopub.status.idle": "2024-12-11T19:01:01.756391Z",
     "shell.execute_reply": "2024-12-11T19:01:01.755644Z"
    },
    "papermill": {
     "duration": 0.037449,
     "end_time": "2024-12-11T19:01:01.758084",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.720635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prompts_df_sorted4[score_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f3b80e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.820147Z",
     "iopub.status.busy": "2024-12-11T19:01:01.819828Z",
     "iopub.status.idle": "2024-12-11T19:01:01.838058Z",
     "shell.execute_reply": "2024-12-11T19:01:01.837316Z"
    },
    "papermill": {
     "duration": 0.051319,
     "end_time": "2024-12-11T19:01:01.839773",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.788454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>score_A</th>\n",
       "      <th>score_B</th>\n",
       "      <th>score_C</th>\n",
       "      <th>score_D</th>\n",
       "      <th>score_E</th>\n",
       "      <th>score_F</th>\n",
       "      <th>score_G</th>\n",
       "      <th>score_H</th>\n",
       "      <th>score_I</th>\n",
       "      <th>score_J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.077959</td>\n",
       "      <td>0.089231</td>\n",
       "      <td>0.168036</td>\n",
       "      <td>0.173265</td>\n",
       "      <td>0.194258</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>0.039040</td>\n",
       "      <td>0.090677</td>\n",
       "      <td>0.059204</td>\n",
       "      <td>0.062431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.090062</td>\n",
       "      <td>0.080256</td>\n",
       "      <td>0.187268</td>\n",
       "      <td>0.262536</td>\n",
       "      <td>0.100791</td>\n",
       "      <td>0.083157</td>\n",
       "      <td>0.036835</td>\n",
       "      <td>0.074557</td>\n",
       "      <td>0.059986</td>\n",
       "      <td>0.024553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.122846</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.248712</td>\n",
       "      <td>0.138671</td>\n",
       "      <td>0.059759</td>\n",
       "      <td>0.184264</td>\n",
       "      <td>0.047713</td>\n",
       "      <td>0.063835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.346235</td>\n",
       "      <td>0.039722</td>\n",
       "      <td>0.159133</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.282259</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>0.046707</td>\n",
       "      <td>0.049715</td>\n",
       "      <td>0.008583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.359583</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>0.189848</td>\n",
       "      <td>0.031369</td>\n",
       "      <td>0.064348</td>\n",
       "      <td>0.203806</td>\n",
       "      <td>0.022874</td>\n",
       "      <td>0.026173</td>\n",
       "      <td>0.010794</td>\n",
       "      <td>0.055414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.047441</td>\n",
       "      <td>0.210075</td>\n",
       "      <td>0.312799</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>0.237114</td>\n",
       "      <td>0.027824</td>\n",
       "      <td>0.017020</td>\n",
       "      <td>0.050053</td>\n",
       "      <td>0.062934</td>\n",
       "      <td>0.010714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.953030</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.032863</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.936540</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.003573</td>\n",
       "      <td>0.049393</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.001691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are a mathematics teac...</td>\n",
       "      <td>0.038867</td>\n",
       "      <td>0.909319</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.010455</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.032876</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                             Prompt  \\\n",
       "0            1869_B  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "1            1869_C  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "2            1869_D  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "3            1870_A  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "4            1870_B  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "5            1870_C  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "6            1871_A  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "7            1871_C  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "8            1871_D  <|im_start|>system\\nYou are a mathematics teac...   \n",
       "\n",
       "    score_A   score_B   score_C   score_D   score_E   score_F   score_G  \\\n",
       "0  0.077959  0.089231  0.168036  0.173265  0.194258  0.045899  0.039040   \n",
       "1  0.090062  0.080256  0.187268  0.262536  0.100791  0.083157  0.036835   \n",
       "2  0.081300  0.122846  0.016343  0.036558  0.248712  0.138671  0.059759   \n",
       "3  0.346235  0.039722  0.159133  0.027559  0.026528  0.282259  0.013559   \n",
       "4  0.359583  0.035790  0.189848  0.031369  0.064348  0.203806  0.022874   \n",
       "5  0.047441  0.210075  0.312799  0.024025  0.237114  0.027824  0.017020   \n",
       "6  0.953030  0.006042  0.000225  0.000244  0.000498  0.005538  0.000883   \n",
       "7  0.936540  0.004092  0.000375  0.000363  0.000648  0.002483  0.003573   \n",
       "8  0.038867  0.909319  0.001271  0.010455  0.002211  0.000415  0.001241   \n",
       "\n",
       "    score_H   score_I   score_J  \n",
       "0  0.090677  0.059204  0.062431  \n",
       "1  0.074557  0.059986  0.024553  \n",
       "2  0.184264  0.047713  0.063835  \n",
       "3  0.046707  0.049715  0.008583  \n",
       "4  0.026173  0.010794  0.055414  \n",
       "5  0.050053  0.062934  0.010714  \n",
       "6  0.032863  0.000459  0.000217  \n",
       "7  0.049393  0.000844  0.001691  \n",
       "8  0.032876  0.003091  0.000254  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_df_sorted = prompts_df_sorted1\n",
    "prompts_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3885c80c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.904960Z",
     "iopub.status.busy": "2024-12-11T19:01:01.904172Z",
     "iopub.status.idle": "2024-12-11T19:01:01.919605Z",
     "shell.execute_reply": "2024-12-11T19:01:01.918710Z"
    },
    "papermill": {
     "duration": 0.048748,
     "end_time": "2024-12-11T19:01:01.921478",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.872730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>misconception_id_retrieve</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>[706, 1345, 1507, 1672, 2306, 1516, 1005, 1963...</td>\n",
       "      <td>706 1345 1507 1672 2306 1516 1005 328 2586 133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>[1345, 706, 2306, 1507, 2532, 1672, 1005, 1392...</td>\n",
       "      <td>1345 706 2306 1507 2532 1672 1005 328 2586 196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>[706, 1672, 1345, 1005, 1507, 2532, 2306, 1516...</td>\n",
       "      <td>706 1672 1345 1005 1507 2532 2306 315 2586 328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>[891, 2142, 1755, 143, 2068, 167, 1535, 2078, ...</td>\n",
       "      <td>891 2142 1755 143 2068 167 1535 2398 885 418 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>[891, 143, 1755, 2142, 2398, 167, 2078, 2068, ...</td>\n",
       "      <td>891 143 1755 2142 2398 167 2078 2068 418 885 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>[2142, 1755, 891, 2068, 167, 143, 1535, 1904, ...</td>\n",
       "      <td>2142 1755 891 2068 167 143 1535 2398 885 418 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>[1287, 1073, 2439, 1094, 1665, 1797, 1521, 827...</td>\n",
       "      <td>1287 1073 2439 1094 1665 1797 1521 1059 2551 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>[1287, 1073, 2439, 1094, 1665, 1521, 1797, 827...</td>\n",
       "      <td>1287 1073 2439 1094 1665 1521 1797 1059 2551 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>[1287, 1073, 2439, 1797, 2151, 365, 2551, 1059...</td>\n",
       "      <td>1287 1073 2439 1797 2151 365 2551 1059 2064 21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                          misconception_id_retrieve  \\\n",
       "0            1869_B  [706, 1345, 1507, 1672, 2306, 1516, 1005, 1963...   \n",
       "1            1869_C  [1345, 706, 2306, 1507, 2532, 1672, 1005, 1392...   \n",
       "2            1869_D  [706, 1672, 1345, 1005, 1507, 2532, 2306, 1516...   \n",
       "3            1870_A  [891, 2142, 1755, 143, 2068, 167, 1535, 2078, ...   \n",
       "4            1870_B  [891, 143, 1755, 2142, 2398, 167, 2078, 2068, ...   \n",
       "5            1870_C  [2142, 1755, 891, 2068, 167, 143, 1535, 1904, ...   \n",
       "6            1871_A  [1287, 1073, 2439, 1094, 1665, 1797, 1521, 827...   \n",
       "7            1871_C  [1287, 1073, 2439, 1094, 1665, 1521, 1797, 827...   \n",
       "8            1871_D  [1287, 1073, 2439, 1797, 2151, 365, 2551, 1059...   \n",
       "\n",
       "                                     MisconceptionId  \n",
       "0  706 1345 1507 1672 2306 1516 1005 328 2586 133...  \n",
       "1  1345 706 2306 1507 2532 1672 1005 328 2586 196...  \n",
       "2  706 1672 1345 1005 1507 2532 2306 315 2586 328...  \n",
       "3  891 2142 1755 143 2068 167 1535 2398 885 418 3...  \n",
       "4  891 143 1755 2142 2398 167 2078 2068 418 885 8...  \n",
       "5  2142 1755 891 2068 167 143 1535 2398 885 418 1...  \n",
       "6  1287 1073 2439 1094 1665 1797 1521 1059 2551 1...  \n",
       "7  1287 1073 2439 1094 1665 1521 1797 1059 2551 6...  \n",
       "8  1287 1073 2439 1797 2151 365 2551 1059 2064 21...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_target_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f50c06f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:01.984658Z",
     "iopub.status.busy": "2024-12-11T19:01:01.984422Z",
     "iopub.status.idle": "2024-12-11T19:01:02.007813Z",
     "shell.execute_reply": "2024-12-11T19:01:02.006972Z"
    },
    "papermill": {
     "duration": 0.05559,
     "end_time": "2024-12-11T19:01:02.009457",
     "exception": false,
     "start_time": "2024-12-11T19:01:01.953867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>2306 1672 1507 328 1345 706 1338 2586 1516 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1507 2306 2532 1345 1672 706 328 2586 1005 196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>1507 315 2532 1672 706 328 2306 2586 1005 1345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>891 167 1755 885 2398 2142 143 2068 1535 418 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>891 167 1755 2398 885 143 2142 2068 2078 418 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>891 167 1755 885 2398 2142 143 2068 1535 418 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1287 1059 1073 1797 1521 1665 2551 1094 2439 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1287 1059 1073 1797 1521 632 2551 1665 2439 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1073 1287 1059 1797 2064 2151 2439 2551 365 21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  2306 1672 1507 328 1345 706 1338 2586 1516 100...\n",
       "1            1869_C  1507 2306 2532 1345 1672 706 328 2586 1005 196...\n",
       "2            1869_D  1507 315 2532 1672 706 328 2306 2586 1005 1345...\n",
       "3            1870_A  891 167 1755 885 2398 2142 143 2068 1535 418 3...\n",
       "4            1870_B  891 167 1755 2398 885 143 2142 2068 2078 418 8...\n",
       "5            1870_C  891 167 1755 885 2398 2142 143 2068 1535 418 1...\n",
       "6            1871_A  1287 1059 1073 1797 1521 1665 2551 1094 2439 1...\n",
       "7            1871_C  1287 1059 1073 1797 1521 632 2551 1665 2439 10...\n",
       "8            1871_D  1073 1287 1059 1797 2064 2151 2439 2551 365 21..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_rerank = rerank(prompts_df_sorted, submission_target_2nd, out_path=\"./submission.csv\", RANK_TOP_N=rerank_top_n_2nd)\n",
    "submission_rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c54eed4",
   "metadata": {
    "papermill": {
     "duration": 0.029501,
     "end_time": "2024-12-11T19:01:02.070195",
     "exception": false,
     "start_time": "2024-12-11T19:01:02.040694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9edaf9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:02.131014Z",
     "iopub.status.busy": "2024-12-11T19:01:02.130785Z",
     "iopub.status.idle": "2024-12-11T19:01:03.134298Z",
     "shell.execute_reply": "2024-12-11T19:01:03.133275Z"
    },
    "papermill": {
     "duration": 1.03642,
     "end_time": "2024-12-11T19:01:03.136273",
     "exception": false,
     "start_time": "2024-12-11T19:01:02.099853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm rerank.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4824b622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T19:01:03.198473Z",
     "iopub.status.busy": "2024-12-11T19:01:03.198174Z",
     "iopub.status.idle": "2024-12-11T19:01:04.189121Z",
     "shell.execute_reply": "2024-12-11T19:01:04.188143Z"
    },
    "papermill": {
     "duration": 1.023857,
     "end_time": "2024-12-11T19:01:04.191228",
     "exception": false,
     "start_time": "2024-12-11T19:01:03.167371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm rerank_72b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eaed12",
   "metadata": {
    "papermill": {
     "duration": 0.03024,
     "end_time": "2024-12-11T19:01:04.252612",
     "exception": false,
     "start_time": "2024-12-11T19:01:04.222372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5934956,
     "sourceId": 9704411,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6083063,
     "sourceId": 9902277,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6138067,
     "sourceId": 9975980,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5980827,
     "sourceId": 10016119,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6183071,
     "sourceId": 10038081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6191318,
     "sourceId": 10050205,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6205676,
     "sourceId": 10090822,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5933492,
     "sourceId": 10101508,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10150129,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6282979,
     "sourceId": 10172862,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 179142715,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 204710089,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 205348876,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 207563780,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 209384666,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 212532794,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 118183,
     "sourceId": 139552,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 145467,
     "modelInstanceId": 122385,
     "sourceId": 144593,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 150852,
     "modelInstanceId": 127932,
     "sourceId": 150653,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 151522,
     "modelInstanceId": 128649,
     "sourceId": 151489,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 147618,
     "modelInstanceId": 129463,
     "sourceId": 152437,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 166529,
     "modelInstanceId": 147491,
     "sourceId": 173255,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 170763,
     "modelInstanceId": 148254,
     "sourceId": 174153,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 166105,
     "modelInstanceId": 143514,
     "sourceId": 176431,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 186118,
     "modelInstanceId": 164324,
     "sourceId": 192702,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 186755,
     "modelInstanceId": 164436,
     "sourceId": 192827,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 187896,
     "modelInstanceId": 165567,
     "sourceId": 194194,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 186797,
     "modelInstanceId": 164470,
     "sourceId": 194262,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1825.172682,
   "end_time": "2024-12-11T19:01:04.702379",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-11T18:30:39.529697",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
