exp_type: "misconception-retrieve"
exp_name: "e032-mistral-synthetic-gen1-gen2-gen3"
seed: 42

# phase: train
n_folds: 5
max_epochs: &max_epochs 60
check_val_every_n_epoch: 1
num_classes: &num_classes 2587
use_lora: true

# cpu/gpu
sync_batchnorm: True
precision: "16-mixed"
num_workers: 16

# dataset
train_batch_size: 1
accumulate_grad_batchs: 6
valid_batch_size: 12
test_batch_size: 8

dataset_config:
  name: "EediClsDatasetV2"
  prompt_name: "mistral"
  num_classes: *num_classes
  params:
    # questionのmax token len 430くらい
    # misconceptionはmax 42
    use_negative_samples: false
    synthetic_negative: false
    max_tokens: 768
    p_synthetic_exist: 0.43
    p_other_exist: 0.3
    p_other_generate: 0.3
      
llm_config:
  name: "EediClsV2"
  backbone: "Linq-AI-Research/Linq-Embed-Mistral"
  num_classes: *num_classes
  use_metrics_learning: true
  metrics_learning_module: 
    module_name: "ArcFace"
    params:
      params1:
        ratio: 1.0
        s: 20.0
        m: 0.3
  cls_head: true
  seq_head: true

loss_config:
  name: Seq2SeqCls2wayLoss
  params:
    seq_weight: 0.5
    cls_weight: 0.5

# lr scheduler
lr_scheduler_config: 
  name: cosine_schedule_with_warmup 
  interval: step
  params: 
    num_cycles: 0.5
    max_epochs: *max_epochs
    warmup_steps_ratio: 0.10

# optimizer
optimizer_config: 
  name: AdamW
  params: 
    lr: 4.0e-5
    weight_decay: 0.001


# logger
logger: wandb