{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eLo9km7SerdA"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"data/synthetic_generation\"\n",
        "MODEL_NAME = \"Qwen/QwQ-32B-Preview\"\n",
        "OUTPUT_PATH = \"Pipeline1/Linq-Embed-Mistral\"\n",
        "MODEL_OUTPUT_PATH = f\"{OUTPUT_PATH}/output_retrieval\"\n",
        "\n",
        "RETRIEVE_NUM = 25\n",
        "SEED = 985\n",
        "EPOCH = 10\n",
        "LR = 4e-05\n",
        "BS = 32\n",
        "\n",
        "TRAINING = True\n",
        "DEBUG = False\n",
        "WANDB = False\n",
        "REPORT_TO = \"none\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hXhowgBLfPGh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-29 01:33:35.863192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1732811615.938360 2485954 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1732811615.958887 2485954 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-29 01:33:36.095256: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import os\n",
        "import random\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import sentence_transformers\n",
        "import wandb\n",
        "from accelerate.utils import release_memory\n",
        "from datasets import Dataset, load_dataset\n",
        "from sentence_transformers import (\n",
        "    SentenceTransformer,\n",
        "    SentenceTransformerTrainer,\n",
        "    SentenceTransformerTrainingArguments,\n",
        "    models\n",
        ")\n",
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator, TripletEvaluator\n",
        "from sentence_transformers.losses import CachedMultipleNegativesRankingLoss, MultipleNegativesRankingLoss\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    # prepare_model_for_int8_training,\n",
        "    set_peft_model_state_dict,\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from transformers import BitsAndBytesConfig, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from llm2vec import LLM2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iFzjng-y-S-q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from fnmatch import fnmatch\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable\n",
        "\n",
        "import huggingface_hub\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, MT5Config, T5Config\n",
        "from transformers.utils import is_peft_available\n",
        "\n",
        "def _save_pretrained_wrapper(_save_pretrained_fn: Callable, subfolder: str) -> Callable[..., None]:\n",
        "    def wrapper(save_directory: str | Path, **kwargs) -> None:\n",
        "        os.makedirs(Path(save_directory) / subfolder, exist_ok=True)\n",
        "        return _save_pretrained_fn(Path(save_directory) / subfolder, **kwargs)\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "class CustomTransformer(models.Transformer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name_or_path: str,\n",
        "        max_seq_length: int | None = None,\n",
        "        model_args: dict[str, Any] | None = None,\n",
        "        tokenizer_args: dict[str, Any] | None = None,\n",
        "        config_args: dict[str, Any] | None = None,\n",
        "        cache_dir: str | None = None,\n",
        "        do_lower_case: bool = False,\n",
        "        tokenizer_name_or_path: str = None,\n",
        "        backend: str = \"torch\",\n",
        "    ) -> None:\n",
        "        super().__init__(model_name_or_path)\n",
        "        self.config_keys = [\"max_seq_length\", \"do_lower_case\"]\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.backend = backend\n",
        "        if model_args is None:\n",
        "            model_args = {}\n",
        "        if tokenizer_args is None:\n",
        "            tokenizer_args = {}\n",
        "        if config_args is None:\n",
        "            config_args = {}\n",
        "\n",
        "        config = AutoConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        self.auto_model = LLM2Vec.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            quantization_config=bnb_config,\n",
        "        ).model\n",
        "        config = LoraConfig(\n",
        "            r=64,\n",
        "            lora_alpha=128,\n",
        "            target_modules=[\n",
        "                \"q_proj\",\n",
        "                \"k_proj\",\n",
        "                \"v_proj\",\n",
        "                \"o_proj\",\n",
        "                \"gate_proj\",\n",
        "                \"up_proj\",\n",
        "                \"down_proj\",\n",
        "            ],\n",
        "            bias=\"none\",\n",
        "            lora_dropout=0.05,\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "\n",
        "        self.auto_model = get_peft_model(self.auto_model, config)\n",
        "        print(self.auto_model.print_trainable_parameters())\n",
        "\n",
        "        if max_seq_length is not None and \"model_max_length\" not in tokenizer_args:\n",
        "            tokenizer_args[\"model_max_length\"] = max_seq_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            tokenizer_name_or_path if tokenizer_name_or_path is not None else model_name_or_path,\n",
        "            cache_dir=cache_dir,\n",
        "            **tokenizer_args,\n",
        "        )\n",
        "\n",
        "        # No max_seq_length set. Try to infer from model\n",
        "        if max_seq_length is None:\n",
        "            if (\n",
        "                hasattr(self.auto_model, \"config\")\n",
        "                and hasattr(self.auto_model.config, \"max_position_embeddings\")\n",
        "                and hasattr(self.tokenizer, \"model_max_length\")\n",
        "            ):\n",
        "                max_seq_length = min(self.auto_model.config.max_position_embeddings, self.tokenizer.model_max_length)\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        if tokenizer_name_or_path is not None:\n",
        "            self.auto_model.config.tokenizer_class = self.tokenizer.__class__.__name__\n",
        "\n",
        "    def _load_model(self, model_name_or_path, config, cache_dir, backend, **model_args) -> None:\n",
        "        \"\"\"Loads the transformer model\"\"\"\n",
        "        if backend == \"torch\":\n",
        "            if isinstance(config, T5Config):\n",
        "                self._load_t5_model(model_name_or_path, config, cache_dir, **model_args)\n",
        "            elif isinstance(config, MT5Config):\n",
        "                self._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n",
        "            else:\n",
        "                self.auto_model = AutoModel.from_pretrained(\n",
        "                    model_name_or_path, config=config, cache_dir=cache_dir, **model_args\n",
        "                )\n",
        "        elif backend == \"onnx\":\n",
        "            self._load_onnx_model(model_name_or_path, config, cache_dir, **model_args)\n",
        "        elif backend == \"openvino\":\n",
        "            self._load_openvino_model(model_name_or_path, config, cache_dir, **model_args)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backend '{backend}'. `backend` should be `torch`, `onnx`, or `openvino`.\")\n",
        "\n",
        "    def _load_openvino_model(self, model_name_or_path, config, cache_dir, **model_args) -> None:\n",
        "        if isinstance(config, T5Config) or isinstance(config, MT5Config):\n",
        "            raise ValueError(\"T5 models are not yet supported by the OpenVINO backend.\")\n",
        "\n",
        "        try:\n",
        "            from optimum.intel import OVModelForFeatureExtraction\n",
        "            from optimum.intel.openvino import OV_XML_FILE_NAME\n",
        "        except ModuleNotFoundError:\n",
        "            raise Exception(\n",
        "                \"Using the OpenVINO backend requires installing Optimum and OpenVINO. \"\n",
        "                \"You can install them with pip: `pip install optimum[openvino]`.\"\n",
        "            )\n",
        "\n",
        "        load_path = Path(model_name_or_path)\n",
        "        is_local = load_path.exists()\n",
        "        backend_name = \"OpenVINO\"\n",
        "        target_file_glob = \"openvino*.xml\"\n",
        "\n",
        "        # Determine whether the model should be exported or whether we can load it directly\n",
        "        export, model_args = self._backend_should_export(\n",
        "            load_path, is_local, model_args, OV_XML_FILE_NAME, target_file_glob, backend_name\n",
        "        )\n",
        "\n",
        "        # If we're exporting, then there's no need for a file_name to load the model from\n",
        "        if export:\n",
        "            model_args.pop(\"file_name\", None)\n",
        "\n",
        "        # ov_config can be either a dictionary, or point to a json file with an OpenVINO config\n",
        "        if \"ov_config\" in model_args:\n",
        "            ov_config = model_args[\"ov_config\"]\n",
        "            if not isinstance(ov_config, dict):\n",
        "                if not Path(ov_config).exists():\n",
        "                    raise ValueError(\n",
        "                        \"ov_config should be a dictionary or a path to a .json file containing an OpenVINO config\"\n",
        "                    )\n",
        "                with open(ov_config, encoding=\"utf-8\") as f:\n",
        "                    model_args[\"ov_config\"] = json.load(f)\n",
        "        else:\n",
        "            model_args[\"ov_config\"] = {}\n",
        "\n",
        "        # Either load an exported model, or export the model to OpenVINO\n",
        "        self.auto_model: OVModelForFeatureExtraction = OVModelForFeatureExtraction.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=cache_dir,\n",
        "            export=export,\n",
        "            **model_args,\n",
        "        )\n",
        "        # Wrap the save_pretrained method to save the model in the correct subfolder\n",
        "        self.auto_model._save_pretrained = _save_pretrained_wrapper(self.auto_model._save_pretrained, self.backend)\n",
        "\n",
        "        # Warn the user to save the model if they haven't already\n",
        "        if export:\n",
        "            self._backend_warn_to_save(model_name_or_path, is_local, backend_name)\n",
        "\n",
        "    def _load_onnx_model(self, model_name_or_path, config, cache_dir, **model_args) -> None:\n",
        "        try:\n",
        "            import onnxruntime as ort\n",
        "            from optimum.onnxruntime import ONNX_WEIGHTS_NAME, ORTModelForFeatureExtraction\n",
        "        except ModuleNotFoundError:\n",
        "            raise Exception(\n",
        "                \"Using the ONNX backend requires installing Optimum and ONNX Runtime. \"\n",
        "                \"You can install them with pip: `pip install optimum[onnxruntime]` \"\n",
        "                \"or `pip install optimum[onnxruntime-gpu]`\"\n",
        "            )\n",
        "\n",
        "        # Default to the highest priority available provider if not specified\n",
        "        # E.g. Tensorrt > CUDA > CPU\n",
        "        model_args[\"provider\"] = model_args.pop(\"provider\", ort.get_available_providers()[0])\n",
        "\n",
        "        load_path = Path(model_name_or_path)\n",
        "        is_local = load_path.exists()\n",
        "        backend_name = \"ONNX\"\n",
        "        target_file_glob = \"*.onnx\"\n",
        "\n",
        "        # Determine whether the model should be exported or whether we can load it directly\n",
        "        export, model_args = self._backend_should_export(\n",
        "            load_path, is_local, model_args, ONNX_WEIGHTS_NAME, target_file_glob, backend_name\n",
        "        )\n",
        "\n",
        "        # If we're exporting, then there's no need for a file_name to load the model from\n",
        "        if export:\n",
        "            model_args.pop(\"file_name\", None)\n",
        "\n",
        "        # Either load an exported model, or export the model to ONNX\n",
        "        self.auto_model: ORTModelForFeatureExtraction = ORTModelForFeatureExtraction.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            config=config,\n",
        "            cache_dir=cache_dir,\n",
        "            export=export,\n",
        "            **model_args,\n",
        "        )\n",
        "        # Wrap the save_pretrained method to save the model in the correct subfolder\n",
        "        self.auto_model._save_pretrained = _save_pretrained_wrapper(self.auto_model._save_pretrained, self.backend)\n",
        "\n",
        "        # Warn the user to save the model if they haven't already\n",
        "        if export:\n",
        "            self._backend_warn_to_save(model_name_or_path, is_local, backend_name)\n",
        "\n",
        "    def _backend_should_export(\n",
        "        self,\n",
        "        load_path: Path,\n",
        "        is_local: bool,\n",
        "        model_args: dict[str, Any],\n",
        "        target_file_name: str,\n",
        "        target_file_glob: str,\n",
        "        backend_name: str,\n",
        "    ) -> tuple[bool, dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Determines whether the model should be exported to the backend, or if it can be loaded directly.\n",
        "        Also update the `file_name` and `subfolder` model_args if necessary.\n",
        "\n",
        "        These are the cases:\n",
        "\n",
        "        1. If export is set in model_args, just return export\n",
        "        2. If `<subfolder>/<file_name>` exists; set export to False\n",
        "        3. If `<backend>/<file_name>` exists; set export to False and set subfolder to the backend (e.g. \"onnx\")\n",
        "        4. If `<file_name>` contains a folder, add those folders to the subfolder and set the file_name to the last part\n",
        "\n",
        "        We will warn if:\n",
        "\n",
        "        1. The expected file does not exist in the model directory given the optional file_name and subfolder.\n",
        "           If there are valid files for this backend, but they're don't align with file_name, then we give a useful warning.\n",
        "        2. Multiple files are found in the model directory that match the target file name and the user did not\n",
        "           specify the desired file name via `model_kwargs={\"file_name\": \"<file_name>\"}`\n",
        "\n",
        "        Args:\n",
        "            load_path: The model repository or directory, as a Path instance\n",
        "            is_local: Whether the model is local or remote, i.e. whether load_path is a local directory\n",
        "            model_args: The model_args dictionary. Notable keys are \"export\", \"file_name\", and \"subfolder\"\n",
        "            target_file_name: The expected file name in the model directory, e.g. \"model.onnx\" or \"openvino_model.xml\"\n",
        "            target_file_glob: The glob pattern to match the target file name, e.g. \"*.onnx\" or \"openvino*.xml\"\n",
        "            backend_name: The human-readable name of the backend for use in warnings, e.g. \"ONNX\" or \"OpenVINO\"\n",
        "\n",
        "        Returns:\n",
        "            Tuple[bool, dict[str, Any]]: A tuple of the export boolean and the updated model_args dictionary.\n",
        "        \"\"\"\n",
        "\n",
        "        export = model_args.pop(\"export\", None)\n",
        "        if export is not None:\n",
        "            return export, model_args\n",
        "\n",
        "        file_name = model_args.get(\"file_name\", target_file_name)\n",
        "        subfolder = model_args.get(\"subfolder\", None)\n",
        "        primary_full_path = Path(subfolder, file_name).as_posix() if subfolder else Path(file_name).as_posix()\n",
        "        secondary_full_path = (\n",
        "            Path(subfolder, self.backend, file_name).as_posix()\n",
        "            if subfolder\n",
        "            else Path(self.backend, file_name).as_posix()\n",
        "        )\n",
        "        glob_pattern = f\"{subfolder}/**/{target_file_glob}\" if subfolder else f\"**/{target_file_glob}\"\n",
        "\n",
        "        # Get the list of files in the model directory that match the target file name\n",
        "        if is_local:\n",
        "            model_file_names = [path.relative_to(load_path).as_posix() for path in load_path.glob(glob_pattern)]\n",
        "        else:\n",
        "            all_files = huggingface_hub.list_repo_files(\n",
        "                load_path.as_posix(),\n",
        "                repo_type=\"model\",\n",
        "                revision=model_args.get(\"revision\", None),\n",
        "                token=model_args.get(\"token\", None),\n",
        "            )\n",
        "            model_file_names = [fname for fname in all_files if fnmatch(fname, glob_pattern)]\n",
        "\n",
        "        # First check if the expected file exists in the root of the model directory\n",
        "        # If it doesn't, check if it exists in the backend subfolder.\n",
        "        # If it does, set the subfolder to include the backend\n",
        "        export = primary_full_path not in model_file_names\n",
        "        if export and \"subfolder\" not in model_args:\n",
        "            export = secondary_full_path not in model_file_names\n",
        "            if not export:\n",
        "                if len(model_file_names) > 1 and \"file_name\" not in model_args:\n",
        "                    logger.warning(\n",
        "                        f\"Multiple {backend_name} files found in {load_path.as_posix()!r}: {model_file_names}, defaulting to {secondary_full_path!r}. \"\n",
        "                        f'Please specify the desired file name via `model_kwargs={{\"file_name\": \"<file_name>\"}}`.'\n",
        "                    )\n",
        "                model_args[\"subfolder\"] = self.backend\n",
        "                model_args[\"file_name\"] = file_name\n",
        "\n",
        "        # If the file_name contains subfolders, set it as the subfolder instead\n",
        "        file_name_parts = Path(file_name).parts\n",
        "        if len(file_name_parts) > 1:\n",
        "            model_args[\"file_name\"] = file_name_parts[-1]\n",
        "            model_args[\"subfolder\"] = Path(model_args.get(\"subfolder\", \"\"), *file_name_parts[:-1]).as_posix()\n",
        "\n",
        "        if export:\n",
        "            logger.warning(\n",
        "                f\"No {file_name!r} found in {load_path.as_posix()!r}. Exporting the model to {backend_name}.\"\n",
        "            )\n",
        "            if model_file_names:\n",
        "                logger.warning(\n",
        "                    f\"If you intended to load one of the {model_file_names} {backend_name} files, \"\n",
        "                    f'please specify the desired file name via `model_kwargs={{\"file_name\": \"{model_file_names[0]}\"}}`.'\n",
        "                )\n",
        "\n",
        "        return export, model_args\n",
        "\n",
        "    def _backend_warn_to_save(self, model_name_or_path: str, is_local: str, backend_name: str) -> None:\n",
        "        to_log = f\"Saving the exported {backend_name} model is heavily recommended to avoid having to export it again.\"\n",
        "        if is_local:\n",
        "            to_log += f\" Do so with `model.save_pretrained({model_name_or_path!r})`.\"\n",
        "        else:\n",
        "            to_log += f\" Do so with `model.push_to_hub({model_name_or_path!r}, create_pr=True)`.\"\n",
        "        logger.warning(to_log)\n",
        "\n",
        "    def _load_t5_model(self, model_name_or_path, config, cache_dir, **model_args) -> None:\n",
        "        \"\"\"Loads the encoder model from T5\"\"\"\n",
        "        from transformers import T5EncoderModel\n",
        "\n",
        "        T5EncoderModel._keys_to_ignore_on_load_unexpected = [\"decoder.*\"]\n",
        "        self.auto_model = T5EncoderModel.from_pretrained(\n",
        "            model_name_or_path, config=config, cache_dir=cache_dir, **model_args\n",
        "        )\n",
        "\n",
        "    def _load_mt5_model(self, model_name_or_path, config, cache_dir, **model_args) -> None:\n",
        "        \"\"\"Loads the encoder model from T5\"\"\"\n",
        "        from transformers import MT5EncoderModel\n",
        "\n",
        "        MT5EncoderModel._keys_to_ignore_on_load_unexpected = [\"decoder.*\"]\n",
        "        self.auto_model = MT5EncoderModel.from_pretrained(\n",
        "            model_name_or_path, config=config, cache_dir=cache_dir, **model_args\n",
        "        )\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Transformer({self.get_config_dict()}) with Transformer model: {self.auto_model.__class__.__name__} \"\n",
        "\n",
        "    def forward(self, features: dict[str, torch.Tensor], **kwargs) -> dict[str, torch.Tensor]:\n",
        "        \"\"\"Returns token_embeddings, cls_token\"\"\"\n",
        "        trans_features = {\"input_ids\": features[\"input_ids\"], \"attention_mask\": features[\"attention_mask\"]}\n",
        "        if \"token_type_ids\" in features:\n",
        "            trans_features[\"token_type_ids\"] = features[\"token_type_ids\"]\n",
        "\n",
        "        output_states = self.auto_model(**trans_features, **kwargs, return_dict=False)\n",
        "        output_tokens = output_states[0]\n",
        "\n",
        "        # If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\n",
        "        # We need to extend the attention mask to include these virtual tokens, or the pooling will fail\n",
        "        if is_peft_available():\n",
        "            from peft import PeftModelForFeatureExtraction\n",
        "\n",
        "            if (\n",
        "                isinstance(self.auto_model, PeftModelForFeatureExtraction)\n",
        "                and self.auto_model.active_peft_config.is_prompt_learning\n",
        "            ):\n",
        "                batch_size = output_tokens.size(0)\n",
        "                attention_mask = features[\"attention_mask\"]\n",
        "                prefix_attention_mask = torch.ones(\n",
        "                    batch_size, self.auto_model.active_peft_config.num_virtual_tokens, device=attention_mask.device\n",
        "                )\n",
        "                features[\"attention_mask\"] = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n",
        "\n",
        "        features[\"token_embeddings\"] = output_tokens\n",
        "\n",
        "        if self.auto_model.config.output_hidden_states and len(output_states) > 2:\n",
        "            all_layer_idx = 2  # I.e. after last_hidden_states and pooler_output\n",
        "            if len(output_states) < 3:  # Some models only output last_hidden_states and all_hidden_states\n",
        "                all_layer_idx = 1\n",
        "\n",
        "            hidden_states = output_states[all_layer_idx]\n",
        "            features[\"all_layer_embeddings\"] = hidden_states\n",
        "\n",
        "        return features\n",
        "\n",
        "    def get_word_embedding_dimension(self) -> int:\n",
        "        return self.auto_model.config.hidden_size\n",
        "\n",
        "    def tokenize(\n",
        "        self, texts: list[str] | list[dict] | list[tuple[str, str]], padding: str | bool = True\n",
        "    ) -> dict[str, torch.Tensor]:\n",
        "        \"\"\"Tokenizes a text and maps tokens to token-ids\"\"\"\n",
        "        output = {}\n",
        "        if isinstance(texts[0], str):\n",
        "            to_tokenize = [texts]\n",
        "        elif isinstance(texts[0], dict):\n",
        "            to_tokenize = []\n",
        "            output[\"text_keys\"] = []\n",
        "            for lookup in texts:\n",
        "                text_key, text = next(iter(lookup.items()))\n",
        "                to_tokenize.append(text)\n",
        "                output[\"text_keys\"].append(text_key)\n",
        "            to_tokenize = [to_tokenize]\n",
        "        else:\n",
        "            batch1, batch2 = [], []\n",
        "            for text_tuple in texts:\n",
        "                batch1.append(text_tuple[0])\n",
        "                batch2.append(text_tuple[1])\n",
        "            to_tokenize = [batch1, batch2]\n",
        "\n",
        "        # strip\n",
        "        to_tokenize = [[str(s).strip() for s in col] for col in to_tokenize]\n",
        "\n",
        "        # Lowercase\n",
        "        if self.do_lower_case:\n",
        "            to_tokenize = [[s.lower() for s in col] for col in to_tokenize]\n",
        "\n",
        "        output.update(\n",
        "            self.tokenizer(\n",
        "                *to_tokenize,\n",
        "                padding=padding,\n",
        "                truncation=\"longest_first\",\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=self.max_seq_length,\n",
        "            )\n",
        "        )\n",
        "        return output\n",
        "\n",
        "    def get_config_dict(self) -> dict[str, Any]:\n",
        "        return {key: self.__dict__[key] for key in self.config_keys}\n",
        "\n",
        "    def save(self, output_path: str, safe_serialization: bool = True) -> None:\n",
        "        self.auto_model.save_pretrained(output_path, safe_serialization=safe_serialization)\n",
        "        self.tokenizer.save_pretrained(output_path)\n",
        "\n",
        "        with open(os.path.join(output_path, \"sentence_bert_config.json\"), \"w\") as fOut:\n",
        "            json.dump(self.get_config_dict(), fOut, indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, input_path: str):\n",
        "        # Old classes used other config names than 'sentence_bert_config.json'\n",
        "        for config_name in [\n",
        "            \"sentence_bert_config.json\",\n",
        "            \"sentence_roberta_config.json\",\n",
        "            \"sentence_distilbert_config.json\",\n",
        "            \"sentence_camembert_config.json\",\n",
        "            \"sentence_albert_config.json\",\n",
        "            \"sentence_xlm-roberta_config.json\",\n",
        "            \"sentence_xlnet_config.json\",\n",
        "        ]:\n",
        "            sbert_config_path = os.path.join(input_path, config_name)\n",
        "            if os.path.exists(sbert_config_path):\n",
        "                break\n",
        "\n",
        "        with open(sbert_config_path) as fIn:\n",
        "            config = json.load(fIn)\n",
        "        # Don't allow configs to set trust_remote_code\n",
        "        if \"model_args\" in config and \"trust_remote_code\" in config[\"model_args\"]:\n",
        "            config[\"model_args\"].pop(\"trust_remote_code\")\n",
        "        if \"tokenizer_args\" in config and \"trust_remote_code\" in config[\"tokenizer_args\"]:\n",
        "            config[\"tokenizer_args\"].pop(\"trust_remote_code\")\n",
        "        if \"config_args\" in config and \"trust_remote_code\" in config[\"config_args\"]:\n",
        "            config[\"config_args\"].pop(\"trust_remote_code\")\n",
        "        return cls(model_name_or_path=input_path, **config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ljhhULJgOxA",
        "outputId": "e21a21ee-0b7e-4445-dbbc-c0c290651c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        }
      ],
      "source": [
        "NUM_PROC = 16\n",
        "print(NUM_PROC)\n",
        "\n",
        "def seed_everything(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    pl.set_random_seed(seed)\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U_syvyigfbI",
        "outputId": "63bed537-9915-4e6b-a1e1-8999cceb5310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4370, 33)\n"
          ]
        }
      ],
      "source": [
        "# /content/drive/MyDrive/kaggle-eedi/input/train_5folds_with_llm_infer.csv\n",
        "df = pd.read_csv(f\"{DATA_PATH}/train_5folds_with_llm_infer.csv\")\n",
        "df['is_synthetic'] = False\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1_VdMrrb1Xw",
        "outputId": "8c1d6177-93b8-4a0f-f1c3-ab701a7cc589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16312, 26)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MisconceptionId</th>\n",
              "      <th>MisconceptionName</th>\n",
              "      <th>ThirdSubjectName</th>\n",
              "      <th>QuestionText</th>\n",
              "      <th>author</th>\n",
              "      <th>ThirdSubjectId</th>\n",
              "      <th>SecondSubjectId</th>\n",
              "      <th>SecondSubjectName</th>\n",
              "      <th>FirstSubjectId</th>\n",
              "      <th>FirstSubjectName</th>\n",
              "      <th>...</th>\n",
              "      <th>AnswerText-qwen-answer-final-seed42</th>\n",
              "      <th>CorrectAnswerText</th>\n",
              "      <th>AnswerText</th>\n",
              "      <th>ConstructName-qwen25-72b-instruct-seed99</th>\n",
              "      <th>ConstructName</th>\n",
              "      <th>QuestionId</th>\n",
              "      <th>QuestionId_Answer</th>\n",
              "      <th>quality-gpt4o-mini</th>\n",
              "      <th>p000-qwen25-32b-instruct-cot_misunderstanding</th>\n",
              "      <th>p000-qwen25-32b-instruct-cot-v2_misunderstanding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Believes there are 100 degrees in a full turn</td>\n",
              "      <td>Basic Angle Facts (straight line, opposite, ar...</td>\n",
              "      <td>If Sally spins around in a full circle, how ma...</td>\n",
              "      <td>llama</td>\n",
              "      <td>181</td>\n",
              "      <td>74.0</td>\n",
              "      <td>Angles</td>\n",
              "      <td>71.0</td>\n",
              "      <td>Geometry and Measure</td>\n",
              "      <td>...</td>\n",
              "      <td>100</td>\n",
              "      <td>360</td>\n",
              "      <td>100</td>\n",
              "      <td>Understand the measure of a full turn in degrees</td>\n",
              "      <td>Understand the measure of a full turn in degrees</td>\n",
              "      <td>10000</td>\n",
              "      <td>10000_A</td>\n",
              "      <td>5</td>\n",
              "      <td>**Explanation:**\\n\\n**Step-by-Step Brief Expla...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Believes there are 100 degrees in a full turn</td>\n",
              "      <td>Basic Angle Facts (straight line, opposite, ar...</td>\n",
              "      <td>What is the sum of the angles around a point?</td>\n",
              "      <td>llama</td>\n",
              "      <td>181</td>\n",
              "      <td>74.0</td>\n",
              "      <td>Angles</td>\n",
              "      <td>71.0</td>\n",
              "      <td>Geometry and Measure</td>\n",
              "      <td>...</td>\n",
              "      <td>100</td>\n",
              "      <td>360</td>\n",
              "      <td>100</td>\n",
              "      <td>Understand the sum of angles around a point</td>\n",
              "      <td>Understand the sum of angles around a point</td>\n",
              "      <td>10001</td>\n",
              "      <td>10001_A</td>\n",
              "      <td>5</td>\n",
              "      <td>**Explanation:**\\n\\n**Step-by-Step Brief Expla...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Believes there are 100 degrees in a full turn</td>\n",
              "      <td>Angles-Others</td>\n",
              "      <td>A car wheel makes one full rotation, how many ...</td>\n",
              "      <td>llama</td>\n",
              "      <td>1177</td>\n",
              "      <td>74.0</td>\n",
              "      <td>Angles</td>\n",
              "      <td>71.0</td>\n",
              "      <td>Geometry and Measure</td>\n",
              "      <td>...</td>\n",
              "      <td>100</td>\n",
              "      <td>360</td>\n",
              "      <td>100</td>\n",
              "      <td>Understand that a full rotation is 360 degrees</td>\n",
              "      <td>Understand that a full rotation is 360 degrees</td>\n",
              "      <td>10002</td>\n",
              "      <td>10002_A</td>\n",
              "      <td>5</td>\n",
              "      <td>Explanation:\\nMisunderstanding: The students s...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Believes there are 100 degrees in a full turn</td>\n",
              "      <td>Angles-Others</td>\n",
              "      <td>If a person turns 180 degrees and then another...</td>\n",
              "      <td>llama</td>\n",
              "      <td>1177</td>\n",
              "      <td>74.0</td>\n",
              "      <td>Angles</td>\n",
              "      <td>71.0</td>\n",
              "      <td>Geometry and Measure</td>\n",
              "      <td>...</td>\n",
              "      <td>100</td>\n",
              "      <td>360</td>\n",
              "      <td>100</td>\n",
              "      <td>Calculate the total degrees turned after multi...</td>\n",
              "      <td>Calculate the total degrees turned after multi...</td>\n",
              "      <td>10004</td>\n",
              "      <td>10004_A</td>\n",
              "      <td>5</td>\n",
              "      <td>It seems that the students have made a signifi...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>Thinks a quadratic without a non variable term...</td>\n",
              "      <td>Factorising into a Double Bracket</td>\n",
              "      <td>Can the expression x^2 - 3x be factorised?</td>\n",
              "      <td>llama</td>\n",
              "      <td>53</td>\n",
              "      <td>153.0</td>\n",
              "      <td>Factorising</td>\n",
              "      <td>49.0</td>\n",
              "      <td>Algebra</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>x(x-3)</td>\n",
              "      <td>No</td>\n",
              "      <td>Factorise a quadratic expression in the form x...</td>\n",
              "      <td>Factorise a quadratic expression in the form x...</td>\n",
              "      <td>10005</td>\n",
              "      <td>10005_A</td>\n",
              "      <td>5</td>\n",
              "      <td>### Explanation of the Mistake\\n\\n**Step-by-St...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16303</th>\n",
              "      <td>1103</td>\n",
              "      <td>When reading integers on a number line, assume...</td>\n",
              "      <td>Place Value</td>\n",
              "      <td>On the number line below, what number does the...</td>\n",
              "      <td>llama</td>\n",
              "      <td>202</td>\n",
              "      <td>144.0</td>\n",
              "      <td>Basic Arithmetic</td>\n",
              "      <td>32.0</td>\n",
              "      <td>Number</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>3.5</td>\n",
              "      <td>6</td>\n",
              "      <td>Read an integer on a number line where the req...</td>\n",
              "      <td>Read an integer on a number line where the req...</td>\n",
              "      <td>26352</td>\n",
              "      <td>26352_A</td>\n",
              "      <td>4</td>\n",
              "      <td>It seems that the students have misunderstood ...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16304</th>\n",
              "      <td>1237</td>\n",
              "      <td>Believes that 1 part of a bar model will alway...</td>\n",
              "      <td>Written Division</td>\n",
              "      <td>Tom has 18 pencils in a box. He wants to share...</td>\n",
              "      <td>llama</td>\n",
              "      <td>208</td>\n",
              "      <td>144.0</td>\n",
              "      <td>Basic Arithmetic</td>\n",
              "      <td>32.0</td>\n",
              "      <td>Number</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Interpret and create bar models for equal shar...</td>\n",
              "      <td>Interpret and create bar models for equal shar...</td>\n",
              "      <td>26353</td>\n",
              "      <td>26353_A</td>\n",
              "      <td>5</td>\n",
              "      <td>### Explanation of the Mistake\\n\\n**Step-by-St...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16305</th>\n",
              "      <td>1474</td>\n",
              "      <td>Forgets to add on the starting value when iden...</td>\n",
              "      <td>Basic Arithmetic-Others</td>\n",
              "      <td>If a bookshelf has 8 shelves, and a book is mo...</td>\n",
              "      <td>llama</td>\n",
              "      <td>1203</td>\n",
              "      <td>144.0</td>\n",
              "      <td>Basic Arithmetic</td>\n",
              "      <td>32.0</td>\n",
              "      <td>Number</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>Understand and apply the concept of position a...</td>\n",
              "      <td>Understand and apply the concept of position a...</td>\n",
              "      <td>26354</td>\n",
              "      <td>26354_A</td>\n",
              "      <td>3</td>\n",
              "      <td>The misunderstanding likely stems from a conce...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16307</th>\n",
              "      <td>2053</td>\n",
              "      <td>When reading integers on a number line, assume...</td>\n",
              "      <td>Mental Addition and Subtraction</td>\n",
              "      <td>On a number line with dashes marked at every 5...</td>\n",
              "      <td>llama</td>\n",
              "      <td>203</td>\n",
              "      <td>144.0</td>\n",
              "      <td>Basic Arithmetic</td>\n",
              "      <td>32.0</td>\n",
              "      <td>Number</td>\n",
              "      <td>...</td>\n",
              "      <td>13</td>\n",
              "      <td>25  or  -5</td>\n",
              "      <td>13</td>\n",
              "      <td>Interpret and use number lines with intervals</td>\n",
              "      <td>Interpret and use number lines with intervals</td>\n",
              "      <td>26356</td>\n",
              "      <td>26356_A</td>\n",
              "      <td>5</td>\n",
              "      <td>**Misunderstanding:**\\n\\nThe students likely m...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16311</th>\n",
              "      <td>2585</td>\n",
              "      <td>Believes a cubic expression should have three ...</td>\n",
              "      <td>Solving Trigonometric Equations</td>\n",
              "      <td>Which of the following is a cubic expression? ...</td>\n",
              "      <td>qwen</td>\n",
              "      <td>1076</td>\n",
              "      <td>154.0</td>\n",
              "      <td>Solving Equations</td>\n",
              "      <td>49.0</td>\n",
              "      <td>Algebra</td>\n",
              "      <td>...</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>c</td>\n",
              "      <td>Identify a cubic expression</td>\n",
              "      <td>Identify a cubic expression</td>\n",
              "      <td>26360</td>\n",
              "      <td>26360_A</td>\n",
              "      <td>5</td>\n",
              "      <td>It seems that your students chose option c) \\(...</td>\n",
              "      <td>STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9440 rows Ã— 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       MisconceptionId                                  MisconceptionName  \\\n",
              "0                    2      Believes there are 100 degrees in a full turn   \n",
              "1                    2      Believes there are 100 degrees in a full turn   \n",
              "2                    2      Believes there are 100 degrees in a full turn   \n",
              "4                    2      Believes there are 100 degrees in a full turn   \n",
              "5                    3  Thinks a quadratic without a non variable term...   \n",
              "...                ...                                                ...   \n",
              "16303             1103  When reading integers on a number line, assume...   \n",
              "16304             1237  Believes that 1 part of a bar model will alway...   \n",
              "16305             1474  Forgets to add on the starting value when iden...   \n",
              "16307             2053  When reading integers on a number line, assume...   \n",
              "16311             2585  Believes a cubic expression should have three ...   \n",
              "\n",
              "                                        ThirdSubjectName  \\\n",
              "0      Basic Angle Facts (straight line, opposite, ar...   \n",
              "1      Basic Angle Facts (straight line, opposite, ar...   \n",
              "2                                          Angles-Others   \n",
              "4                                          Angles-Others   \n",
              "5                      Factorising into a Double Bracket   \n",
              "...                                                  ...   \n",
              "16303                                        Place Value   \n",
              "16304                                   Written Division   \n",
              "16305                            Basic Arithmetic-Others   \n",
              "16307                    Mental Addition and Subtraction   \n",
              "16311                    Solving Trigonometric Equations   \n",
              "\n",
              "                                            QuestionText author  \\\n",
              "0      If Sally spins around in a full circle, how ma...  llama   \n",
              "1          What is the sum of the angles around a point?  llama   \n",
              "2      A car wheel makes one full rotation, how many ...  llama   \n",
              "4      If a person turns 180 degrees and then another...  llama   \n",
              "5             Can the expression x^2 - 3x be factorised?  llama   \n",
              "...                                                  ...    ...   \n",
              "16303  On the number line below, what number does the...  llama   \n",
              "16304  Tom has 18 pencils in a box. He wants to share...  llama   \n",
              "16305  If a bookshelf has 8 shelves, and a book is mo...  llama   \n",
              "16307  On a number line with dashes marked at every 5...  llama   \n",
              "16311  Which of the following is a cubic expression? ...   qwen   \n",
              "\n",
              "       ThirdSubjectId  SecondSubjectId  SecondSubjectName  FirstSubjectId  \\\n",
              "0                 181             74.0             Angles            71.0   \n",
              "1                 181             74.0             Angles            71.0   \n",
              "2                1177             74.0             Angles            71.0   \n",
              "4                1177             74.0             Angles            71.0   \n",
              "5                  53            153.0        Factorising            49.0   \n",
              "...               ...              ...                ...             ...   \n",
              "16303             202            144.0   Basic Arithmetic            32.0   \n",
              "16304             208            144.0   Basic Arithmetic            32.0   \n",
              "16305            1203            144.0   Basic Arithmetic            32.0   \n",
              "16307             203            144.0   Basic Arithmetic            32.0   \n",
              "16311            1076            154.0  Solving Equations            49.0   \n",
              "\n",
              "           FirstSubjectName  ... AnswerText-qwen-answer-final-seed42  \\\n",
              "0      Geometry and Measure  ...                                 100   \n",
              "1      Geometry and Measure  ...                                 100   \n",
              "2      Geometry and Measure  ...                                 100   \n",
              "4      Geometry and Measure  ...                                 100   \n",
              "5                   Algebra  ...                                  No   \n",
              "...                     ...  ...                                 ...   \n",
              "16303                Number  ...                                   6   \n",
              "16304                Number  ...                                   1   \n",
              "16305                Number  ...                                  -1   \n",
              "16307                Number  ...                                  13   \n",
              "16311               Algebra  ...                                   c   \n",
              "\n",
              "      CorrectAnswerText AnswerText  \\\n",
              "0                   360        100   \n",
              "1                   360        100   \n",
              "2                   360        100   \n",
              "4                   360        100   \n",
              "5                x(x-3)         No   \n",
              "...                 ...        ...   \n",
              "16303               3.5          6   \n",
              "16304                 3          1   \n",
              "16305                 1         -1   \n",
              "16307        25  or  -5         13   \n",
              "16311                 a          c   \n",
              "\n",
              "                ConstructName-qwen25-72b-instruct-seed99  \\\n",
              "0       Understand the measure of a full turn in degrees   \n",
              "1            Understand the sum of angles around a point   \n",
              "2         Understand that a full rotation is 360 degrees   \n",
              "4      Calculate the total degrees turned after multi...   \n",
              "5      Factorise a quadratic expression in the form x...   \n",
              "...                                                  ...   \n",
              "16303  Read an integer on a number line where the req...   \n",
              "16304  Interpret and create bar models for equal shar...   \n",
              "16305  Understand and apply the concept of position a...   \n",
              "16307      Interpret and use number lines with intervals   \n",
              "16311                        Identify a cubic expression   \n",
              "\n",
              "                                           ConstructName QuestionId  \\\n",
              "0       Understand the measure of a full turn in degrees      10000   \n",
              "1            Understand the sum of angles around a point      10001   \n",
              "2         Understand that a full rotation is 360 degrees      10002   \n",
              "4      Calculate the total degrees turned after multi...      10004   \n",
              "5      Factorise a quadratic expression in the form x...      10005   \n",
              "...                                                  ...        ...   \n",
              "16303  Read an integer on a number line where the req...      26352   \n",
              "16304  Interpret and create bar models for equal shar...      26353   \n",
              "16305  Understand and apply the concept of position a...      26354   \n",
              "16307      Interpret and use number lines with intervals      26356   \n",
              "16311                        Identify a cubic expression      26360   \n",
              "\n",
              "      QuestionId_Answer quality-gpt4o-mini  \\\n",
              "0               10000_A                  5   \n",
              "1               10001_A                  5   \n",
              "2               10002_A                  5   \n",
              "4               10004_A                  5   \n",
              "5               10005_A                  5   \n",
              "...                 ...                ...   \n",
              "16303           26352_A                  4   \n",
              "16304           26353_A                  5   \n",
              "16305           26354_A                  3   \n",
              "16307           26356_A                  5   \n",
              "16311           26360_A                  5   \n",
              "\n",
              "           p000-qwen25-32b-instruct-cot_misunderstanding  \\\n",
              "0      **Explanation:**\\n\\n**Step-by-Step Brief Expla...   \n",
              "1      **Explanation:**\\n\\n**Step-by-Step Brief Expla...   \n",
              "2      Explanation:\\nMisunderstanding: The students s...   \n",
              "4      It seems that the students have made a signifi...   \n",
              "5      ### Explanation of the Mistake\\n\\n**Step-by-St...   \n",
              "...                                                  ...   \n",
              "16303  It seems that the students have misunderstood ...   \n",
              "16304  ### Explanation of the Mistake\\n\\n**Step-by-St...   \n",
              "16305  The misunderstanding likely stems from a conce...   \n",
              "16307  **Misunderstanding:**\\n\\nThe students likely m...   \n",
              "16311  It seems that your students chose option c) \\(...   \n",
              "\n",
              "        p000-qwen25-32b-instruct-cot-v2_misunderstanding  \n",
              "0      STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "1      STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "2      STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "4      STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "5      STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "...                                                  ...  \n",
              "16303  STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "16304  STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "16305  STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "16307  STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "16311  STEP-BY-STEP PROCESS OF GETTING THE WRONG ANSW...  \n",
              "\n",
              "[9440 rows x 26 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read synthetic_data\n",
        "df_synth = pd.read_csv(f\"{DATA_PATH}/synthetic_questions_render_with_answer_render_v1.csv\")\n",
        "df_synth = df_synth[~df_synth.isna().any(axis=1)].reset_index(drop=True)\n",
        "df_synth = df_synth[df_synth[\"quality-gpt4o-mini\"] > 2].reset_index(drop=True)\n",
        "df_synth = df_synth.sample(n=4000, random_state=0).reset_index(drop=True)\n",
        "\n",
        "# 3rd subject name\n",
        "df_synth = df_synth.rename({\"ThirdSubjectName\": \"SubjectName\"}, axis=1)\n",
        "df_synth = df_synth.rename({\"MisconceptionName\": \"Misconception\"}, axis=1)\n",
        "\n",
        "df_synth[\"is_synthetic\"] = True\n",
        "df_synth[\"fold\"] = -1\n",
        "print(df_synth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_synth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un_hpExXb8CT",
        "outputId": "c314cf62-a1ae-4b3f-92fb-9828ec409d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2185, 24)\n"
          ]
        }
      ],
      "source": [
        "# synthetic_data\n",
        "df_gpt = pd.read_csv(f\"{DATA_PATH}/synthetic-round1-render.csv\")\n",
        "\n",
        "# rename\n",
        "df_gpt = df_gpt.rename({\"ConstructName-qwen25-72b-instruct\": \"ConstructName\"}, axis=1)\n",
        "df_gpt = df_gpt.rename({\"MisconceptionName\": \"Misconception\"}, axis=1)\n",
        "\n",
        "# Qualityã§çµžã‚Šè¾¼ã¿\n",
        "df_gpt = df_gpt[df_gpt[\"quality-gpt4o-mini\"] > 2].reset_index(drop=True)\n",
        "\n",
        "df_gpt[\"is_synthetic\"] = True\n",
        "df_gpt[\"fold\"] = -2\n",
        "print(df_gpt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8RHdPn7zsHD",
        "outputId": "bc16bf7b-b475-4a80-fb67-4227aff21011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(31868, 27)\n"
          ]
        }
      ],
      "source": [
        "# synthetic_data\n",
        "df_synth2 = pd.read_csv(f\"{DATA_PATH}/synthetic-round2-render.csv\")\n",
        "df_synth2 = df_synth2[~df_synth2.isna().any(axis=1)].reset_index(drop=True)\n",
        "\n",
        "df_synth2 = df_synth2.rename({\"ConstructName-qwen25-72b-instruct\": \"ConstructName\"}, axis=1)\n",
        "df_synth2 = df_synth2.rename({\"MisconceptionName\": \"Misconception\"}, axis=1)\n",
        "\n",
        "df_synth2 = df_synth2[df_synth2[\"quality-gpt4o-mini\"] > 2].reset_index(drop=True)\n",
        "\n",
        "df_synth2[\"is_synthetic\"] = True\n",
        "df_synth2[\"fold\"] = -3\n",
        "print(df_synth2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(29916, 25)\n"
          ]
        }
      ],
      "source": [
        "# synthetic_data\n",
        "df_synth3 = pd.read_csv(f\"{DATA_PATH}/synthetic-round3-render.csv\")\n",
        "df_synth3 = df_synth3[~df_synth3.isna().any(axis=1)].reset_index(drop=True)\n",
        "\n",
        "df_synth3 = df_synth3.rename({\"MisconceptionName\": \"Misconception\"}, axis=1)\n",
        "\n",
        "df_synth3 = df_synth3[df_synth3[\"quality-gpt4o-mini\"] > 2].reset_index(drop=True)\n",
        "\n",
        "df_synth3[\"is_synthetic\"] = True\n",
        "df_synth3[\"fold\"] = -4\n",
        "print(df_synth3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYnf_aJQcDZC",
        "outputId": "706f4f25-c230-472b-a204-4e43fc9131a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(72339, 52)\n"
          ]
        }
      ],
      "source": [
        "df = pd.concat([df, df_synth, df_gpt, df_synth2, df_synth3], axis=0).reset_index(drop=True)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "EiDBljjvg_IJ",
        "outputId": "76e1f057-09ff-4ee9-f90d-a81604df4430"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QuestionId</th>\n",
              "      <th>ConstructId</th>\n",
              "      <th>ConstructName</th>\n",
              "      <th>SubjectId</th>\n",
              "      <th>SubjectName</th>\n",
              "      <th>QuestionText</th>\n",
              "      <th>Answer</th>\n",
              "      <th>AnswerText</th>\n",
              "      <th>Correct</th>\n",
              "      <th>CorrectAnswer</th>\n",
              "      <th>...</th>\n",
              "      <th>quality-gpt4o-mini</th>\n",
              "      <th>p000-qwen25-32b-instruct-cot-v2_misunderstanding</th>\n",
              "      <th>Answer_CoT</th>\n",
              "      <th>CorrectAnswer_CoT</th>\n",
              "      <th>SubjectNameLLM</th>\n",
              "      <th>wrong_answers</th>\n",
              "      <th>correct_answers</th>\n",
              "      <th>AnswerText-qwen</th>\n",
              "      <th>CorrectAnswerText-qwen</th>\n",
              "      <th>ConstructName-qwen25-72b-instruct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>856.0</td>\n",
              "      <td>Use the order of operations to carry out calcu...</td>\n",
              "      <td>33.0</td>\n",
              "      <td>BIDMAS</td>\n",
              "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
              "      <td>D</td>\n",
              "      <td>Does not need brackets</td>\n",
              "      <td>0.0</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1612.0</td>\n",
              "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
              "      <td>1077.0</td>\n",
              "      <td>Simplifying Algebraic Fractions</td>\n",
              "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
              "      <td>A</td>\n",
              "      <td>\\( m+1 \\)</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1612.0</td>\n",
              "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
              "      <td>1077.0</td>\n",
              "      <td>Simplifying Algebraic Fractions</td>\n",
              "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
              "      <td>B</td>\n",
              "      <td>\\( m+2 \\)</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1612.0</td>\n",
              "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
              "      <td>1077.0</td>\n",
              "      <td>Simplifying Algebraic Fractions</td>\n",
              "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
              "      <td>C</td>\n",
              "      <td>\\( m-1 \\)</td>\n",
              "      <td>0.0</td>\n",
              "      <td>D</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>2774.0</td>\n",
              "      <td>Calculate the range from a list of data</td>\n",
              "      <td>339.0</td>\n",
              "      <td>Range and Interquartile Range from a List of Data</td>\n",
              "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
              "      <td>A</td>\n",
              "      <td>Only\\nTom</td>\n",
              "      <td>0.0</td>\n",
              "      <td>B</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   QuestionId  ConstructId                                      ConstructName  \\\n",
              "0           0        856.0  Use the order of operations to carry out calcu...   \n",
              "1           1       1612.0  Simplify an algebraic fraction by factorising ...   \n",
              "2           1       1612.0  Simplify an algebraic fraction by factorising ...   \n",
              "3           1       1612.0  Simplify an algebraic fraction by factorising ...   \n",
              "4           2       2774.0            Calculate the range from a list of data   \n",
              "\n",
              "   SubjectId                                        SubjectName  \\\n",
              "0       33.0                                             BIDMAS   \n",
              "1     1077.0                    Simplifying Algebraic Fractions   \n",
              "2     1077.0                    Simplifying Algebraic Fractions   \n",
              "3     1077.0                    Simplifying Algebraic Fractions   \n",
              "4      339.0  Range and Interquartile Range from a List of Data   \n",
              "\n",
              "                                        QuestionText Answer  \\\n",
              "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...      D   \n",
              "1  Simplify the following, if possible: \\( \\frac{...      A   \n",
              "2  Simplify the following, if possible: \\( \\frac{...      B   \n",
              "3  Simplify the following, if possible: \\( \\frac{...      C   \n",
              "4  Tom and Katie are discussing the \\( 5 \\) plant...      A   \n",
              "\n",
              "               AnswerText  Correct CorrectAnswer  ... quality-gpt4o-mini  \\\n",
              "0  Does not need brackets      0.0             A  ...                NaN   \n",
              "1               \\( m+1 \\)      0.0             D  ...                NaN   \n",
              "2               \\( m+2 \\)      0.0             D  ...                NaN   \n",
              "3               \\( m-1 \\)      0.0             D  ...                NaN   \n",
              "4               Only\\nTom      0.0             B  ...                NaN   \n",
              "\n",
              "   p000-qwen25-32b-instruct-cot-v2_misunderstanding Answer_CoT  \\\n",
              "0                                               NaN        NaN   \n",
              "1                                               NaN        NaN   \n",
              "2                                               NaN        NaN   \n",
              "3                                               NaN        NaN   \n",
              "4                                               NaN        NaN   \n",
              "\n",
              "   CorrectAnswer_CoT SubjectNameLLM  wrong_answers correct_answers  \\\n",
              "0                NaN            NaN            NaN             NaN   \n",
              "1                NaN            NaN            NaN             NaN   \n",
              "2                NaN            NaN            NaN             NaN   \n",
              "3                NaN            NaN            NaN             NaN   \n",
              "4                NaN            NaN            NaN             NaN   \n",
              "\n",
              "   AnswerText-qwen CorrectAnswerText-qwen  ConstructName-qwen25-72b-instruct  \n",
              "0              NaN                    NaN                                NaN  \n",
              "1              NaN                    NaN                                NaN  \n",
              "2              NaN                    NaN                                NaN  \n",
              "3              NaN                    NaN                                NaN  \n",
              "4              NaN                    NaN                                NaN  \n",
              "\n",
              "[5 rows x 52 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "Hqp8ja8ueDET",
        "outputId": "e3c10b9f-530d-4740-a8db-e4eeb234a387"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SubjectName          0\n",
              "ConstructName        0\n",
              "QuestionText         0\n",
              "CorrectAnswerText    0\n",
              "AnswerText           0\n",
              "Misconception        0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[[\"SubjectName\", \"ConstructName\", \"QuestionText\", \"CorrectAnswerText\", \"AnswerText\", \"Misconception\"]].isnull().sum(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jRdM1_MZh7eb"
      },
      "outputs": [],
      "source": [
        "def get_query_text(row):\n",
        "    first_subject = row[\"FirstSubjectName\"]\n",
        "    second_subject = row[\"SecondSubjectName\"]\n",
        "    third_subject = row[\"ThirdSubjectName\"]\n",
        "    construct = row[\"ConstructName\"]\n",
        "    task_description = f'You are an excellent math teacher about to teach students of year group 1 to 14. Here is the detail of your lesson: {first_subject}-{second_subject}-{third_subject}-{construct}. You will be provided a question with a wrong answer from your student. Please retrieve the most relevant misconception behind the wrong answer.'\n",
        "    query_text = f\"###Question###: {row['QuestionText']}\\n###Correct Answer###: {row['CorrectAnswerText']}\\n###Misconcept Wrong answer###: {row['AnswerText']}\\n###Analysis###: {row['p000-qwen25-32b-instruct-cot_misunderstanding']}\"\n",
        "    return f'Instruct: {task_description}\\nQuery: {query_text}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6M6NJjxfihOB"
      },
      "outputs": [],
      "source": [
        "df['InputText'] = df.apply(lambda x: get_query_text(x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruct: You are an excellent math teacher about to teach students of year group 1 to 14. Here is the detail of your lesson: Number-Basic Arithmetic-BIDMAS-Use the order of operations to carry out calculations involving powers. You will be provided a question with a wrong answer from your student. Please retrieve the most relevant misconception behind the wrong answer.\n",
            "Query: ###question###: \\[\n",
            "3 \\times 2+4-5\n",
            "\\]\n",
            "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\n",
            "###Correct Answer###: \\( 3 \\times(2+4)-5 \\)\n",
            "###Misconcept Wrong answer###: Does not need brackets\n",
            "###Analysis###: The students' misunderstanding lies in their lack of recognition of the importance of the order of operations, which is governed by the BIDMAS rule (Brackets, Indices, Division/Multiplication, Addition/Subtraction). BIDMAS helps determine the sequence in which arithmetic operations should be performed to ensure the correct result.\n",
            "\n",
            "In the expression \\( 3 \\times 2 + 4 - 5 \\), the students might have performed the operations from left to right without considering the order of operations. This leads to incorrect calculations.\n",
            "\n",
            "Let's break down the mistake step by step:\n",
            "\n",
            "1. **Without brackets, the expression is evaluated as follows:**\n",
            "   - First, multiply \\( 3 \\times 2 = 6 \\).\n",
            "   - Then, add \\( 6 + 4 = 10 \\).\n",
            "   - Finally, subtract \\( 10 - 5 = 5 \\).\n",
            "\n",
            "   So, without brackets, the expression evaluates to 5, which is incorrect if the goal is to achieve 13.\n",
            "\n",
            "2. **The correct approach involves using brackets to enforce the desired order:**\n",
            "   - The correct placement of brackets should be \\( 3 \\times (2+4) - 5 \\).\n",
            "   - First, evaluate the expression inside the brackets: \\( 2+4 = 6 \\).\n",
            "   - Then, multiply \\( 3 \\times 6 = 18 \\).\n",
            "   - Finally, subtract \\( 18 - 5 = 13 \\).\n",
            "\n",
            "The misunderstanding is that the students did not recognize that the placement of brackets can change the order in which operations are performed, and hence the result. They failed to see that the operations need to be grouped in a specific way to achieve the desired outcome of 13.\n"
          ]
        }
      ],
      "source": [
        "print(df['InputText'].values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "nuI8AjXlg2co",
        "outputId": "88aa66c6-a2d5-4635-bd8f-621fb06870a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    72339.000000\n",
              "mean      2011.721063\n",
              "std        404.317849\n",
              "min        687.000000\n",
              "25%       1748.000000\n",
              "50%       1980.000000\n",
              "75%       2242.000000\n",
              "max      10347.000000\n",
              "Name: InputText, dtype: float64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"InputText\"].map(len).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0G5C4t3RhfL_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "71769"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df[(df[\"InputText\"].map(len) < 3000) | (~df['is_synthetic'])].reset_index(drop=True)\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_mis = pd.read_csv(f\"{DATA_PATH}/misconception_mapping_with_paragraph_v3.csv\")\n",
        "mis_map = df_mis.set_index(\"MisconceptionId\")['a000-llama3-mega-misconception-aug-seed201_misunderstanding'].to_dict()\n",
        "df['a000-llama3-mega-misconception-aug-seed201_misunderstanding'] = df['MisconceptionId'].map(mis_map)\n",
        "df['Misconception'] = df['Misconception'] + ' ' + df['a000-llama3-mega-misconception-aug-seed201_misunderstanding']\n",
        "df[\"Misconception\"] = df[\"Misconception\"].apply(lambda x: 'boxed{' + x + '}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4370, 67399)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_not_synthetic = df[~df['is_synthetic']].reset_index(drop=True)\n",
        "df_synthetic = df[df['is_synthetic']].reset_index(drop=True)\n",
        "len(df_not_synthetic), len(df_synthetic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "grdy2jX-iwgC",
        "outputId": "e91d161c-dcf9-4d42-e94c-35d35345e08a"
      },
      "outputs": [],
      "source": [
        "def _sample_synthetic(df_synthetic):\n",
        "    mis_idx_map = {}\n",
        "    for i in range(len(df_synthetic)):\n",
        "        row = df_synthetic.iloc[i]\n",
        "        misconception_id = row['MisconceptionId']\n",
        "        if misconception_id not in mis_idx_map:\n",
        "            mis_idx_map[misconception_id] = []\n",
        "        mis_idx_map[misconception_id].append(i)\n",
        "    sampled_idxs = []\n",
        "    for misconception_id, idx_list in mis_idx_map.items():\n",
        "        sampled_idx = np.random.choice(idx_list, 1, replace=False)\n",
        "        sampled_idxs.append(sampled_idx[0])\n",
        "    sampled_df = df_synthetic.iloc[sampled_idxs].reset_index(drop=True)\n",
        "    other_df = df_synthetic.drop(sampled_idxs).reset_index(drop=True)\n",
        "    return sampled_df, other_df\n",
        "\n",
        "def sample_synthetic(df_synthetic, num_synthetic):\n",
        "    count = 0\n",
        "    dfs = []\n",
        "    while count < num_synthetic:\n",
        "        print('rest',len(df_synthetic))\n",
        "        sampled_df, other_df = _sample_synthetic(df_synthetic)\n",
        "        dfs.append(sampled_df)\n",
        "        count += len(sampled_df)\n",
        "        df_synthetic = other_df\n",
        "        print('sampled_df_len',len(sampled_df))\n",
        "        print('sampled_mis',len(set(sampled_df['MisconceptionId'])))\n",
        "        \n",
        "    return pd.concat(dfs, axis=0).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rest 67399\n",
            "sampled_df_len 2587\n",
            "sampled_mis 2587\n",
            "rest 64812\n",
            "sampled_df_len 2586\n",
            "sampled_mis 2586\n",
            "rest 62226\n",
            "sampled_df_len 2576\n",
            "sampled_mis 2576\n",
            "rest 59650\n",
            "sampled_df_len 2562\n",
            "sampled_mis 2562\n",
            "rest 57088\n",
            "sampled_df_len 2545\n",
            "sampled_mis 2545\n",
            "rest 54543\n",
            "sampled_df_len 2526\n",
            "sampled_mis 2526\n",
            "rest 52017\n",
            "sampled_df_len 2499\n",
            "sampled_mis 2499\n",
            "rest 49518\n",
            "sampled_df_len 2470\n",
            "sampled_mis 2470\n",
            "rest 47048\n",
            "sampled_df_len 2437\n",
            "sampled_mis 2437\n",
            "rest 44611\n",
            "sampled_df_len 2392\n",
            "sampled_mis 2392\n",
            "rest 42219\n",
            "sampled_df_len 2337\n",
            "sampled_mis 2337\n",
            "rest 39882\n",
            "sampled_df_len 2288\n",
            "sampled_mis 2288\n",
            "rest 37594\n",
            "sampled_df_len 2226\n",
            "sampled_mis 2226\n",
            "rest 35368\n",
            "sampled_df_len 2173\n",
            "sampled_mis 2173\n",
            "rest 33195\n",
            "sampled_df_len 2110\n",
            "sampled_mis 2110\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "36314"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_synthetic_sampled = sample_synthetic(df_synthetic, 36000)\n",
        "len(df_synthetic_sampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40684"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_synthetic_sampled) + 4370"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "42731"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.concat([df_not_synthetic, df_synthetic_sampled], axis=0).reset_index(drop=True)\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjf4Gk7buK7O"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "19b6cegduK7P"
      },
      "outputs": [],
      "source": [
        "df_train = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "F-QMvE1buK7P"
      },
      "outputs": [],
      "source": [
        "train_ds = Dataset.from_pandas(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7sZ_C_TXuK7R"
      },
      "outputs": [],
      "source": [
        "df_courpus = pd.read_csv(f\"{DATA_PATH}/misconception_mapping_with_paragraph_v3.csv\")\n",
        "df_courpus[\"MisconceptionName\"] = df_courpus[\"MisconceptionName\"] + ' ' + df_courpus['a000-llama3-mega-misconception-aug-seed201_misunderstanding']\n",
        "df_courpus[\"MisconceptionName\"] = df_courpus[\"MisconceptionName\"].apply(lambda x: 'boxed{' + x + '}')\n",
        "ir_corpus = df_courpus[[\"MisconceptionId\", \"MisconceptionName\"]].drop_duplicates(['MisconceptionId']).reset_index(drop=True)\n",
        "ir_corpus = dict(zip(ir_corpus.MisconceptionId, ir_corpus.MisconceptionName))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "boxed{Does not know that angles in a triangle sum to 180 degrees Explanation: This misconception arises when students are not aware of or have not fully understood the fundamental property of triangles that the sum of the interior angles of a triangle is always 180 degrees. They may believe that the sum of the angles can be any value or may think that it varies depending on the type of triangle (e.g., equilateral, isosceles, scalene). \n",
            "\n",
            "This misconception can lead to errors when students are asked to find missing angles in a triangle, determine if a set of angles can form a triangle, or solve problems involving the interior angles of triangles.\n",
            "\n",
            "Short cases where this misconception may occur:\n",
            "\n",
            "1. When given two angles of a triangle and asked to find the third angle, a student with this misconception may not know how to proceed or may give an incorrect answer.\n",
            "2. When asked to determine if a set of three angles (e.g., 60Â°, 80Â°, 100Â°) can form a triangle, a student with this misconception may incorrectly say yes or no without considering the sum of the angles.\n",
            "3. When solving a problem involving the interior angles of a triangle, such as finding the sum of the angles in a triangle with two given angles, a student with this misconception may arrive at an incorrect solution.}\n"
          ]
        }
      ],
      "source": [
        "print(df_courpus[\"MisconceptionName\"].values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0iemzfRuIEy"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jGYEDBgbkTl0"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "91c2aa9616ed40ef834a1d35b1ee3cbf",
            "82ea29243568468db4a0f55881af4f25",
            "6725c0f5d729489bb70027366ca89d11",
            "e3e9e206ac344806bc6cf4f7921f50ca",
            "e9365a2dcf66496687bf9c6261a7fad6",
            "d222f273126d4954aa583e78b1cdfa86",
            "27dd2228dd4f47328c23b1514e0bb00f",
            "0261ae9d6e264adf8e350c40fa1d92e4",
            "fbdde8e5ee27407392724cf19ec34155",
            "f86bb7950ae843c3a1f74891b3a18385",
            "afd683dfce07454fb73478e36d8e8767"
          ]
        },
        "id": "rj5zFofIxYJm",
        "outputId": "f857555b-e503-4589-9fd4-591520d05763"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b6c62ccf01348c1a57047d3b6f543cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_model = LLM2Vec.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    # attn_implementation=\"flash_attention_2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CicG4Xpk_x88"
      },
      "outputs": [],
      "source": [
        "base_tokenizer = base_model.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrTQCrZ3ysVC",
        "outputId": "295d73a8-e0b4-4055-bfa8-a8ea95e74265"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Qwen2BiModel(\n",
              "  (embed_tokens): Embedding(152064, 5120)\n",
              "  (layers): ModuleList(\n",
              "    (0-63): 64 x ModifiedQwen2DecoderLayer(\n",
              "      (self_attn): ModifiedQwen2SdpaAttention(\n",
              "        (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
              "        (k_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
              "        (v_proj): Linear4bit(in_features=5120, out_features=1024, bias=True)\n",
              "        (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "        (rotary_emb): Qwen2RotaryEmbedding()\n",
              "      )\n",
              "      (mlp): Qwen2MLP(\n",
              "        (gate_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
              "        (up_proj): Linear4bit(in_features=5120, out_features=27648, bias=False)\n",
              "        (down_proj): Linear4bit(in_features=27648, out_features=5120, bias=False)\n",
              "        (act_fn): SiLU()\n",
              "      )\n",
              "      (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
              "      (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
              "    )\n",
              "  )\n",
              "  (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kv7IZdGGiwWz"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=128,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"FEATURE_EXTRACTION\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoEhF5fdityz",
        "outputId": "72dedc39-a530-4bad-9828-15dabfb99e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 536,870,912 || all params: 32,522,179,584 || trainable%: 1.6508\n"
          ]
        }
      ],
      "source": [
        "base_model = prepare_model_for_kbit_training(base_model.model)\n",
        "base_model = get_peft_model(base_model, config)\n",
        "# base_model = get_peft_model(base_model.model, config)\n",
        "base_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTPy2gVhqU-U",
        "outputId": "96199de0-b38d-481f-93a8-f82a01839a10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name Qwen/Qwen2.5-0.5B-Instruct. Creating a new one with mean pooling.\n"
          ]
        }
      ],
      "source": [
        "model = SentenceTransformer(\"Qwen/Qwen2.5-0.5B-Instruct\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zen2lzLT_5ce",
        "outputId": "768f782d-7f56-42d0-ba2d-028ea9d7b029"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-0.5B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
              "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model._first_module().tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "a_EsxviEqb_x"
      },
      "outputs": [],
      "source": [
        "model._first_module().auto_model = base_model\n",
        "model._first_module().tokenizer = base_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PL9CG02EBMeZ"
      },
      "outputs": [],
      "source": [
        "model[1].pooling_mode_mean_tokens = False\n",
        "model[1].pooling_mode_lasttoken = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCg5Y8-aD2T8",
        "outputId": "c3693c7d-1cad-4093-9f09-8e7756f027b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 32768, 'do_lower_case': False}) with Transformer model: PeftModelForFeatureExtraction \n",
              "  (1): Pooling({'word_embedding_dimension': 896, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': True, 'include_prompt': True})\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GLUCS8Uqqgsz"
      },
      "outputs": [],
      "source": [
        "del base_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HBtQqSiuEgB"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "OEEdjIuhuDsd"
      },
      "outputs": [],
      "source": [
        "loss = CachedMultipleNegativesRankingLoss(model, mini_batch_size=16)\n",
        "# loss = MultipleNegativesRankingLoss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9kWzQX9uuQDS"
      },
      "outputs": [],
      "source": [
        "args = SentenceTransformerTrainingArguments(\n",
        "    # Required parameter:\n",
        "    output_dir=MODEL_OUTPUT_PATH,\n",
        "    # Optional training parameters:\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    num_train_epochs=EPOCH,\n",
        "    dataloader_num_workers=NUM_PROC,\n",
        "    per_device_train_batch_size=BS,\n",
        "#    gradient_accumulation_steps = 2,\n",
        "#    per_device_eval_batch_size=BS,\n",
        "    # learning_rate=LR,\n",
        "    warmup_ratio=0.0,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    # batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
        "    # Optional tracking/debugging parameters:\n",
        "    # lr_scheduler_type=\"cosine_with_restarts\",\n",
        "#    eval_strategy=\"epoch\",\n",
        "#    eval_steps=8,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_steps=1,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=1000,\n",
        "    report_to=REPORT_TO,  # Will be used in W&B if `wandb` is installed\n",
        "#    metric_for_best_model=\"eval_cosine_map@25\", # eval_cosine_recall@25\n",
        "    do_eval=False,\n",
        "    push_to_hub=False,\n",
        "#    load_best_model_at_end=True,\n",
        "    # gradient_checkpointing_kwargs=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "yjBvgP30u9Jh"
      },
      "outputs": [],
      "source": [
        "# dev_evaluator = InformationRetrievalEvaluator(\n",
        "#    ir_queries, ir_corpus, ir_relevant_docs,\n",
        "#    accuracy_at_k=[25],\n",
        "#    precision_recall_at_k=[25, 50, 100],\n",
        "#    map_at_k=[25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "VKDO2nlAvIFu"
      },
      "outputs": [],
      "source": [
        "trainer = SentenceTransformerTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds.select_columns(\n",
        "            [\"InputText\", \"Misconception\"]\n",
        "        ),\n",
        "#        eval_dataset=test_ds.select_columns(\n",
        "#            [\"InputText\", \"Misconception\"]\n",
        "#        ),\n",
        "        loss=loss,\n",
        "#        evaluator=dev_evaluator,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "df30a6b8dc7843cd9311bd38823359f3",
            "953e7d2d7b77444ab77665ef5c38876b",
            "85f1638a3c914a1f81ee2027bde26d69",
            "6ab98c6d02834491be51cc89d3e557b0",
            "b414284b579e4fa7a8ec4a8e55886121",
            "957e588fa7624fd3908e648aa8702372",
            "a3d390eb0631454eb5bca650abd510b7",
            "5c58a2b6aeed4b4cb1a9721a3e026d3d",
            "40b28c372a4c40ae8f485c952ea9e5aa",
            "7236dba500c546e3bdc438c48dab00f5",
            "de3aa3110aed48928c07221f18635d7c"
          ]
        },
        "id": "HmU3JRVIvP9I",
        "outputId": "abcbf842-8d18-4791-c000-785381d3bd2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='12710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [    3/12710 00:49 < 174:14:20, 0.02 it/s, Epoch 0.00/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdacMownxeHj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0261ae9d6e264adf8e350c40fa1d92e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27dd2228dd4f47328c23b1514e0bb00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40b28c372a4c40ae8f485c952ea9e5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c58a2b6aeed4b4cb1a9721a3e026d3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6725c0f5d729489bb70027366ca89d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0261ae9d6e264adf8e350c40fa1d92e4",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbdde8e5ee27407392724cf19ec34155",
            "value": 8
          }
        },
        "6ab98c6d02834491be51cc89d3e557b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7236dba500c546e3bdc438c48dab00f5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_de3aa3110aed48928c07221f18635d7c",
            "value": "â€‡0/1â€‡[00:00&lt;?,â€‡?example/s]"
          }
        },
        "7236dba500c546e3bdc438c48dab00f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82ea29243568468db4a0f55881af4f25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d222f273126d4954aa583e78b1cdfa86",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_27dd2228dd4f47328c23b1514e0bb00f",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "85f1638a3c914a1f81ee2027bde26d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c58a2b6aeed4b4cb1a9721a3e026d3d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40b28c372a4c40ae8f485c952ea9e5aa",
            "value": 1
          }
        },
        "91c2aa9616ed40ef834a1d35b1ee3cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82ea29243568468db4a0f55881af4f25",
              "IPY_MODEL_6725c0f5d729489bb70027366ca89d11",
              "IPY_MODEL_e3e9e206ac344806bc6cf4f7921f50ca"
            ],
            "layout": "IPY_MODEL_e9365a2dcf66496687bf9c6261a7fad6"
          }
        },
        "953e7d2d7b77444ab77665ef5c38876b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_957e588fa7624fd3908e648aa8702372",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a3d390eb0631454eb5bca650abd510b7",
            "value": "Computingâ€‡widgetâ€‡examples:â€‡â€‡â€‡0%"
          }
        },
        "957e588fa7624fd3908e648aa8702372": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d390eb0631454eb5bca650abd510b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afd683dfce07454fb73478e36d8e8767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b414284b579e4fa7a8ec4a8e55886121": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "d222f273126d4954aa583e78b1cdfa86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de3aa3110aed48928c07221f18635d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df30a6b8dc7843cd9311bd38823359f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_953e7d2d7b77444ab77665ef5c38876b",
              "IPY_MODEL_85f1638a3c914a1f81ee2027bde26d69",
              "IPY_MODEL_6ab98c6d02834491be51cc89d3e557b0"
            ],
            "layout": "IPY_MODEL_b414284b579e4fa7a8ec4a8e55886121"
          }
        },
        "e3e9e206ac344806bc6cf4f7921f50ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f86bb7950ae843c3a1f74891b3a18385",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_afd683dfce07454fb73478e36d8e8767",
            "value": "â€‡8/8â€‡[00:11&lt;00:00,â€‡â€‡1.17s/it]"
          }
        },
        "e9365a2dcf66496687bf9c6261a7fad6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f86bb7950ae843c3a1f74891b3a18385": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbdde8e5ee27407392724cf19ec34155": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
